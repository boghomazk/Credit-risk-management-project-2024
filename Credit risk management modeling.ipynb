{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost\n",
    "import catboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "import joblib\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, LinearLR, MultiStepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:\n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                              if filename.startswith('train')])\n",
    "    print(dataset_paths)\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "    if verbose:\n",
    "        print('Reading chunks:\\n')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        print('chunk_path', chunk_path)\n",
    "        chunk = pd.read_parquet(chunk_path,columns=columns)\n",
    "        res.append(chunk)\n",
    "\n",
    "    return pd.concat(res).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_dataset(path):\n",
    "    for i, elem in enumerate(['df_'+str(a) for a in range(0, 12)]):\n",
    "        elem = read_parquet_dataset_from_local(path, i, 1)\n",
    "        if i==0:\n",
    "            df = elem.copy()\n",
    "        else:\n",
    "            df = pd.concat([df, elem], ignore_index=True, axis=0)\n",
    "        del elem\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41fbe0071b7d470e869854b9043303f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_0.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c49f0d6c9734933b245675df98052ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_1.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de0bb63d6ed4cde91da026b5bce7d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_10.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461dfb1aea82466db15f303388ae1290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_11.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f4561d6a584ba095c0abf5bdf238e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_2.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b24bf3dab134c04bc877d78bee15064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_3.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89e8227362544adaa1083d551075f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_4.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202c0b661506439693a549118e799055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_5.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fd11c1f9c94915b4463426f03ed785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_6.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3c9b1ace82404985b1ce41b9b8f912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_7.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caaaa7990a49436f934055afbb6a5fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_8.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\1433847333.py:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3ddb4643464db7871fbe530919bb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_9.pq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rn</th>\n",
       "      <th>pre_since_opened</th>\n",
       "      <th>pre_since_confirmed</th>\n",
       "      <th>pre_pterm</th>\n",
       "      <th>pre_fterm</th>\n",
       "      <th>pre_till_pclose</th>\n",
       "      <th>pre_till_fclose</th>\n",
       "      <th>pre_loans_credit_limit</th>\n",
       "      <th>pre_loans_next_pay_summ</th>\n",
       "      <th>pre_loans_outstanding</th>\n",
       "      <th>pre_loans_total_overdue</th>\n",
       "      <th>pre_loans_max_overdue_sum</th>\n",
       "      <th>pre_loans_credit_cost_rate</th>\n",
       "      <th>pre_loans5</th>\n",
       "      <th>pre_loans530</th>\n",
       "      <th>pre_loans3060</th>\n",
       "      <th>pre_loans6090</th>\n",
       "      <th>pre_loans90</th>\n",
       "      <th>is_zero_loans5</th>\n",
       "      <th>is_zero_loans530</th>\n",
       "      <th>is_zero_loans3060</th>\n",
       "      <th>is_zero_loans6090</th>\n",
       "      <th>is_zero_loans90</th>\n",
       "      <th>pre_util</th>\n",
       "      <th>pre_over2limit</th>\n",
       "      <th>pre_maxover2limit</th>\n",
       "      <th>is_zero_util</th>\n",
       "      <th>is_zero_over2limit</th>\n",
       "      <th>is_zero_maxover2limit</th>\n",
       "      <th>enc_paym_0</th>\n",
       "      <th>enc_paym_1</th>\n",
       "      <th>enc_paym_2</th>\n",
       "      <th>enc_paym_3</th>\n",
       "      <th>enc_paym_4</th>\n",
       "      <th>enc_paym_5</th>\n",
       "      <th>enc_paym_6</th>\n",
       "      <th>enc_paym_7</th>\n",
       "      <th>enc_paym_8</th>\n",
       "      <th>enc_paym_9</th>\n",
       "      <th>enc_paym_10</th>\n",
       "      <th>enc_paym_11</th>\n",
       "      <th>enc_paym_12</th>\n",
       "      <th>enc_paym_13</th>\n",
       "      <th>enc_paym_14</th>\n",
       "      <th>enc_paym_15</th>\n",
       "      <th>enc_paym_16</th>\n",
       "      <th>enc_paym_17</th>\n",
       "      <th>enc_paym_18</th>\n",
       "      <th>enc_paym_19</th>\n",
       "      <th>enc_paym_20</th>\n",
       "      <th>enc_paym_21</th>\n",
       "      <th>enc_paym_22</th>\n",
       "      <th>enc_paym_23</th>\n",
       "      <th>enc_paym_24</th>\n",
       "      <th>enc_loans_account_holder_type</th>\n",
       "      <th>enc_loans_credit_status</th>\n",
       "      <th>enc_loans_credit_type</th>\n",
       "      <th>enc_loans_account_cur</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  rn  pre_since_opened  pre_since_confirmed  pre_pterm  pre_fterm  \\\n",
       "0   0   1                18                    9          2          3   \n",
       "1   0   2                18                    9         14         14   \n",
       "2   0   3                18                    9          4          8   \n",
       "3   0   4                 4                    1          9         12   \n",
       "4   0   5                 5                   12         15          2   \n",
       "\n",
       "   pre_till_pclose  pre_till_fclose  pre_loans_credit_limit  \\\n",
       "0               16               10                      11   \n",
       "1               12               12                       0   \n",
       "2                1               11                      11   \n",
       "3               16                7                      12   \n",
       "4               11               12                      10   \n",
       "\n",
       "   pre_loans_next_pay_summ  pre_loans_outstanding  pre_loans_total_overdue  \\\n",
       "0                        3                      3                        0   \n",
       "1                        3                      3                        0   \n",
       "2                        0                      5                        0   \n",
       "3                        2                      3                        0   \n",
       "4                        2                      3                        0   \n",
       "\n",
       "   pre_loans_max_overdue_sum  pre_loans_credit_cost_rate  pre_loans5  \\\n",
       "0                          2                          11           6   \n",
       "1                          2                          11           6   \n",
       "2                          2                           8           6   \n",
       "3                          2                           4           6   \n",
       "4                          2                           4           6   \n",
       "\n",
       "   pre_loans530  pre_loans3060  pre_loans6090  pre_loans90  is_zero_loans5  \\\n",
       "0            16              5              4            8               1   \n",
       "1            16              5              4            8               1   \n",
       "2            16              5              4            8               1   \n",
       "3            16              5              4            8               0   \n",
       "4            16              5              4            8               1   \n",
       "\n",
       "   is_zero_loans530  is_zero_loans3060  is_zero_loans6090  is_zero_loans90  \\\n",
       "0                 1                  1                  1                1   \n",
       "1                 1                  1                  1                1   \n",
       "2                 1                  1                  1                1   \n",
       "3                 1                  1                  1                1   \n",
       "4                 1                  1                  1                1   \n",
       "\n",
       "   pre_util  pre_over2limit  pre_maxover2limit  is_zero_util  \\\n",
       "0        16               2                 17             1   \n",
       "1        16               2                 17             1   \n",
       "2        15               2                 17             0   \n",
       "3        16               2                 17             1   \n",
       "4        16               2                 17             1   \n",
       "\n",
       "   is_zero_over2limit  is_zero_maxover2limit  enc_paym_0  enc_paym_1  \\\n",
       "0                   1                      1           0           0   \n",
       "1                   1                      1           0           0   \n",
       "2                   1                      1           0           0   \n",
       "3                   1                      1           1           0   \n",
       "4                   1                      1           0           0   \n",
       "\n",
       "   enc_paym_2  enc_paym_3  enc_paym_4  enc_paym_5  enc_paym_6  enc_paym_7  \\\n",
       "0           3           3           3           3           3           3   \n",
       "1           0           0           0           0           0           0   \n",
       "2           0           0           0           0           0           0   \n",
       "3           0           0           0           0           0           0   \n",
       "4           0           0           0           0           0           3   \n",
       "\n",
       "   enc_paym_8  enc_paym_9  enc_paym_10  enc_paym_11  enc_paym_12  enc_paym_13  \\\n",
       "0           3           3            3            4            3            3   \n",
       "1           0           0            0            1            0            0   \n",
       "2           0           0            0            1            0            0   \n",
       "3           0           0            0            1            3            3   \n",
       "4           3           3            3            4            3            3   \n",
       "\n",
       "   enc_paym_14  enc_paym_15  enc_paym_16  enc_paym_17  enc_paym_18  \\\n",
       "0            3            3            3            3            3   \n",
       "1            0            0            0            0            0   \n",
       "2            0            0            0            0            0   \n",
       "3            3            3            3            3            3   \n",
       "4            3            3            3            3            3   \n",
       "\n",
       "   enc_paym_19  enc_paym_20  enc_paym_21  enc_paym_22  enc_paym_23  \\\n",
       "0            3            4            3            3            3   \n",
       "1            0            1            0            0            0   \n",
       "2            0            1            0            0            0   \n",
       "3            3            4            3            3            3   \n",
       "4            3            4            3            3            3   \n",
       "\n",
       "   enc_paym_24  enc_loans_account_holder_type  enc_loans_credit_status  \\\n",
       "0            4                              1                        3   \n",
       "1            4                              1                        3   \n",
       "2            4                              1                        2   \n",
       "3            4                              1                        3   \n",
       "4            4                              1                        3   \n",
       "\n",
       "   enc_loans_credit_type  enc_loans_account_cur  pclose_flag  fclose_flag  \n",
       "0                      4                      1            0            0  \n",
       "1                      4                      1            0            0  \n",
       "2                      3                      1            1            1  \n",
       "3                      1                      1            0            0  \n",
       "4                      4                      1            0            0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'train_data/'\n",
    "df = create_dataset(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26162717, 61)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "id\n",
      "2891734    58\n",
      "1782755    55\n",
      "1658519    54\n",
      "62963      51\n",
      "508303     51\n",
      "318999     50\n",
      "755726     50\n",
      "517425     49\n",
      "2479204    49\n",
      "2160706    48\n",
      "Name: count, dtype: int64\n",
      "3000000\n",
      "------------------------------\n",
      "rn\n",
      "1     3000000\n",
      "2     2779455\n",
      "3     2550810\n",
      "4     2320731\n",
      "5     2094188\n",
      "6     1876509\n",
      "7     1668536\n",
      "8     1473997\n",
      "9     1294408\n",
      "10    1130303\n",
      "Name: count, dtype: int64\n",
      "58\n",
      "------------------------------\n",
      "pre_since_opened\n",
      "13    1645938\n",
      "0     1561873\n",
      "6     1522882\n",
      "10    1409770\n",
      "8     1406244\n",
      "18    1388400\n",
      "14    1315095\n",
      "9     1312790\n",
      "7     1307521\n",
      "1     1300842\n",
      "Name: count, dtype: int64\n",
      "20\n",
      "------------------------------\n",
      "pre_since_confirmed\n",
      "9     4909419\n",
      "6     1948337\n",
      "4     1514861\n",
      "17    1503451\n",
      "3     1435553\n",
      "14    1410504\n",
      "10    1361991\n",
      "13    1313856\n",
      "7     1291490\n",
      "1     1245131\n",
      "Name: count, dtype: int64\n",
      "18\n",
      "------------------------------\n",
      "pre_pterm\n",
      "4     4456185\n",
      "14    2336504\n",
      "2     1532229\n",
      "17    1520418\n",
      "1     1483207\n",
      "16    1460983\n",
      "11    1430459\n",
      "9     1428200\n",
      "6     1400005\n",
      "7     1391864\n",
      "Name: count, dtype: int64\n",
      "18\n",
      "------------------------------\n",
      "pre_fterm\n",
      "8     6204663\n",
      "9     1695807\n",
      "0     1597751\n",
      "14    1566914\n",
      "6     1493807\n",
      "16    1463400\n",
      "7     1460416\n",
      "13    1363752\n",
      "15    1323654\n",
      "11    1303353\n",
      "Name: count, dtype: int64\n",
      "17\n",
      "------------------------------\n",
      "pre_till_pclose\n",
      "1     4910941\n",
      "8     1499795\n",
      "13    1466470\n",
      "14    1451729\n",
      "12    1441898\n",
      "3     1419577\n",
      "6     1395040\n",
      "5     1357688\n",
      "9     1327500\n",
      "10    1323903\n",
      "Name: count, dtype: int64\n",
      "17\n",
      "------------------------------\n",
      "pre_till_fclose\n",
      "11    7082796\n",
      "5     1491932\n",
      "3     1490175\n",
      "8     1427444\n",
      "10    1377705\n",
      "13    1360559\n",
      "1     1326558\n",
      "4     1307602\n",
      "7     1300230\n",
      "12    1289604\n",
      "Name: count, dtype: int64\n",
      "16\n",
      "------------------------------\n",
      "pre_loans_credit_limit\n",
      "14    2136533\n",
      "19    1459193\n",
      "11    1422358\n",
      "15    1411541\n",
      "0     1408767\n",
      "7     1383034\n",
      "8     1375453\n",
      "2     1366620\n",
      "10    1360834\n",
      "16    1352225\n",
      "Name: count, dtype: int64\n",
      "20\n",
      "------------------------------\n",
      "pre_loans_next_pay_summ\n",
      "2    17777162\n",
      "0     1452141\n",
      "5     1438360\n",
      "3     1406202\n",
      "1     1382525\n",
      "4     1372003\n",
      "6     1334324\n",
      "Name: count, dtype: int64\n",
      "7\n",
      "------------------------------\n",
      "pre_loans_outstanding\n",
      "3    20909470\n",
      "1     1343015\n",
      "2     1316280\n",
      "5     1306107\n",
      "4     1287845\n",
      "Name: count, dtype: int64\n",
      "5\n",
      "------------------------------\n",
      "pre_loans_total_overdue\n",
      "0    26162716\n",
      "1           1\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "pre_loans_max_overdue_sum\n",
      "2    24071329\n",
      "3     1113827\n",
      "1      977560\n",
      "0           1\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "pre_loans_credit_cost_rate\n",
      "4     7018834\n",
      "2     6262438\n",
      "3     1638369\n",
      "5     1524119\n",
      "9     1375623\n",
      "1     1357623\n",
      "7     1355353\n",
      "0     1242721\n",
      "11    1215253\n",
      "13    1187225\n",
      "Name: count, dtype: int64\n",
      "14\n",
      "------------------------------\n",
      "pre_loans5\n",
      "6     26019398\n",
      "0        95329\n",
      "3        28998\n",
      "5        11810\n",
      "2         4386\n",
      "16        1766\n",
      "13         640\n",
      "7          250\n",
      "1           91\n",
      "8           32\n",
      "Name: count, dtype: int64\n",
      "13\n",
      "------------------------------\n",
      "pre_loans530\n",
      "16    25550977\n",
      "13      422128\n",
      "0       110507\n",
      "18       42054\n",
      "6        19217\n",
      "3         9117\n",
      "2         4433\n",
      "12        2185\n",
      "15         864\n",
      "4          575\n",
      "Name: count, dtype: int64\n",
      "20\n",
      "------------------------------\n",
      "pre_loans3060\n",
      "5    26147054\n",
      "8       12924\n",
      "2        2115\n",
      "7         468\n",
      "9         112\n",
      "1          34\n",
      "6           6\n",
      "0           2\n",
      "3           1\n",
      "4           1\n",
      "Name: count, dtype: int64\n",
      "10\n",
      "------------------------------\n",
      "pre_loans6090\n",
      "4    26161392\n",
      "1        1236\n",
      "2          76\n",
      "3          12\n",
      "0           1\n",
      "Name: count, dtype: int64\n",
      "5\n",
      "------------------------------\n",
      "pre_loans90\n",
      "8     26147777\n",
      "14       11519\n",
      "13        2191\n",
      "19         791\n",
      "2          278\n",
      "10         120\n",
      "3           41\n",
      "Name: count, dtype: int64\n",
      "7\n",
      "------------------------------\n",
      "is_zero_loans5\n",
      "1    24035861\n",
      "0     2126856\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "is_zero_loans530\n",
      "1    21772900\n",
      "0     4389817\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "is_zero_loans3060\n",
      "1    25047143\n",
      "0     1115574\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "is_zero_loans6090\n",
      "1    25519933\n",
      "0      642784\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "is_zero_loans90\n",
      "1    25535248\n",
      "0      627469\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "pre_util\n",
      "16    18599223\n",
      "9      1751629\n",
      "3       962796\n",
      "18      595518\n",
      "6       487768\n",
      "11      453764\n",
      "15      380229\n",
      "1       332213\n",
      "7       289571\n",
      "10      277074\n",
      "Name: count, dtype: int64\n",
      "20\n",
      "------------------------------\n",
      "pre_over2limit\n",
      "2     24403395\n",
      "5      1717063\n",
      "17       12381\n",
      "6         7073\n",
      "15        3031\n",
      "4         2688\n",
      "8         1784\n",
      "14        1415\n",
      "11        1390\n",
      "13        1277\n",
      "Name: count, dtype: int64\n",
      "20\n",
      "------------------------------\n",
      "pre_maxover2limit\n",
      "17    23505568\n",
      "4      1721548\n",
      "3       571242\n",
      "0       161648\n",
      "11       73027\n",
      "15       24387\n",
      "5        23609\n",
      "9        23511\n",
      "16        9324\n",
      "8         6894\n",
      "Name: count, dtype: int64\n",
      "20\n",
      "------------------------------\n",
      "is_zero_util\n",
      "1    18258067\n",
      "0     7904650\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "is_zero_over2limit\n",
      "1    24334366\n",
      "0     1828351\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "is_zero_maxover2limit\n",
      "1    22383950\n",
      "0     3778767\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "enc_paym_0\n",
      "0    24136144\n",
      "3     1109198\n",
      "1      853844\n",
      "2       63531\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_1\n",
      "0    21792098\n",
      "3     2733662\n",
      "1     1521366\n",
      "2      115591\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_2\n",
      "0    20686082\n",
      "3     4008468\n",
      "1     1362284\n",
      "2      105883\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_3\n",
      "0    19601983\n",
      "3     5167483\n",
      "1     1292438\n",
      "2      100813\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_4\n",
      "0    18500050\n",
      "3     6356585\n",
      "1     1212916\n",
      "2       93166\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_5\n",
      "0    17409261\n",
      "3     7542048\n",
      "1     1125455\n",
      "2       85953\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_6\n",
      "0    16073316\n",
      "3     8980208\n",
      "1     1034735\n",
      "2       74458\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_7\n",
      "0    14780042\n",
      "3    10354898\n",
      "1      959831\n",
      "2       67946\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_8\n",
      "0    13842200\n",
      "3    11370158\n",
      "1      888696\n",
      "2       61663\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_9\n",
      "0    12943923\n",
      "3    12343335\n",
      "1      820177\n",
      "2       55282\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_10\n",
      "3    13486126\n",
      "0    11886401\n",
      "1      739935\n",
      "2       50255\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_11\n",
      "4    14650304\n",
      "1    10796534\n",
      "2      669937\n",
      "3       45942\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_12\n",
      "3    15844536\n",
      "0     9676643\n",
      "1      597470\n",
      "2       44068\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_13\n",
      "3    16924629\n",
      "0     8638241\n",
      "1      557416\n",
      "2       42431\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_14\n",
      "3    17485829\n",
      "0     8103776\n",
      "1      531865\n",
      "2       41247\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_15\n",
      "3    17925332\n",
      "0     7689930\n",
      "1      507580\n",
      "2       39875\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_16\n",
      "3    18322875\n",
      "0     7315283\n",
      "1      486053\n",
      "2       38506\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_17\n",
      "3    18697435\n",
      "0     6965264\n",
      "1      463426\n",
      "2       36592\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_18\n",
      "3    19094597\n",
      "0     6591695\n",
      "1      440732\n",
      "2       35693\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_19\n",
      "3    19453734\n",
      "0     6253782\n",
      "1      421092\n",
      "2       34109\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_20\n",
      "4    19762231\n",
      "1     5965048\n",
      "2      402922\n",
      "3       32516\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_21\n",
      "3    20049435\n",
      "0     5697416\n",
      "1      384819\n",
      "2       31047\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_22\n",
      "3    20332673\n",
      "0     5435204\n",
      "1      365132\n",
      "2       29708\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_paym_23\n",
      "3    20634992\n",
      "0     5157075\n",
      "1      342900\n",
      "2       27750\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_paym_24\n",
      "4    21966247\n",
      "1     3919903\n",
      "2      251452\n",
      "3       25115\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "enc_loans_account_holder_type\n",
      "1    25807322\n",
      "3      213021\n",
      "4      117422\n",
      "6       20625\n",
      "5        2232\n",
      "2        2031\n",
      "0          64\n",
      "Name: count, dtype: int64\n",
      "7\n",
      "------------------------------\n",
      "enc_loans_credit_status\n",
      "3    17561773\n",
      "2     8276203\n",
      "4      201958\n",
      "5       68431\n",
      "1       48568\n",
      "6        3803\n",
      "0        1981\n",
      "Name: count, dtype: int64\n",
      "7\n",
      "------------------------------\n",
      "enc_loans_credit_type\n",
      "4    14774986\n",
      "3     7736233\n",
      "5     1050346\n",
      "1      880196\n",
      "0      756480\n",
      "2      617386\n",
      "7      310217\n",
      "6       36873\n",
      "Name: count, dtype: int64\n",
      "8\n",
      "------------------------------\n",
      "enc_loans_account_cur\n",
      "1    26100882\n",
      "2       53102\n",
      "0        8550\n",
      "3         183\n",
      "Name: count, dtype: int64\n",
      "4\n",
      "------------------------------\n",
      "pclose_flag\n",
      "0    22259294\n",
      "1     3903423\n",
      "Name: count, dtype: int64\n",
      "2\n",
      "------------------------------\n",
      "fclose_flag\n",
      "0    20172292\n",
      "1     5990425\n",
      "Name: count, dtype: int64\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for elem in df.columns:\n",
    "    print('------------------------------')\n",
    "    print(df[elem].value_counts().head(10))\n",
    "    print(df[elem].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#распределение кредитный продуктов по клиентам\n",
    "plt.hist(df['rn'], color = 'blue', edgecolor = 'black', bins = int(df['rn'].nunique()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['pre_since_opened', 'pre_since_confirmed', 'pre_pterm',\n",
    "       'pre_fterm', 'pre_till_pclose', 'pre_till_fclose',\n",
    "       'pre_loans_credit_limit', 'pre_loans_next_pay_summ',\n",
    "       'pre_loans_outstanding', 'pre_loans_total_overdue',\n",
    "       'pre_loans_max_overdue_sum', 'pre_loans_credit_cost_rate', 'pre_loans5',\n",
    "       'pre_loans530', 'pre_loans3060', 'pre_loans6090', 'pre_loans90', \n",
    "        'pre_util', 'pre_over2limit', 'pre_maxover2limit', 'enc_paym_0', 'enc_paym_1', 'enc_paym_2',\n",
    "       'enc_paym_3', 'enc_paym_4', 'enc_paym_5', 'enc_paym_6', 'enc_paym_7',\n",
    "       'enc_paym_8', 'enc_paym_9', 'enc_paym_10', 'enc_paym_11', 'enc_paym_12',\n",
    "       'enc_paym_13', 'enc_paym_14', 'enc_paym_15', 'enc_paym_16',\n",
    "       'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_20',\n",
    "       'enc_paym_21', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24',\n",
    "       'enc_loans_account_holder_type', 'enc_loans_credit_status',\n",
    "       'enc_loans_credit_type', 'enc_loans_account_cur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "rn                         0\n",
       "pre_since_opened           0\n",
       "pre_since_confirmed        0\n",
       "pre_pterm                  0\n",
       "                          ..\n",
       "enc_loans_credit_status    0\n",
       "enc_loans_credit_type      0\n",
       "enc_loans_account_cur      0\n",
       "pclose_flag                0\n",
       "fclose_flag                0\n",
       "Length: 61, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()\n",
    "#пропусков нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999995</th>\n",
       "      <td>2999995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999996</th>\n",
       "      <td>2999996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999997</th>\n",
       "      <td>2999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999998</th>\n",
       "      <td>2999998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999999</th>\n",
       "      <td>2999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  flag\n",
       "0              0     0\n",
       "1              1     0\n",
       "2              2     0\n",
       "3              3     0\n",
       "4              4     0\n",
       "...          ...   ...\n",
       "2999995  2999995     0\n",
       "2999996  2999996     0\n",
       "2999997  2999997     0\n",
       "2999998  2999998     0\n",
       "2999999  2999999     0\n",
       "\n",
       "[3000000 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('train_target.csv')\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#так как часть данных бинаризована имеет смысл провести кодирование с последующей группировкой + данные по длине совпадут с таргетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\3302740645.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore', dtype='int8')\n",
    "df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
    "df.drop(columns=cat_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26162717, 362)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#новый признак\n",
    "df_1 = df.iloc[:, :2].copy().groupby(['id']).max()\n",
    "df_1.columns = ['max_rn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\AppData\\Local\\Temp\\ipykernel_572\\2228575633.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df.loc[:, 'target'] = targets.loc[:, 'flag']\n"
     ]
    }
   ],
   "source": [
    "df = df.groupby(['id']).sum()\n",
    "df.merge(df_1, how='left', on='id').drop(columns='rn')\n",
    "df.loc[:, 'target'] = targets.loc[:, 'flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_64_cols  = df.select_dtypes('int64').columns\n",
    "df[int_64_cols] = df[int_64_cols].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3000000 entries, 0 to 2999999\n",
      "Columns: 362 entries, rn to target\n",
      "dtypes: int8(362)\n",
      "memory usage: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    2893558\n",
       "1     106442\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03678585326438938"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()[1]/df['target'].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['target'], color = 'blue', edgecolor = 'black', bins = int(df['target'].nunique()))\n",
    "plt.title('Target distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Amount');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7300298634541145\n",
      "test:  0.7195067107844934\n",
      "CPU times: total: 59.6 s\n",
      "Wall time: 59.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators=15,\n",
    "                           max_depth=9,\n",
    "                           min_samples_leaf=8).fit(X_train, y_train)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))\n",
    "print('test: ', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-11 14:39:03,955] A new study created in memory with name: no-name-1330a484-cba5-4e15-9316-bdf9d38710e2\n",
      "[I 2024-09-11 14:41:02,636] Trial 0 finished with value: 0.7258250291061215 and parameters: {'n_estimators': 27, 'max_depth': 10, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:42:05,072] Trial 1 finished with value: 0.7146212460243859 and parameters: {'n_estimators': 20, 'max_depth': 7, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:43:19,629] Trial 2 finished with value: 0.7181767691075756 and parameters: {'n_estimators': 23, 'max_depth': 8, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:45:05,072] Trial 3 finished with value: 0.7251672187765128 and parameters: {'n_estimators': 26, 'max_depth': 10, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:45:54,389] Trial 4 finished with value: 0.704203318113463 and parameters: {'n_estimators': 20, 'max_depth': 6, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:47:26,896] Trial 5 finished with value: 0.7110944523209972 and parameters: {'n_estimators': 39, 'max_depth': 6, 'min_samples_leaf': 7}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:48:17,199] Trial 6 finished with value: 0.7001272577057135 and parameters: {'n_estimators': 24, 'max_depth': 5, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:49:22,293] Trial 7 finished with value: 0.7101707180796641 and parameters: {'n_estimators': 27, 'max_depth': 6, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:51:00,195] Trial 8 finished with value: 0.7205581758847186 and parameters: {'n_estimators': 32, 'max_depth': 8, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:51:53,030] Trial 9 finished with value: 0.7002989165200284 and parameters: {'n_estimators': 26, 'max_depth': 5, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.7258250291061215.\n",
      "[I 2024-09-11 14:53:56,206] Trial 10 finished with value: 0.7269150843155572 and parameters: {'n_estimators': 33, 'max_depth': 10, 'min_samples_leaf': 13}. Best is trial 10 with value: 0.7269150843155572.\n",
      "[I 2024-09-11 14:55:59,638] Trial 11 finished with value: 0.7278116250344678 and parameters: {'n_estimators': 33, 'max_depth': 10, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 14:57:55,096] Trial 12 finished with value: 0.72315364055679 and parameters: {'n_estimators': 34, 'max_depth': 9, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 14:59:57,662] Trial 13 finished with value: 0.724318998565851 and parameters: {'n_estimators': 36, 'max_depth': 9, 'min_samples_leaf': 15}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:01:55,620] Trial 14 finished with value: 0.7265235450773222 and parameters: {'n_estimators': 31, 'max_depth': 10, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:04:05,511] Trial 15 finished with value: 0.7254458991725723 and parameters: {'n_estimators': 39, 'max_depth': 9, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:06:22,282] Trial 16 finished with value: 0.7244715805944963 and parameters: {'n_estimators': 36, 'max_depth': 9, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:08:22,307] Trial 17 finished with value: 0.7268728562451685 and parameters: {'n_estimators': 29, 'max_depth': 10, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:10:09,551] Trial 18 finished with value: 0.7204084411790744 and parameters: {'n_estimators': 34, 'max_depth': 8, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:11:38,257] Trial 19 finished with value: 0.7149289783486492 and parameters: {'n_estimators': 30, 'max_depth': 7, 'min_samples_leaf': 11}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:13:54,430] Trial 20 finished with value: 0.7269028365426923 and parameters: {'n_estimators': 33, 'max_depth': 10, 'min_samples_leaf': 15}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:16:09,175] Trial 21 finished with value: 0.7262130161050897 and parameters: {'n_estimators': 33, 'max_depth': 10, 'min_samples_leaf': 15}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:18:20,839] Trial 22 finished with value: 0.7232302168962428 and parameters: {'n_estimators': 36, 'max_depth': 9, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:20:21,933] Trial 23 finished with value: 0.7257044395202306 and parameters: {'n_estimators': 29, 'max_depth': 10, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:22:26,023] Trial 24 finished with value: 0.7238970761020522 and parameters: {'n_estimators': 35, 'max_depth': 9, 'min_samples_leaf': 15}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:24:50,071] Trial 25 finished with value: 0.7273541426241381 and parameters: {'n_estimators': 38, 'max_depth': 10, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:27:08,858] Trial 26 finished with value: 0.7249078449258821 and parameters: {'n_estimators': 40, 'max_depth': 9, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:29:32,410] Trial 27 finished with value: 0.7275697590089256 and parameters: {'n_estimators': 38, 'max_depth': 10, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:31:27,104] Trial 28 finished with value: 0.7214965901710826 and parameters: {'n_estimators': 38, 'max_depth': 8, 'min_samples_leaf': 11}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:33:55,946] Trial 29 finished with value: 0.7273118290039181 and parameters: {'n_estimators': 38, 'max_depth': 10, 'min_samples_leaf': 7}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:36:18,702] Trial 30 finished with value: 0.7274452166927727 and parameters: {'n_estimators': 37, 'max_depth': 10, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:38:37,660] Trial 31 finished with value: 0.7274482666145752 and parameters: {'n_estimators': 37, 'max_depth': 10, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:40:57,996] Trial 32 finished with value: 0.7272994682381264 and parameters: {'n_estimators': 37, 'max_depth': 10, 'min_samples_leaf': 11}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:43:15,776] Trial 33 finished with value: 0.7249735656966827 and parameters: {'n_estimators': 40, 'max_depth': 9, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:45:27,512] Trial 34 finished with value: 0.7273876348677225 and parameters: {'n_estimators': 35, 'max_depth': 10, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:47:36,851] Trial 35 finished with value: 0.7247849264492376 and parameters: {'n_estimators': 37, 'max_depth': 9, 'min_samples_leaf': 9}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:50:14,612] Trial 36 finished with value: 0.726739843285727 and parameters: {'n_estimators': 37, 'max_depth': 10, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:51:53,389] Trial 37 finished with value: 0.7164409655058754 and parameters: {'n_estimators': 35, 'max_depth': 7, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:54:21,141] Trial 38 finished with value: 0.7266877055887208 and parameters: {'n_estimators': 39, 'max_depth': 10, 'min_samples_leaf': 8}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:55:57,531] Trial 39 finished with value: 0.719565043059494 and parameters: {'n_estimators': 31, 'max_depth': 8, 'min_samples_leaf': 10}. Best is trial 11 with value: 0.7278116250344678.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-11 15:57:15,722] Trial 40 finished with value: 0.7224512214481229 and parameters: {'n_estimators': 22, 'max_depth': 9, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 15:59:31,003] Trial 41 finished with value: 0.7264532969159494 and parameters: {'n_estimators': 35, 'max_depth': 10, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:01:42,392] Trial 42 finished with value: 0.7268488308842687 and parameters: {'n_estimators': 34, 'max_depth': 10, 'min_samples_leaf': 14}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:03:59,140] Trial 43 finished with value: 0.7247377266025172 and parameters: {'n_estimators': 36, 'max_depth': 10, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:06:02,908] Trial 44 finished with value: 0.7273937710233973 and parameters: {'n_estimators': 32, 'max_depth': 10, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:08:02,101] Trial 45 finished with value: 0.7259647132414913 and parameters: {'n_estimators': 32, 'max_depth': 10, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:09:37,049] Trial 46 finished with value: 0.724514554939325 and parameters: {'n_estimators': 28, 'max_depth': 9, 'min_samples_leaf': 11}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:10:51,526] Trial 47 finished with value: 0.7114859879965705 and parameters: {'n_estimators': 32, 'max_depth': 6, 'min_samples_leaf': 13}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:13:14,404] Trial 48 finished with value: 0.7274805892592656 and parameters: {'n_estimators': 39, 'max_depth': 10, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.7278116250344678.\n",
      "[I 2024-09-11 16:15:27,272] Trial 49 finished with value: 0.7251469239421594 and parameters: {'n_estimators': 40, 'max_depth': 9, 'min_samples_leaf': 12}. Best is trial 11 with value: 0.7278116250344678.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1h 35min 56s\n",
      "Wall time: 1h 36min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 20, 40)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 10)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf',  2, 15)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=2, scoring='roc_auc').mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 33, 'max_depth': 10, 'min_samples_leaf': 14}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7435445833965082\n",
      "test:  0.7284031662748849\n",
      "CPU times: total: 2min 16s\n",
      "Wall time: 2min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                           max_depth=best_params['max_depth'],\n",
    "                           min_samples_leaf=best_params['min_samples_leaf']).fit(X_train, y_train)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))\n",
    "print('test: ', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.DataFrame({'features': X_train.columns.tolist(), 'imp': rf.feature_importances_.tolist()})\n",
    "new_columns = imp_df.loc[imp_df['imp']!=0, 'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7423180453915181\n",
      "test:  0.7264555741531732\n",
      "CPU times: total: 2min 10s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                           max_depth=best_params['max_depth'],\n",
    "                           min_samples_leaf=best_params['min_samples_leaf']).fit(X_train[new_columns], y_train)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, rf.predict_proba(X_train[new_columns])[:,1]))\n",
    "print('test: ', roc_auc_score(y_test, rf.predict_proba(X_test[new_columns])[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-11 16:47:06,948] A new study created in memory with name: no-name-5d7a2a73-131c-4275-a6c3-3d68af496585\n",
      "[I 2024-09-11 16:48:25,993] Trial 0 finished with value: 0.7024097943796903 and parameters: {'n_estimators': 38, 'max_depth': 5, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.7024097943796903.\n",
      "[I 2024-09-11 16:50:34,582] Trial 1 finished with value: 0.7056682868792233 and parameters: {'n_estimators': 68, 'max_depth': 5, 'min_samples_leaf': 13}. Best is trial 1 with value: 0.7056682868792233.\n",
      "[I 2024-09-11 16:52:10,890] Trial 2 finished with value: 0.7248859378181118 and parameters: {'n_estimators': 30, 'max_depth': 9, 'min_samples_leaf': 8}. Best is trial 2 with value: 0.7248859378181118.\n",
      "[I 2024-09-11 16:54:50,076] Trial 3 finished with value: 0.72257673315996 and parameters: {'n_estimators': 57, 'max_depth': 8, 'min_samples_leaf': 16}. Best is trial 2 with value: 0.7248859378181118.\n",
      "[I 2024-09-11 16:56:24,497] Trial 4 finished with value: 0.7097894894844551 and parameters: {'n_estimators': 44, 'max_depth': 6, 'min_samples_leaf': 16}. Best is trial 2 with value: 0.7248859378181118.\n",
      "[I 2024-09-11 17:01:59,426] Trial 5 finished with value: 0.7319407418378496 and parameters: {'n_estimators': 89, 'max_depth': 11, 'min_samples_leaf': 15}. Best is trial 5 with value: 0.7319407418378496.\n",
      "[I 2024-09-11 17:04:50,654] Trial 6 finished with value: 0.7227289965791909 and parameters: {'n_estimators': 60, 'max_depth': 8, 'min_samples_leaf': 30}. Best is trial 5 with value: 0.7319407418378496.\n",
      "[I 2024-09-11 17:09:18,373] Trial 7 finished with value: 0.7300524197069325 and parameters: {'n_estimators': 76, 'max_depth': 10, 'min_samples_leaf': 26}. Best is trial 5 with value: 0.7319407418378496.\n",
      "[I 2024-09-11 17:10:16,893] Trial 8 finished with value: 0.7096590049519402 and parameters: {'n_estimators': 26, 'max_depth': 6, 'min_samples_leaf': 7}. Best is trial 5 with value: 0.7319407418378496.\n",
      "[I 2024-09-11 17:13:42,786] Trial 9 finished with value: 0.7191616280979105 and parameters: {'n_estimators': 82, 'max_depth': 7, 'min_samples_leaf': 30}. Best is trial 5 with value: 0.7319407418378496.\n",
      "[I 2024-09-11 17:20:03,055] Trial 10 finished with value: 0.7325500915257201 and parameters: {'n_estimators': 99, 'max_depth': 11, 'min_samples_leaf': 23}. Best is trial 10 with value: 0.7325500915257201.\n",
      "[I 2024-09-11 17:26:21,332] Trial 11 finished with value: 0.7322914059205912 and parameters: {'n_estimators': 97, 'max_depth': 11, 'min_samples_leaf': 21}. Best is trial 10 with value: 0.7325500915257201.\n",
      "[I 2024-09-11 17:33:10,966] Trial 12 finished with value: 0.7326273948823263 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_leaf': 23}. Best is trial 12 with value: 0.7326273948823263.\n",
      "[I 2024-09-11 17:39:15,957] Trial 13 finished with value: 0.7309263924440383 and parameters: {'n_estimators': 99, 'max_depth': 10, 'min_samples_leaf': 22}. Best is trial 12 with value: 0.7326273948823263.\n",
      "[I 2024-09-11 17:44:56,204] Trial 14 finished with value: 0.7301296215807578 and parameters: {'n_estimators': 90, 'max_depth': 10, 'min_samples_leaf': 23}. Best is trial 12 with value: 0.7326273948823263.\n",
      "[I 2024-09-11 17:50:31,828] Trial 15 finished with value: 0.732796926565521 and parameters: {'n_estimators': 77, 'max_depth': 11, 'min_samples_leaf': 26}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 17:55:05,534] Trial 16 finished with value: 0.726802176167338 and parameters: {'n_estimators': 74, 'max_depth': 9, 'min_samples_leaf': 26}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:00:04,941] Trial 17 finished with value: 0.727321601630158 and parameters: {'n_estimators': 87, 'max_depth': 9, 'min_samples_leaf': 20}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:03:41,987] Trial 18 finished with value: 0.7313302028639663 and parameters: {'n_estimators': 49, 'max_depth': 11, 'min_samples_leaf': 26}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:08:03,191] Trial 19 finished with value: 0.7293240200938664 and parameters: {'n_estimators': 65, 'max_depth': 10, 'min_samples_leaf': 12}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:12:28,301] Trial 20 finished with value: 0.727076739837409 and parameters: {'n_estimators': 79, 'max_depth': 9, 'min_samples_leaf': 31}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:19:17,548] Trial 21 finished with value: 0.7324142870119568 and parameters: {'n_estimators': 98, 'max_depth': 11, 'min_samples_leaf': 19}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:26:15,979] Trial 22 finished with value: 0.7326659720850993 and parameters: {'n_estimators': 92, 'max_depth': 11, 'min_samples_leaf': 24}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:31:47,139] Trial 23 finished with value: 0.7301237158624738 and parameters: {'n_estimators': 86, 'max_depth': 10, 'min_samples_leaf': 27}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:38:08,745] Trial 24 finished with value: 0.7325479675488935 and parameters: {'n_estimators': 92, 'max_depth': 11, 'min_samples_leaf': 24}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:42:41,909] Trial 25 finished with value: 0.7302174668186985 and parameters: {'n_estimators': 72, 'max_depth': 10, 'min_samples_leaf': 28}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:48:31,553] Trial 26 finished with value: 0.7322738819195005 and parameters: {'n_estimators': 81, 'max_depth': 11, 'min_samples_leaf': 19}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:53:14,496] Trial 27 finished with value: 0.7228479595259654 and parameters: {'n_estimators': 93, 'max_depth': 8, 'min_samples_leaf': 24}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 18:58:29,322] Trial 28 finished with value: 0.7307322855605926 and parameters: {'n_estimators': 84, 'max_depth': 10, 'min_samples_leaf': 28}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 19:04:45,235] Trial 29 finished with value: 0.7324551405311372 and parameters: {'n_estimators': 94, 'max_depth': 11, 'min_samples_leaf': 18}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 19:08:34,813] Trial 30 finished with value: 0.7244926765332186 and parameters: {'n_estimators': 70, 'max_depth': 9, 'min_samples_leaf': 2}. Best is trial 15 with value: 0.732796926565521.\n",
      "[I 2024-09-11 19:15:07,089] Trial 31 finished with value: 0.7329188686234716 and parameters: {'n_estimators': 99, 'max_depth': 11, 'min_samples_leaf': 24}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:21:44,153] Trial 32 finished with value: 0.732678725264414 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_leaf': 25}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:27:26,066] Trial 33 finished with value: 0.7297714449968693 and parameters: {'n_estimators': 93, 'max_depth': 10, 'min_samples_leaf': 25}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:32:42,439] Trial 34 finished with value: 0.7325972155289395 and parameters: {'n_estimators': 78, 'max_depth': 11, 'min_samples_leaf': 29}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:37:00,131] Trial 35 finished with value: 0.7307587096811982 and parameters: {'n_estimators': 65, 'max_depth': 11, 'min_samples_leaf': 21}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:39:46,472] Trial 36 finished with value: 0.7061023109355411 and parameters: {'n_estimators': 88, 'max_depth': 5, 'min_samples_leaf': 14}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:45:24,648] Trial 37 finished with value: 0.7307070528114518 and parameters: {'n_estimators': 96, 'max_depth': 10, 'min_samples_leaf': 28}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:48:59,289] Trial 38 finished with value: 0.719699351503297 and parameters: {'n_estimators': 84, 'max_depth': 7, 'min_samples_leaf': 17}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 19:51:36,696] Trial 39 finished with value: 0.730249955501092 and parameters: {'n_estimators': 40, 'max_depth': 11, 'min_samples_leaf': 25}. Best is trial 31 with value: 0.7329188686234716.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-11 19:54:49,982] Trial 40 finished with value: 0.7286112442209363 and parameters: {'n_estimators': 54, 'max_depth': 10, 'min_samples_leaf': 10}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 20:01:42,638] Trial 41 finished with value: 0.7324203790072682 and parameters: {'n_estimators': 99, 'max_depth': 11, 'min_samples_leaf': 23}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 20:08:46,003] Trial 42 finished with value: 0.7323359559181566 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_leaf': 21}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 20:14:55,563] Trial 43 finished with value: 0.7325484517532441 and parameters: {'n_estimators': 90, 'max_depth': 11, 'min_samples_leaf': 25}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 20:21:08,831] Trial 44 finished with value: 0.7324385320712982 and parameters: {'n_estimators': 95, 'max_depth': 11, 'min_samples_leaf': 27}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 20:26:59,376] Trial 45 finished with value: 0.730372190902889 and parameters: {'n_estimators': 90, 'max_depth': 10, 'min_samples_leaf': 22}. Best is trial 31 with value: 0.7329188686234716.\n",
      "[I 2024-09-11 20:34:23,447] Trial 46 finished with value: 0.7329509165042678 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_leaf': 31}. Best is trial 46 with value: 0.7329509165042678.\n",
      "[I 2024-09-11 20:39:33,872] Trial 47 finished with value: 0.7236480165540318 and parameters: {'n_estimators': 94, 'max_depth': 8, 'min_samples_leaf': 30}. Best is trial 46 with value: 0.7329509165042678.\n",
      "[I 2024-09-11 20:41:12,434] Trial 48 finished with value: 0.7268599444533801 and parameters: {'n_estimators': 23, 'max_depth': 10, 'min_samples_leaf': 31}. Best is trial 46 with value: 0.7329509165042678.\n",
      "[I 2024-09-11 20:47:22,789] Trial 49 finished with value: 0.7322082872655202 and parameters: {'n_estimators': 85, 'max_depth': 11, 'min_samples_leaf': 29}. Best is trial 46 with value: 0.7329509165042678.\n",
      "[I 2024-09-11 20:48:52,013] Trial 50 finished with value: 0.7165525796793673 and parameters: {'n_estimators': 32, 'max_depth': 7, 'min_samples_leaf': 27}. Best is trial 46 with value: 0.7329509165042678.\n",
      "[I 2024-09-11 20:55:53,589] Trial 51 finished with value: 0.7331885720816428 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_leaf': 24}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 21:02:38,182] Trial 52 finished with value: 0.7329509931845393 and parameters: {'n_estimators': 96, 'max_depth': 11, 'min_samples_leaf': 24}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 21:09:25,481] Trial 53 finished with value: 0.7330203345195991 and parameters: {'n_estimators': 96, 'max_depth': 11, 'min_samples_leaf': 26}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 21:34:12,399] Trial 54 finished with value: 0.732252477666486 and parameters: {'n_estimators': 96, 'max_depth': 11, 'min_samples_leaf': 22}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 21:40:18,297] Trial 55 finished with value: 0.7298872085691044 and parameters: {'n_estimators': 97, 'max_depth': 10, 'min_samples_leaf': 29}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 21:46:23,917] Trial 56 finished with value: 0.7328143188844967 and parameters: {'n_estimators': 88, 'max_depth': 11, 'min_samples_leaf': 26}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 21:51:41,338] Trial 57 finished with value: 0.7299321704652351 and parameters: {'n_estimators': 90, 'max_depth': 10, 'min_samples_leaf': 31}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:00:54,813] Trial 58 finished with value: 0.7268945627114336 and parameters: {'n_estimators': 97, 'max_depth': 9, 'min_samples_leaf': 27}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:06:05,653] Trial 59 finished with value: 0.7317005729753162 and parameters: {'n_estimators': 81, 'max_depth': 11, 'min_samples_leaf': 23}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:25:28,852] Trial 60 finished with value: 0.732075486862673 and parameters: {'n_estimators': 91, 'max_depth': 11, 'min_samples_leaf': 20}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:31:14,242] Trial 61 finished with value: 0.7328569940947586 and parameters: {'n_estimators': 87, 'max_depth': 11, 'min_samples_leaf': 26}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:36:56,782] Trial 62 finished with value: 0.7326381628669019 and parameters: {'n_estimators': 88, 'max_depth': 11, 'min_samples_leaf': 26}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:43:28,118] Trial 63 finished with value: 0.7321951793349354 and parameters: {'n_estimators': 100, 'max_depth': 11, 'min_samples_leaf': 24}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:49:05,548] Trial 64 finished with value: 0.7302461937330161 and parameters: {'n_estimators': 95, 'max_depth': 10, 'min_samples_leaf': 28}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 22:54:51,609] Trial 65 finished with value: 0.733185067704144 and parameters: {'n_estimators': 87, 'max_depth': 11, 'min_samples_leaf': 30}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 23:01:14,136] Trial 66 finished with value: 0.7327476127805851 and parameters: {'n_estimators': 97, 'max_depth': 11, 'min_samples_leaf': 30}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 23:04:42,806] Trial 67 finished with value: 0.7121576337165568 and parameters: {'n_estimators': 93, 'max_depth': 6, 'min_samples_leaf': 29}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 23:10:09,377] Trial 68 finished with value: 0.7326297569223774 and parameters: {'n_estimators': 82, 'max_depth': 11, 'min_samples_leaf': 31}. Best is trial 51 with value: 0.7331885720816428.\n",
      "[I 2024-09-11 23:15:36,372] Trial 69 finished with value: 0.7301065774730833 and parameters: {'n_estimators': 86, 'max_depth': 10, 'min_samples_leaf': 27}. Best is trial 51 with value: 0.7331885720816428.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5h 52min 11s\n",
      "Wall time: 6h 28min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 20, 100)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 11)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf',  2, 31)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n",
    "    score = cross_val_score(model, X_train[new_columns], y_train, cv=2, scoring='roc_auc').mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=70)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7542475988164754\n",
      "test:  0.7324634480010748\n",
      "CPU times: total: 7min 51s\n",
      "Wall time: 7min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                           max_depth=best_params['max_depth'],\n",
    "                           min_samples_leaf=best_params['min_samples_leaf']).fit(X_train, y_train)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, rf.predict_proba(X_train)[:,1]))\n",
    "print('test: ', roc_auc_score(y_test, rf.predict_proba(X_test)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.71161\tvalidation_1-auc:0.70195\n",
      "[10]\tvalidation_0-auc:0.73547\tvalidation_1-auc:0.72135\n",
      "[20]\tvalidation_0-auc:0.74660\tvalidation_1-auc:0.72781\n",
      "[30]\tvalidation_0-auc:0.76035\tvalidation_1-auc:0.73324\n",
      "[40]\tvalidation_0-auc:0.77237\tvalidation_1-auc:0.73815\n",
      "[50]\tvalidation_0-auc:0.78398\tvalidation_1-auc:0.74251\n",
      "[60]\tvalidation_0-auc:0.79339\tvalidation_1-auc:0.74554\n",
      "[70]\tvalidation_0-auc:0.80181\tvalidation_1-auc:0.74823\n",
      "[80]\tvalidation_0-auc:0.80941\tvalidation_1-auc:0.75005\n",
      "[90]\tvalidation_0-auc:0.81582\tvalidation_1-auc:0.75161\n",
      "[100]\tvalidation_0-auc:0.82144\tvalidation_1-auc:0.75294\n",
      "[110]\tvalidation_0-auc:0.82651\tvalidation_1-auc:0.75386\n",
      "[120]\tvalidation_0-auc:0.83110\tvalidation_1-auc:0.75472\n",
      "[130]\tvalidation_0-auc:0.83484\tvalidation_1-auc:0.75529\n",
      "[140]\tvalidation_0-auc:0.83869\tvalidation_1-auc:0.75573\n",
      "[150]\tvalidation_0-auc:0.84231\tvalidation_1-auc:0.75611\n",
      "[160]\tvalidation_0-auc:0.84549\tvalidation_1-auc:0.75644\n",
      "[170]\tvalidation_0-auc:0.84832\tvalidation_1-auc:0.75679\n",
      "[180]\tvalidation_0-auc:0.85111\tvalidation_1-auc:0.75703\n",
      "[190]\tvalidation_0-auc:0.85323\tvalidation_1-auc:0.75729\n",
      "[200]\tvalidation_0-auc:0.85526\tvalidation_1-auc:0.75742\n",
      "[210]\tvalidation_0-auc:0.85701\tvalidation_1-auc:0.75747\n",
      "[220]\tvalidation_0-auc:0.85885\tvalidation_1-auc:0.75767\n",
      "[230]\tvalidation_0-auc:0.86067\tvalidation_1-auc:0.75773\n",
      "[240]\tvalidation_0-auc:0.86259\tvalidation_1-auc:0.75774\n",
      "[242]\tvalidation_0-auc:0.86287\tvalidation_1-auc:0.75777\n",
      "CPU times: total: 1h 10min 21s\n",
      "Wall time: 10min 11s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=10,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=350, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=10,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=350, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=10,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=350, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = xgboost.XGBClassifier(\n",
    "    n_estimators=350,\n",
    "    objective='binary:logistic',\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df = pd.DataFrame({'features': X_train.columns.tolist(), 'imp': model.feature_importances_.tolist()})\n",
    "new_columns = imp_df.loc[imp_df['imp']!=0, 'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-13 22:29:55,672] A new study created in memory with name: no-name-0b88ef18-7c64-48a9-a5e2-0e927d9c1465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.69741\tvalidation_1-auc:0.68559\n",
      "[10]\tvalidation_0-auc:0.76287\tvalidation_1-auc:0.73285\n",
      "[20]\tvalidation_0-auc:0.78629\tvalidation_1-auc:0.74156\n",
      "[30]\tvalidation_0-auc:0.80561\tvalidation_1-auc:0.74610\n",
      "[40]\tvalidation_0-auc:0.81854\tvalidation_1-auc:0.74818\n",
      "[50]\tvalidation_0-auc:0.82935\tvalidation_1-auc:0.74913\n",
      "[60]\tvalidation_0-auc:0.83774\tvalidation_1-auc:0.74974\n",
      "[70]\tvalidation_0-auc:0.84544\tvalidation_1-auc:0.74987\n",
      "[75]\tvalidation_0-auc:0.84864\tvalidation_1-auc:0.74991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-13 22:36:59,482] Trial 0 finished with value: 0.749911128942446 and parameters: {'n_estimators': 190, 'max_depth': 10, 'learning_rate': 0.161511876567625, 'subsample': 0.8244521823185775, 'colsample_bytree': 0.5015345900285135, 'colsample_bylevel': 0.8370947456605976, 'colsample_bynode': 0.9172872713445984}. Best is trial 0 with value: 0.749911128942446.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.69360\tvalidation_1-auc:0.68299\n",
      "[10]\tvalidation_0-auc:0.74967\tvalidation_1-auc:0.72870\n",
      "[20]\tvalidation_0-auc:0.75692\tvalidation_1-auc:0.73338\n",
      "[30]\tvalidation_0-auc:0.76555\tvalidation_1-auc:0.73696\n",
      "[40]\tvalidation_0-auc:0.77280\tvalidation_1-auc:0.73914\n",
      "[50]\tvalidation_0-auc:0.77985\tvalidation_1-auc:0.74175\n",
      "[60]\tvalidation_0-auc:0.78668\tvalidation_1-auc:0.74395\n",
      "[70]\tvalidation_0-auc:0.79295\tvalidation_1-auc:0.74579\n",
      "[80]\tvalidation_0-auc:0.79892\tvalidation_1-auc:0.74764\n",
      "[90]\tvalidation_0-auc:0.80414\tvalidation_1-auc:0.74925\n",
      "[100]\tvalidation_0-auc:0.80946\tvalidation_1-auc:0.75062\n",
      "[110]\tvalidation_0-auc:0.81499\tvalidation_1-auc:0.75189\n",
      "[120]\tvalidation_0-auc:0.81877\tvalidation_1-auc:0.75293\n",
      "[130]\tvalidation_0-auc:0.82217\tvalidation_1-auc:0.75350\n",
      "[140]\tvalidation_0-auc:0.82526\tvalidation_1-auc:0.75423\n",
      "[150]\tvalidation_0-auc:0.82909\tvalidation_1-auc:0.75491\n",
      "[159]\tvalidation_0-auc:0.83212\tvalidation_1-auc:0.75544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-13 22:50:55,369] Trial 1 finished with value: 0.7554409473833255 and parameters: {'n_estimators': 160, 'max_depth': 10, 'learning_rate': 0.046878702823603756, 'subsample': 0.9267854122866171, 'colsample_bytree': 0.7107403313497744, 'colsample_bylevel': 0.846544027693068, 'colsample_bynode': 0.45921348458229183}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.66996\tvalidation_1-auc:0.66385\n",
      "[10]\tvalidation_0-auc:0.72450\tvalidation_1-auc:0.71488\n",
      "[20]\tvalidation_0-auc:0.73494\tvalidation_1-auc:0.72408\n",
      "[30]\tvalidation_0-auc:0.73865\tvalidation_1-auc:0.72659\n",
      "[40]\tvalidation_0-auc:0.74025\tvalidation_1-auc:0.72767\n",
      "[50]\tvalidation_0-auc:0.74182\tvalidation_1-auc:0.72870\n",
      "[60]\tvalidation_0-auc:0.74309\tvalidation_1-auc:0.72971\n",
      "[70]\tvalidation_0-auc:0.74414\tvalidation_1-auc:0.73046\n",
      "[80]\tvalidation_0-auc:0.74512\tvalidation_1-auc:0.73115\n",
      "[90]\tvalidation_0-auc:0.74618\tvalidation_1-auc:0.73181\n",
      "[100]\tvalidation_0-auc:0.74749\tvalidation_1-auc:0.73268\n",
      "[110]\tvalidation_0-auc:0.74865\tvalidation_1-auc:0.73339\n",
      "[120]\tvalidation_0-auc:0.74957\tvalidation_1-auc:0.73368\n",
      "[130]\tvalidation_0-auc:0.75074\tvalidation_1-auc:0.73429\n",
      "[140]\tvalidation_0-auc:0.75213\tvalidation_1-auc:0.73526\n",
      "[150]\tvalidation_0-auc:0.75279\tvalidation_1-auc:0.73551\n",
      "[160]\tvalidation_0-auc:0.75411\tvalidation_1-auc:0.73638\n",
      "[170]\tvalidation_0-auc:0.75493\tvalidation_1-auc:0.73681\n",
      "[180]\tvalidation_0-auc:0.75624\tvalidation_1-auc:0.73761\n",
      "[189]\tvalidation_0-auc:0.75734\tvalidation_1-auc:0.73837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-13 23:07:51,413] Trial 2 finished with value: 0.7383663207764164 and parameters: {'n_estimators': 190, 'max_depth': 9, 'learning_rate': 0.013067613328025893, 'subsample': 0.5092608335872872, 'colsample_bytree': 0.3803120244084715, 'colsample_bylevel': 0.6064094599015112, 'colsample_bynode': 0.21803840867137184}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.68870\tvalidation_1-auc:0.67687\n",
      "[10]\tvalidation_0-auc:0.75133\tvalidation_1-auc:0.72563\n",
      "[20]\tvalidation_0-auc:0.76785\tvalidation_1-auc:0.73389\n",
      "[30]\tvalidation_0-auc:0.78207\tvalidation_1-auc:0.73923\n",
      "[40]\tvalidation_0-auc:0.79375\tvalidation_1-auc:0.74312\n",
      "[50]\tvalidation_0-auc:0.80391\tvalidation_1-auc:0.74555\n",
      "[60]\tvalidation_0-auc:0.81109\tvalidation_1-auc:0.74740\n",
      "[70]\tvalidation_0-auc:0.81792\tvalidation_1-auc:0.74887\n",
      "[80]\tvalidation_0-auc:0.82362\tvalidation_1-auc:0.75003\n",
      "[90]\tvalidation_0-auc:0.82859\tvalidation_1-auc:0.75086\n",
      "[100]\tvalidation_0-auc:0.83309\tvalidation_1-auc:0.75169\n",
      "[110]\tvalidation_0-auc:0.83701\tvalidation_1-auc:0.75214\n",
      "[120]\tvalidation_0-auc:0.84095\tvalidation_1-auc:0.75238\n",
      "[130]\tvalidation_0-auc:0.84447\tvalidation_1-auc:0.75253\n",
      "[140]\tvalidation_0-auc:0.84721\tvalidation_1-auc:0.75271\n",
      "[150]\tvalidation_0-auc:0.84946\tvalidation_1-auc:0.75268\n",
      "[160]\tvalidation_0-auc:0.85314\tvalidation_1-auc:0.75275\n",
      "[170]\tvalidation_0-auc:0.85578\tvalidation_1-auc:0.75289\n",
      "[180]\tvalidation_0-auc:0.85855\tvalidation_1-auc:0.75290\n",
      "[186]\tvalidation_0-auc:0.86013\tvalidation_1-auc:0.75274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-13 23:25:09,809] Trial 3 finished with value: 0.7527734735202671 and parameters: {'n_estimators': 220, 'max_depth': 11, 'learning_rate': 0.09420033417226022, 'subsample': 0.601098228514666, 'colsample_bytree': 0.4201858265709551, 'colsample_bylevel': 0.6193211547251812, 'colsample_bynode': 0.23774715457784554}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.63286\tvalidation_1-auc:0.63060\n",
      "[10]\tvalidation_0-auc:0.70063\tvalidation_1-auc:0.69928\n",
      "[20]\tvalidation_0-auc:0.71606\tvalidation_1-auc:0.71332\n",
      "[30]\tvalidation_0-auc:0.72509\tvalidation_1-auc:0.72197\n",
      "[40]\tvalidation_0-auc:0.72955\tvalidation_1-auc:0.72577\n",
      "[50]\tvalidation_0-auc:0.73308\tvalidation_1-auc:0.72878\n",
      "[60]\tvalidation_0-auc:0.73606\tvalidation_1-auc:0.73130\n",
      "[70]\tvalidation_0-auc:0.73867\tvalidation_1-auc:0.73367\n",
      "[80]\tvalidation_0-auc:0.74129\tvalidation_1-auc:0.73601\n",
      "[90]\tvalidation_0-auc:0.74341\tvalidation_1-auc:0.73782\n",
      "[100]\tvalidation_0-auc:0.74518\tvalidation_1-auc:0.73925\n",
      "[110]\tvalidation_0-auc:0.74684\tvalidation_1-auc:0.74059\n",
      "[120]\tvalidation_0-auc:0.74804\tvalidation_1-auc:0.74156\n",
      "[130]\tvalidation_0-auc:0.74925\tvalidation_1-auc:0.74257\n",
      "[140]\tvalidation_0-auc:0.75049\tvalidation_1-auc:0.74350\n",
      "[150]\tvalidation_0-auc:0.75153\tvalidation_1-auc:0.74423\n",
      "[160]\tvalidation_0-auc:0.75255\tvalidation_1-auc:0.74505\n",
      "[170]\tvalidation_0-auc:0.75349\tvalidation_1-auc:0.74578\n",
      "[180]\tvalidation_0-auc:0.75445\tvalidation_1-auc:0.74646\n",
      "[189]\tvalidation_0-auc:0.75520\tvalidation_1-auc:0.74706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-13 23:40:59,636] Trial 4 finished with value: 0.7470636251202062 and parameters: {'n_estimators': 190, 'max_depth': 5, 'learning_rate': 0.06718629030332496, 'subsample': 0.5787635966997519, 'colsample_bytree': 0.8225846376399627, 'colsample_bylevel': 0.10348178613983762, 'colsample_bynode': 0.6581477718013764}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.63716\tvalidation_1-auc:0.63338\n",
      "[10]\tvalidation_0-auc:0.73108\tvalidation_1-auc:0.71562\n",
      "[20]\tvalidation_0-auc:0.75733\tvalidation_1-auc:0.73330\n",
      "[30]\tvalidation_0-auc:0.76818\tvalidation_1-auc:0.73845\n",
      "[40]\tvalidation_0-auc:0.77757\tvalidation_1-auc:0.74222\n",
      "[50]\tvalidation_0-auc:0.78519\tvalidation_1-auc:0.74394\n",
      "[60]\tvalidation_0-auc:0.79081\tvalidation_1-auc:0.74515\n",
      "[70]\tvalidation_0-auc:0.79637\tvalidation_1-auc:0.74565\n",
      "[80]\tvalidation_0-auc:0.80108\tvalidation_1-auc:0.74617\n",
      "[90]\tvalidation_0-auc:0.80493\tvalidation_1-auc:0.74667\n",
      "[100]\tvalidation_0-auc:0.80820\tvalidation_1-auc:0.74674\n",
      "[107]\tvalidation_0-auc:0.81083\tvalidation_1-auc:0.74666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-13 23:50:49,057] Trial 5 finished with value: 0.7466607261972841 and parameters: {'n_estimators': 210, 'max_depth': 9, 'learning_rate': 0.2435175028152844, 'subsample': 0.8794098756440447, 'colsample_bytree': 0.2732135458112331, 'colsample_bylevel': 0.46516252217531684, 'colsample_bynode': 0.18843038735724102}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.64919\tvalidation_1-auc:0.64565\n",
      "[10]\tvalidation_0-auc:0.72086\tvalidation_1-auc:0.71625\n",
      "[20]\tvalidation_0-auc:0.73278\tvalidation_1-auc:0.72598\n",
      "[30]\tvalidation_0-auc:0.74240\tvalidation_1-auc:0.73358\n",
      "[40]\tvalidation_0-auc:0.74825\tvalidation_1-auc:0.73779\n",
      "[50]\tvalidation_0-auc:0.75251\tvalidation_1-auc:0.74057\n",
      "[60]\tvalidation_0-auc:0.75670\tvalidation_1-auc:0.74313\n",
      "[70]\tvalidation_0-auc:0.75994\tvalidation_1-auc:0.74489\n",
      "[80]\tvalidation_0-auc:0.76261\tvalidation_1-auc:0.74634\n",
      "[90]\tvalidation_0-auc:0.76486\tvalidation_1-auc:0.74729\n",
      "[100]\tvalidation_0-auc:0.76704\tvalidation_1-auc:0.74805\n",
      "[110]\tvalidation_0-auc:0.76926\tvalidation_1-auc:0.74891\n",
      "[120]\tvalidation_0-auc:0.77133\tvalidation_1-auc:0.74961\n",
      "[130]\tvalidation_0-auc:0.77301\tvalidation_1-auc:0.75016\n",
      "[140]\tvalidation_0-auc:0.77464\tvalidation_1-auc:0.75052\n",
      "[150]\tvalidation_0-auc:0.77630\tvalidation_1-auc:0.75091\n",
      "[160]\tvalidation_0-auc:0.77745\tvalidation_1-auc:0.75137\n",
      "[169]\tvalidation_0-auc:0.77883\tvalidation_1-auc:0.75170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 00:05:49,343] Trial 6 finished with value: 0.751702767417999 and parameters: {'n_estimators': 170, 'max_depth': 7, 'learning_rate': 0.10095050004303552, 'subsample': 0.5032070357071157, 'colsample_bytree': 0.3870675411206709, 'colsample_bylevel': 0.1832787951997413, 'colsample_bynode': 0.8427943211422522}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.70027\tvalidation_1-auc:0.68642\n",
      "[10]\tvalidation_0-auc:0.77927\tvalidation_1-auc:0.73422\n",
      "[20]\tvalidation_0-auc:0.81304\tvalidation_1-auc:0.74114\n",
      "[30]\tvalidation_0-auc:0.83179\tvalidation_1-auc:0.74291\n",
      "[40]\tvalidation_0-auc:0.84381\tvalidation_1-auc:0.74342\n",
      "[46]\tvalidation_0-auc:0.85092\tvalidation_1-auc:0.74322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 00:10:23,128] Trial 7 finished with value: 0.7433450834241371 and parameters: {'n_estimators': 200, 'max_depth': 11, 'learning_rate': 0.21824388144235177, 'subsample': 0.6952138908553469, 'colsample_bytree': 0.5405704329713454, 'colsample_bylevel': 0.5963356327595103, 'colsample_bynode': 0.7160353242775659}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.69994\tvalidation_1-auc:0.68695\n",
      "[10]\tvalidation_0-auc:0.74336\tvalidation_1-auc:0.72605\n",
      "[20]\tvalidation_0-auc:0.74922\tvalidation_1-auc:0.72976\n",
      "[30]\tvalidation_0-auc:0.75187\tvalidation_1-auc:0.73098\n",
      "[40]\tvalidation_0-auc:0.75397\tvalidation_1-auc:0.73214\n",
      "[50]\tvalidation_0-auc:0.75677\tvalidation_1-auc:0.73339\n",
      "[60]\tvalidation_0-auc:0.76025\tvalidation_1-auc:0.73484\n",
      "[70]\tvalidation_0-auc:0.76358\tvalidation_1-auc:0.73618\n",
      "[80]\tvalidation_0-auc:0.76676\tvalidation_1-auc:0.73724\n",
      "[90]\tvalidation_0-auc:0.77000\tvalidation_1-auc:0.73821\n",
      "[100]\tvalidation_0-auc:0.77345\tvalidation_1-auc:0.73926\n",
      "[110]\tvalidation_0-auc:0.77689\tvalidation_1-auc:0.74029\n",
      "[120]\tvalidation_0-auc:0.78028\tvalidation_1-auc:0.74156\n",
      "[130]\tvalidation_0-auc:0.78380\tvalidation_1-auc:0.74262\n",
      "[140]\tvalidation_0-auc:0.78705\tvalidation_1-auc:0.74373\n",
      "[150]\tvalidation_0-auc:0.78968\tvalidation_1-auc:0.74455\n",
      "[160]\tvalidation_0-auc:0.79254\tvalidation_1-auc:0.74545\n",
      "[170]\tvalidation_0-auc:0.79578\tvalidation_1-auc:0.74638\n",
      "[180]\tvalidation_0-auc:0.79835\tvalidation_1-auc:0.74714\n",
      "[190]\tvalidation_0-auc:0.80093\tvalidation_1-auc:0.74793\n",
      "[199]\tvalidation_0-auc:0.80329\tvalidation_1-auc:0.74865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 00:28:19,445] Trial 8 finished with value: 0.7486532773825357 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.019327839011844195, 'subsample': 0.8117759432052736, 'colsample_bytree': 0.712728504367715, 'colsample_bylevel': 0.6840642588902938, 'colsample_bynode': 0.9275178855885574}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.57745\tvalidation_1-auc:0.57413\n",
      "[10]\tvalidation_0-auc:0.65544\tvalidation_1-auc:0.64767\n",
      "[20]\tvalidation_0-auc:0.68236\tvalidation_1-auc:0.67531\n",
      "[30]\tvalidation_0-auc:0.69521\tvalidation_1-auc:0.68701\n",
      "[40]\tvalidation_0-auc:0.70276\tvalidation_1-auc:0.69319\n",
      "[50]\tvalidation_0-auc:0.71119\tvalidation_1-auc:0.70050\n",
      "[60]\tvalidation_0-auc:0.71815\tvalidation_1-auc:0.70704\n",
      "[70]\tvalidation_0-auc:0.72217\tvalidation_1-auc:0.71035\n",
      "[80]\tvalidation_0-auc:0.72754\tvalidation_1-auc:0.71525\n",
      "[90]\tvalidation_0-auc:0.73154\tvalidation_1-auc:0.71843\n",
      "[100]\tvalidation_0-auc:0.73407\tvalidation_1-auc:0.72048\n",
      "[110]\tvalidation_0-auc:0.73620\tvalidation_1-auc:0.72208\n",
      "[120]\tvalidation_0-auc:0.73790\tvalidation_1-auc:0.72305\n",
      "[130]\tvalidation_0-auc:0.73949\tvalidation_1-auc:0.72396\n",
      "[140]\tvalidation_0-auc:0.74111\tvalidation_1-auc:0.72498\n",
      "[150]\tvalidation_0-auc:0.74373\tvalidation_1-auc:0.72696\n",
      "[160]\tvalidation_0-auc:0.74463\tvalidation_1-auc:0.72762\n",
      "[170]\tvalidation_0-auc:0.74630\tvalidation_1-auc:0.72887\n",
      "[179]\tvalidation_0-auc:0.74773\tvalidation_1-auc:0.72971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 00:43:52,642] Trial 9 finished with value: 0.7297092478598769 and parameters: {'n_estimators': 180, 'max_depth': 10, 'learning_rate': 0.10131514714060086, 'subsample': 0.7187977352286066, 'colsample_bytree': 0.07207256114305227, 'colsample_bylevel': 0.223829551864628, 'colsample_bynode': 0.33909394461262443}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.67140\tvalidation_1-auc:0.66568\n",
      "[10]\tvalidation_0-auc:0.74228\tvalidation_1-auc:0.73125\n",
      "[20]\tvalidation_0-auc:0.76150\tvalidation_1-auc:0.74240\n",
      "[30]\tvalidation_0-auc:0.77211\tvalidation_1-auc:0.74648\n",
      "[40]\tvalidation_0-auc:0.77919\tvalidation_1-auc:0.74839\n",
      "[50]\tvalidation_0-auc:0.78504\tvalidation_1-auc:0.74933\n",
      "[60]\tvalidation_0-auc:0.78931\tvalidation_1-auc:0.74975\n",
      "[70]\tvalidation_0-auc:0.79310\tvalidation_1-auc:0.74973\n",
      "[75]\tvalidation_0-auc:0.79540\tvalidation_1-auc:0.74969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 00:50:41,657] Trial 10 finished with value: 0.7496927655549788 and parameters: {'n_estimators': 270, 'max_depth': 7, 'learning_rate': 0.29753062810780206, 'subsample': 0.9782752859192283, 'colsample_bytree': 0.9632833688443286, 'colsample_bylevel': 0.9954376700999024, 'colsample_bynode': 0.46132630431942045}. Best is trial 1 with value: 0.7554409473833255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.66984\tvalidation_1-auc:0.65442\n",
      "[10]\tvalidation_0-auc:0.75072\tvalidation_1-auc:0.72521\n",
      "[20]\tvalidation_0-auc:0.76320\tvalidation_1-auc:0.73187\n",
      "[30]\tvalidation_0-auc:0.77231\tvalidation_1-auc:0.73569\n",
      "[40]\tvalidation_0-auc:0.77920\tvalidation_1-auc:0.73810\n",
      "[50]\tvalidation_0-auc:0.78778\tvalidation_1-auc:0.74162\n",
      "[60]\tvalidation_0-auc:0.79614\tvalidation_1-auc:0.74423\n",
      "[70]\tvalidation_0-auc:0.80203\tvalidation_1-auc:0.74628\n",
      "[80]\tvalidation_0-auc:0.80840\tvalidation_1-auc:0.74792\n",
      "[90]\tvalidation_0-auc:0.81325\tvalidation_1-auc:0.74929\n",
      "[100]\tvalidation_0-auc:0.81867\tvalidation_1-auc:0.75067\n",
      "[110]\tvalidation_0-auc:0.82346\tvalidation_1-auc:0.75174\n",
      "[120]\tvalidation_0-auc:0.82710\tvalidation_1-auc:0.75241\n",
      "[130]\tvalidation_0-auc:0.83113\tvalidation_1-auc:0.75296\n",
      "[140]\tvalidation_0-auc:0.83478\tvalidation_1-auc:0.75350\n",
      "[150]\tvalidation_0-auc:0.83857\tvalidation_1-auc:0.75403\n",
      "[160]\tvalidation_0-auc:0.84128\tvalidation_1-auc:0.75450\n",
      "[170]\tvalidation_0-auc:0.84428\tvalidation_1-auc:0.75483\n",
      "[180]\tvalidation_0-auc:0.84665\tvalidation_1-auc:0.75506\n",
      "[190]\tvalidation_0-auc:0.84878\tvalidation_1-auc:0.75517\n",
      "[200]\tvalidation_0-auc:0.85098\tvalidation_1-auc:0.75535\n",
      "[210]\tvalidation_0-auc:0.85330\tvalidation_1-auc:0.75544\n",
      "[220]\tvalidation_0-auc:0.85541\tvalidation_1-auc:0.75560\n",
      "[230]\tvalidation_0-auc:0.85752\tvalidation_1-auc:0.75576\n",
      "[240]\tvalidation_0-auc:0.85947\tvalidation_1-auc:0.75577\n",
      "[249]\tvalidation_0-auc:0.86096\tvalidation_1-auc:0.75589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 01:13:36,554] Trial 11 finished with value: 0.7558939908887452 and parameters: {'n_estimators': 250, 'max_depth': 11, 'learning_rate': 0.06735579460360834, 'subsample': 0.9996402635396491, 'colsample_bytree': 0.7236008349889372, 'colsample_bylevel': 0.8190023295381147, 'colsample_bynode': 0.057800614822374946}. Best is trial 11 with value: 0.7558939908887452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.63773\tvalidation_1-auc:0.63109\n",
      "[10]\tvalidation_0-auc:0.71971\tvalidation_1-auc:0.70249\n",
      "[20]\tvalidation_0-auc:0.73152\tvalidation_1-auc:0.71183\n",
      "[30]\tvalidation_0-auc:0.74117\tvalidation_1-auc:0.71810\n",
      "[40]\tvalidation_0-auc:0.74893\tvalidation_1-auc:0.72310\n",
      "[50]\tvalidation_0-auc:0.75485\tvalidation_1-auc:0.72634\n",
      "[60]\tvalidation_0-auc:0.75985\tvalidation_1-auc:0.72927\n",
      "[70]\tvalidation_0-auc:0.76518\tvalidation_1-auc:0.73234\n",
      "[80]\tvalidation_0-auc:0.77065\tvalidation_1-auc:0.73470\n",
      "[90]\tvalidation_0-auc:0.77542\tvalidation_1-auc:0.73723\n",
      "[100]\tvalidation_0-auc:0.77877\tvalidation_1-auc:0.73813\n",
      "[110]\tvalidation_0-auc:0.78229\tvalidation_1-auc:0.73957\n",
      "[120]\tvalidation_0-auc:0.78544\tvalidation_1-auc:0.74100\n",
      "[130]\tvalidation_0-auc:0.78790\tvalidation_1-auc:0.74213\n",
      "[140]\tvalidation_0-auc:0.79044\tvalidation_1-auc:0.74298\n",
      "[150]\tvalidation_0-auc:0.79265\tvalidation_1-auc:0.74369\n",
      "[160]\tvalidation_0-auc:0.79530\tvalidation_1-auc:0.74465\n",
      "[170]\tvalidation_0-auc:0.79716\tvalidation_1-auc:0.74538\n",
      "[180]\tvalidation_0-auc:0.79902\tvalidation_1-auc:0.74608\n",
      "[190]\tvalidation_0-auc:0.80112\tvalidation_1-auc:0.74680\n",
      "[200]\tvalidation_0-auc:0.80326\tvalidation_1-auc:0.74750\n",
      "[210]\tvalidation_0-auc:0.80529\tvalidation_1-auc:0.74807\n",
      "[220]\tvalidation_0-auc:0.80712\tvalidation_1-auc:0.74857\n",
      "[230]\tvalidation_0-auc:0.80862\tvalidation_1-auc:0.74894\n",
      "[240]\tvalidation_0-auc:0.81041\tvalidation_1-auc:0.74940\n",
      "[250]\tvalidation_0-auc:0.81217\tvalidation_1-auc:0.74987\n",
      "[259]\tvalidation_0-auc:0.81362\tvalidation_1-auc:0.75023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 01:36:54,130] Trial 12 finished with value: 0.7502280850653281 and parameters: {'n_estimators': 260, 'max_depth': 11, 'learning_rate': 0.05640185779446525, 'subsample': 0.9944909031712248, 'colsample_bytree': 0.6776236673090396, 'colsample_bylevel': 0.8827522623879193, 'colsample_bynode': 0.017688367745230582}. Best is trial 11 with value: 0.7558939908887452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.63267\tvalidation_1-auc:0.63197\n",
      "[10]\tvalidation_0-auc:0.71383\tvalidation_1-auc:0.70550\n",
      "[20]\tvalidation_0-auc:0.73264\tvalidation_1-auc:0.72151\n",
      "[30]\tvalidation_0-auc:0.74440\tvalidation_1-auc:0.72963\n",
      "[40]\tvalidation_0-auc:0.75240\tvalidation_1-auc:0.73495\n",
      "[50]\tvalidation_0-auc:0.75700\tvalidation_1-auc:0.73795\n",
      "[60]\tvalidation_0-auc:0.76197\tvalidation_1-auc:0.74086\n",
      "[70]\tvalidation_0-auc:0.76632\tvalidation_1-auc:0.74264\n",
      "[80]\tvalidation_0-auc:0.77015\tvalidation_1-auc:0.74427\n",
      "[90]\tvalidation_0-auc:0.77310\tvalidation_1-auc:0.74542\n",
      "[100]\tvalidation_0-auc:0.77632\tvalidation_1-auc:0.74663\n",
      "[110]\tvalidation_0-auc:0.77885\tvalidation_1-auc:0.74749\n",
      "[120]\tvalidation_0-auc:0.78114\tvalidation_1-auc:0.74813\n",
      "[130]\tvalidation_0-auc:0.78335\tvalidation_1-auc:0.74869\n",
      "[140]\tvalidation_0-auc:0.78525\tvalidation_1-auc:0.74902\n",
      "[150]\tvalidation_0-auc:0.78709\tvalidation_1-auc:0.74946\n",
      "[160]\tvalidation_0-auc:0.78883\tvalidation_1-auc:0.74985\n",
      "[170]\tvalidation_0-auc:0.79057\tvalidation_1-auc:0.74999\n",
      "[180]\tvalidation_0-auc:0.79207\tvalidation_1-auc:0.75026\n",
      "[190]\tvalidation_0-auc:0.79338\tvalidation_1-auc:0.75048\n",
      "[200]\tvalidation_0-auc:0.79512\tvalidation_1-auc:0.75059\n",
      "[210]\tvalidation_0-auc:0.79656\tvalidation_1-auc:0.75082\n",
      "[220]\tvalidation_0-auc:0.79786\tvalidation_1-auc:0.75104\n",
      "[230]\tvalidation_0-auc:0.79902\tvalidation_1-auc:0.75123\n",
      "[240]\tvalidation_0-auc:0.80032\tvalidation_1-auc:0.75130\n",
      "[250]\tvalidation_0-auc:0.80149\tvalidation_1-auc:0.75153\n",
      "[260]\tvalidation_0-auc:0.80266\tvalidation_1-auc:0.75162\n",
      "[270]\tvalidation_0-auc:0.80378\tvalidation_1-auc:0.75165\n",
      "[280]\tvalidation_0-auc:0.80470\tvalidation_1-auc:0.75175\n",
      "[290]\tvalidation_0-auc:0.80595\tvalidation_1-auc:0.75183\n",
      "[299]\tvalidation_0-auc:0.80688\tvalidation_1-auc:0.75194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 02:01:46,877] Trial 13 finished with value: 0.7519365109733452 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.13302191064342322, 'subsample': 0.9172185080925574, 'colsample_bytree': 0.8890473214771155, 'colsample_bylevel': 0.8191845990140474, 'colsample_bynode': 0.025885989778272944}. Best is trial 11 with value: 0.7558939908887452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.67407\tvalidation_1-auc:0.66483\n",
      "[10]\tvalidation_0-auc:0.74591\tvalidation_1-auc:0.72899\n",
      "[20]\tvalidation_0-auc:0.76642\tvalidation_1-auc:0.73911\n",
      "[30]\tvalidation_0-auc:0.78099\tvalidation_1-auc:0.74484\n",
      "[40]\tvalidation_0-auc:0.79158\tvalidation_1-auc:0.74773\n",
      "[50]\tvalidation_0-auc:0.79803\tvalidation_1-auc:0.74940\n",
      "[60]\tvalidation_0-auc:0.80473\tvalidation_1-auc:0.75046\n",
      "[70]\tvalidation_0-auc:0.81060\tvalidation_1-auc:0.75134\n",
      "[80]\tvalidation_0-auc:0.81526\tvalidation_1-auc:0.75191\n",
      "[90]\tvalidation_0-auc:0.81961\tvalidation_1-auc:0.75223\n",
      "[100]\tvalidation_0-auc:0.82341\tvalidation_1-auc:0.75234\n",
      "[110]\tvalidation_0-auc:0.82624\tvalidation_1-auc:0.75237\n",
      "[120]\tvalidation_0-auc:0.82945\tvalidation_1-auc:0.75245\n",
      "[124]\tvalidation_0-auc:0.83041\tvalidation_1-auc:0.75237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-14 02:12:56,637] Trial 14 finished with value: 0.7524526954073771 and parameters: {'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.16239787908094344, 'subsample': 0.9192736087570007, 'colsample_bytree': 0.687626537858815, 'colsample_bylevel': 0.3293514152505958, 'colsample_bynode': 0.52113603660481}. Best is trial 11 with value: 0.7558939908887452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.70708\tvalidation_1-auc:0.68798\n",
      "[10]\tvalidation_0-auc:0.76079\tvalidation_1-auc:0.73204\n",
      "[20]\tvalidation_0-auc:0.76847\tvalidation_1-auc:0.73500\n",
      "[30]\tvalidation_0-auc:0.77999\tvalidation_1-auc:0.73886\n",
      "[40]\tvalidation_0-auc:0.78965\tvalidation_1-auc:0.74164\n",
      "[50]\tvalidation_0-auc:0.79926\tvalidation_1-auc:0.74397\n",
      "[60]\tvalidation_0-auc:0.80868\tvalidation_1-auc:0.74638\n",
      "[70]\tvalidation_0-auc:0.81744\tvalidation_1-auc:0.74834\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 150, 300, step=10)\n",
    "    max_depth = trial.suggest_int('max_depth', 5, 11)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1)\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.01, 1)\n",
    "    colsample_bylevel = trial.suggest_float('colsample_bylevel', 0.01, 1)\n",
    "    colsample_bynode = trial.suggest_float('colsample_bynode', 0.01, 1)\n",
    "    \n",
    "\n",
    "    model = xgboost.XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, \n",
    "                                  subsample=subsample, colsample_bytree=colsample_bytree, colsample_bylevel=colsample_bylevel,\n",
    "                                  colsample_bynode=colsample_bynode, objective='binary:logistic', \n",
    "                                  eval_metric='auc', early_stopping_rounds=10).fit(X_train[new_columns], y_train, \n",
    "                                  eval_set=[(X_train[new_columns], y_train), (X_test[new_columns], y_test)], verbose=10)\n",
    "    metric = model.evals_result()['validation_1']['auc'][-1]\n",
    "    \n",
    "    return metric\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_score = study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.68944\tvalidation_1-auc:0.67947\n",
      "[10]\tvalidation_0-auc:0.74558\tvalidation_1-auc:0.72982\n",
      "[20]\tvalidation_0-auc:0.75246\tvalidation_1-auc:0.73360\n",
      "[30]\tvalidation_0-auc:0.76233\tvalidation_1-auc:0.73783\n",
      "[40]\tvalidation_0-auc:0.77015\tvalidation_1-auc:0.74088\n",
      "[50]\tvalidation_0-auc:0.77798\tvalidation_1-auc:0.74360\n",
      "[60]\tvalidation_0-auc:0.78524\tvalidation_1-auc:0.74582\n",
      "[70]\tvalidation_0-auc:0.79241\tvalidation_1-auc:0.74801\n",
      "[80]\tvalidation_0-auc:0.79854\tvalidation_1-auc:0.74990\n",
      "[90]\tvalidation_0-auc:0.80395\tvalidation_1-auc:0.75143\n",
      "[100]\tvalidation_0-auc:0.80898\tvalidation_1-auc:0.75267\n",
      "[110]\tvalidation_0-auc:0.81413\tvalidation_1-auc:0.75389\n",
      "[120]\tvalidation_0-auc:0.81846\tvalidation_1-auc:0.75464\n",
      "[130]\tvalidation_0-auc:0.82218\tvalidation_1-auc:0.75545\n",
      "[140]\tvalidation_0-auc:0.82596\tvalidation_1-auc:0.75622\n",
      "[150]\tvalidation_0-auc:0.82918\tvalidation_1-auc:0.75681\n",
      "[160]\tvalidation_0-auc:0.83211\tvalidation_1-auc:0.75712\n",
      "[170]\tvalidation_0-auc:0.83482\tvalidation_1-auc:0.75751\n",
      "[180]\tvalidation_0-auc:0.83790\tvalidation_1-auc:0.75789\n",
      "[190]\tvalidation_0-auc:0.83987\tvalidation_1-auc:0.75818\n",
      "[200]\tvalidation_0-auc:0.84247\tvalidation_1-auc:0.75847\n",
      "[210]\tvalidation_0-auc:0.84484\tvalidation_1-auc:0.75858\n",
      "[220]\tvalidation_0-auc:0.84704\tvalidation_1-auc:0.75877\n",
      "[230]\tvalidation_0-auc:0.84906\tvalidation_1-auc:0.75897\n",
      "[240]\tvalidation_0-auc:0.85075\tvalidation_1-auc:0.75916\n",
      "[250]\tvalidation_0-auc:0.85247\tvalidation_1-auc:0.75923\n",
      "[260]\tvalidation_0-auc:0.85411\tvalidation_1-auc:0.75939\n",
      "[270]\tvalidation_0-auc:0.85572\tvalidation_1-auc:0.75947\n",
      "[280]\tvalidation_0-auc:0.85754\tvalidation_1-auc:0.75957\n",
      "[290]\tvalidation_0-auc:0.85937\tvalidation_1-auc:0.75970\n",
      "[300]\tvalidation_0-auc:0.86094\tvalidation_1-auc:0.75975\n",
      "[310]\tvalidation_0-auc:0.86227\tvalidation_1-auc:0.75981\n",
      "[320]\tvalidation_0-auc:0.86362\tvalidation_1-auc:0.75990\n",
      "[330]\tvalidation_0-auc:0.86485\tvalidation_1-auc:0.75995\n",
      "[340]\tvalidation_0-auc:0.86583\tvalidation_1-auc:0.75999\n",
      "[349]\tvalidation_0-auc:0.86674\tvalidation_1-auc:0.75993\n",
      "CPU times: total: 3h 36min 44s\n",
      "Wall time: 32min 6s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.8465, colsample_bynode=0.4592,\n",
       "              colsample_bytree=0.7107, device=None, early_stopping_rounds=10,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.047, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=350, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;XGBClassifier<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.8465, colsample_bynode=0.4592,\n",
       "              colsample_bytree=0.7107, device=None, early_stopping_rounds=10,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.047, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=350, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=0.8465, colsample_bynode=0.4592,\n",
       "              colsample_bytree=0.7107, device=None, early_stopping_rounds=10,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.047, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=350, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=None, ...)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = xgboost.XGBClassifier(\n",
    "    n_estimators=350,\n",
    "    objective='binary:logistic',\n",
    "    max_depth=10,\n",
    "    learning_rate=0.047,\n",
    "    subsample=0.9268,\n",
    "    colsample_bytree=0.7107,\n",
    "    colsample_bylevel=0.8465,\n",
    "    colsample_bynode=0.4592,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "model.fit(X_train[new_columns], y_train, eval_set=[(X_train[new_columns], y_train), (X_test[new_columns], y_test)], verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ad2f2085b74124bd1f6cf796cb7425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5136239\tbest: 0.5136239 (0)\ttotal: 316ms\tremaining: 1m 2s\n",
      "20:\ttest: 0.7268109\tbest: 0.7268109 (20)\ttotal: 11s\tremaining: 1m 34s\n",
      "40:\ttest: 0.7377880\tbest: 0.7377880 (40)\ttotal: 21.7s\tremaining: 1m 24s\n",
      "60:\ttest: 0.7429433\tbest: 0.7429433 (60)\ttotal: 32.5s\tremaining: 1m 14s\n",
      "80:\ttest: 0.7452983\tbest: 0.7452983 (80)\ttotal: 42.8s\tremaining: 1m 2s\n",
      "100:\ttest: 0.7465679\tbest: 0.7465679 (100)\ttotal: 52.6s\tremaining: 51.6s\n",
      "120:\ttest: 0.7474535\tbest: 0.7474535 (120)\ttotal: 1m 2s\tremaining: 40.7s\n",
      "140:\ttest: 0.7483265\tbest: 0.7483265 (140)\ttotal: 1m 11s\tremaining: 30.1s\n",
      "160:\ttest: 0.7487045\tbest: 0.7487069 (158)\ttotal: 1m 21s\tremaining: 19.7s\n",
      "180:\ttest: 0.7490716\tbest: 0.7490716 (180)\ttotal: 1m 30s\tremaining: 9.5s\n",
      "199:\ttest: 0.7493597\tbest: 0.7493773 (197)\ttotal: 1m 40s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7493772559\n",
      "bestIteration = 197\n",
      "\n",
      "Shrink model to first 198 iterations.\n",
      "train:  0.7578590434922184\n",
      "test:  0.7493772559287459\n"
     ]
    }
   ],
   "source": [
    "boosting_model = catboost.CatBoostClassifier(n_estimators=200, learning_rate=0.3, eval_metric='AUC',\n",
    "                                             early_stopping_rounds=20, verbose=20, random_seed=42)\n",
    "boosting_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=20, plot=True)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, boosting_model.predict_proba(X_train)[:, 1]))\n",
    "print('test: ', roc_auc_score(y_test, boosting_model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063358a704a54a6497f81ea47d5f48e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 177ms\tremaining: 35.2s\n",
      "20:\ttest: 0.6941101\tbest: 0.6941101 (20)\ttotal: 4.86s\tremaining: 41.4s\n",
      "40:\ttest: 0.7078560\tbest: 0.7078560 (40)\ttotal: 10.6s\tremaining: 41.1s\n",
      "60:\ttest: 0.7169596\tbest: 0.7169596 (60)\ttotal: 16.5s\tremaining: 37.5s\n",
      "80:\ttest: 0.7235050\tbest: 0.7235050 (80)\ttotal: 22.7s\tremaining: 33.3s\n",
      "100:\ttest: 0.7275101\tbest: 0.7275101 (100)\ttotal: 29.3s\tremaining: 28.7s\n",
      "120:\ttest: 0.7309723\tbest: 0.7309723 (120)\ttotal: 36.3s\tremaining: 23.7s\n",
      "140:\ttest: 0.7341186\tbest: 0.7341186 (140)\ttotal: 43.2s\tremaining: 18.1s\n",
      "160:\ttest: 0.7363189\tbest: 0.7363189 (160)\ttotal: 49.5s\tremaining: 12s\n",
      "180:\ttest: 0.7382495\tbest: 0.7382495 (180)\ttotal: 56s\tremaining: 5.88s\n",
      "199:\ttest: 0.7393601\tbest: 0.7393601 (199)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7393601424\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "0:\tloss: 0.7393601\tbest: 0.7393601 (0)\ttotal: 1m 4s\tremaining: 33m 17s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7110566\tbest: 0.7110566 (20)\ttotal: 6.41s\tremaining: 54.6s\n",
      "40:\ttest: 0.7257202\tbest: 0.7257202 (40)\ttotal: 12.5s\tremaining: 48.5s\n",
      "60:\ttest: 0.7332879\tbest: 0.7332879 (60)\ttotal: 19.2s\tremaining: 43.7s\n",
      "80:\ttest: 0.7373864\tbest: 0.7373864 (80)\ttotal: 26s\tremaining: 38.2s\n",
      "100:\ttest: 0.7404107\tbest: 0.7404107 (100)\ttotal: 32.8s\tremaining: 32.1s\n",
      "120:\ttest: 0.7423518\tbest: 0.7423518 (120)\ttotal: 39.9s\tremaining: 26s\n",
      "140:\ttest: 0.7433657\tbest: 0.7433657 (140)\ttotal: 45.9s\tremaining: 19.2s\n",
      "160:\ttest: 0.7448501\tbest: 0.7448501 (160)\ttotal: 53s\tremaining: 12.8s\n",
      "180:\ttest: 0.7455125\tbest: 0.7455128 (179)\ttotal: 59.1s\tremaining: 6.21s\n",
      "199:\ttest: 0.7459531\tbest: 0.7459531 (199)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7459531324\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "1:\tloss: 0.7459531\tbest: 0.7459531 (1)\ttotal: 2m 9s\tremaining: 32m 21s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 235ms\tremaining: 46.8s\n",
      "20:\ttest: 0.7185768\tbest: 0.7185768 (20)\ttotal: 6.48s\tremaining: 55.3s\n",
      "40:\ttest: 0.7308254\tbest: 0.7308254 (40)\ttotal: 12.8s\tremaining: 49.5s\n",
      "60:\ttest: 0.7380665\tbest: 0.7380665 (60)\ttotal: 19.2s\tremaining: 43.7s\n",
      "80:\ttest: 0.7411564\tbest: 0.7411564 (80)\ttotal: 25.4s\tremaining: 37.3s\n",
      "100:\ttest: 0.7432658\tbest: 0.7432658 (100)\ttotal: 31.6s\tremaining: 31s\n",
      "120:\ttest: 0.7446442\tbest: 0.7446442 (120)\ttotal: 37.7s\tremaining: 24.6s\n",
      "140:\ttest: 0.7451662\tbest: 0.7451662 (140)\ttotal: 43.9s\tremaining: 18.4s\n",
      "160:\ttest: 0.7462863\tbest: 0.7463030 (158)\ttotal: 50.4s\tremaining: 12.2s\n",
      "180:\ttest: 0.7470859\tbest: 0.7470859 (180)\ttotal: 56.8s\tremaining: 5.96s\n",
      "199:\ttest: 0.7474472\tbest: 0.7474523 (197)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7474523159\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "2:\tloss: 0.7474523\tbest: 0.7474523 (2)\ttotal: 3m 12s\tremaining: 30m 58s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 231ms\tremaining: 46s\n",
      "20:\ttest: 0.7249919\tbest: 0.7249919 (20)\ttotal: 7.35s\tremaining: 1m 2s\n",
      "40:\ttest: 0.7353034\tbest: 0.7353034 (40)\ttotal: 14.1s\tremaining: 54.8s\n",
      "60:\ttest: 0.7392334\tbest: 0.7392334 (60)\ttotal: 20.4s\tremaining: 46.4s\n",
      "80:\ttest: 0.7430480\tbest: 0.7430480 (80)\ttotal: 27.6s\tremaining: 40.6s\n",
      "100:\ttest: 0.7444092\tbest: 0.7444092 (100)\ttotal: 34.8s\tremaining: 34.1s\n",
      "120:\ttest: 0.7458438\tbest: 0.7458438 (120)\ttotal: 40.8s\tremaining: 26.6s\n",
      "140:\ttest: 0.7468424\tbest: 0.7468424 (140)\ttotal: 46.9s\tremaining: 19.6s\n",
      "160:\ttest: 0.7474859\tbest: 0.7474935 (155)\ttotal: 53.1s\tremaining: 12.9s\n",
      "180:\ttest: 0.7480808\tbest: 0.7480857 (178)\ttotal: 59.3s\tremaining: 6.22s\n",
      "199:\ttest: 0.7482791\tbest: 0.7483154 (197)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7483154481\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "3:\tloss: 0.7483154\tbest: 0.7483154 (3)\ttotal: 4m 16s\tremaining: 29m 58s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 256ms\tremaining: 51s\n",
      "20:\ttest: 0.7008331\tbest: 0.7008331 (20)\ttotal: 6.49s\tremaining: 55.4s\n",
      "40:\ttest: 0.7134338\tbest: 0.7134338 (40)\ttotal: 13.2s\tremaining: 51s\n",
      "60:\ttest: 0.7215962\tbest: 0.7215962 (60)\ttotal: 19.5s\tremaining: 44.4s\n",
      "80:\ttest: 0.7282169\tbest: 0.7282169 (80)\ttotal: 26.3s\tremaining: 38.6s\n",
      "100:\ttest: 0.7326643\tbest: 0.7326643 (100)\ttotal: 33.2s\tremaining: 32.5s\n",
      "120:\ttest: 0.7352217\tbest: 0.7352217 (120)\ttotal: 39.6s\tremaining: 25.9s\n",
      "140:\ttest: 0.7377332\tbest: 0.7377332 (140)\ttotal: 46.2s\tremaining: 19.3s\n",
      "160:\ttest: 0.7396521\tbest: 0.7396521 (160)\ttotal: 53.1s\tremaining: 12.9s\n",
      "180:\ttest: 0.7410868\tbest: 0.7410868 (180)\ttotal: 1m\tremaining: 6.3s\n",
      "199:\ttest: 0.7421636\tbest: 0.7421636 (199)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7421636392\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "4:\tloss: 0.7421636\tbest: 0.7483154 (3)\ttotal: 5m 23s\tremaining: 29m 6s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 234ms\tremaining: 46.6s\n",
      "20:\ttest: 0.7153431\tbest: 0.7153431 (20)\ttotal: 6.69s\tremaining: 57.1s\n",
      "40:\ttest: 0.7301921\tbest: 0.7301921 (40)\ttotal: 13.9s\tremaining: 53.8s\n",
      "60:\ttest: 0.7363345\tbest: 0.7363345 (60)\ttotal: 20.9s\tremaining: 47.7s\n",
      "80:\ttest: 0.7401659\tbest: 0.7401856 (79)\ttotal: 28s\tremaining: 41.2s\n",
      "100:\ttest: 0.7423290\tbest: 0.7423290 (100)\ttotal: 34.6s\tremaining: 33.9s\n",
      "120:\ttest: 0.7440772\tbest: 0.7440772 (120)\ttotal: 41.4s\tremaining: 27s\n",
      "140:\ttest: 0.7451441\tbest: 0.7451441 (140)\ttotal: 48s\tremaining: 20.1s\n",
      "160:\ttest: 0.7460738\tbest: 0.7460738 (160)\ttotal: 54.6s\tremaining: 13.2s\n",
      "180:\ttest: 0.7466708\tbest: 0.7466708 (180)\ttotal: 1m 1s\tremaining: 6.42s\n",
      "199:\ttest: 0.7470675\tbest: 0.7470706 (198)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7470706135\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "5:\tloss: 0.7470706\tbest: 0.7483154 (3)\ttotal: 6m 30s\tremaining: 28m 12s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 238ms\tremaining: 47.3s\n",
      "20:\ttest: 0.7225821\tbest: 0.7225821 (20)\ttotal: 6.86s\tremaining: 58.5s\n",
      "40:\ttest: 0.7353872\tbest: 0.7353872 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7398937\tbest: 0.7398937 (60)\ttotal: 20.7s\tremaining: 47.2s\n",
      "80:\ttest: 0.7433685\tbest: 0.7433778 (78)\ttotal: 27.2s\tremaining: 39.9s\n",
      "100:\ttest: 0.7453004\tbest: 0.7453340 (99)\ttotal: 33.5s\tremaining: 32.8s\n",
      "120:\ttest: 0.7468075\tbest: 0.7468075 (120)\ttotal: 39.9s\tremaining: 26s\n",
      "140:\ttest: 0.7474972\tbest: 0.7475054 (139)\ttotal: 46s\tremaining: 19.3s\n",
      "160:\ttest: 0.7479784\tbest: 0.7479784 (160)\ttotal: 52.5s\tremaining: 12.7s\n",
      "180:\ttest: 0.7484241\tbest: 0.7484526 (179)\ttotal: 58.7s\tremaining: 6.16s\n",
      "199:\ttest: 0.7487468\tbest: 0.7487472 (198)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7487472057\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "6:\tloss: 0.7487472\tbest: 0.7487472 (6)\ttotal: 7m 35s\tremaining: 27m 6s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 239ms\tremaining: 47.5s\n",
      "20:\ttest: 0.7272551\tbest: 0.7272551 (20)\ttotal: 7.09s\tremaining: 1m\n",
      "40:\ttest: 0.7386984\tbest: 0.7386984 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7419438\tbest: 0.7419438 (60)\ttotal: 20.3s\tremaining: 46.4s\n",
      "80:\ttest: 0.7448588\tbest: 0.7448588 (80)\ttotal: 26.6s\tremaining: 39.1s\n",
      "100:\ttest: 0.7466908\tbest: 0.7466908 (100)\ttotal: 33.5s\tremaining: 32.9s\n",
      "120:\ttest: 0.7474343\tbest: 0.7474343 (120)\ttotal: 39.5s\tremaining: 25.8s\n",
      "140:\ttest: 0.7480092\tbest: 0.7480092 (140)\ttotal: 45.7s\tremaining: 19.1s\n",
      "160:\ttest: 0.7484148\tbest: 0.7484318 (159)\ttotal: 51.8s\tremaining: 12.5s\n",
      "180:\ttest: 0.7489792\tbest: 0.7489792 (180)\ttotal: 58.3s\tremaining: 6.12s\n",
      "199:\ttest: 0.7494230\tbest: 0.7494230 (199)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7494229948\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "7:\tloss: 0.7494230\tbest: 0.7494230 (7)\ttotal: 8m 40s\tremaining: 26m\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 231ms\tremaining: 45.9s\n",
      "20:\ttest: 0.7034370\tbest: 0.7034370 (20)\ttotal: 6.65s\tremaining: 56.7s\n",
      "40:\ttest: 0.7186187\tbest: 0.7186187 (40)\ttotal: 14.1s\tremaining: 54.7s\n",
      "60:\ttest: 0.7279600\tbest: 0.7279600 (60)\ttotal: 21.6s\tremaining: 49.3s\n",
      "80:\ttest: 0.7331008\tbest: 0.7331008 (80)\ttotal: 29.5s\tremaining: 43.3s\n",
      "100:\ttest: 0.7367479\tbest: 0.7367479 (100)\ttotal: 37.5s\tremaining: 36.7s\n",
      "120:\ttest: 0.7396022\tbest: 0.7396022 (120)\ttotal: 45.2s\tremaining: 29.5s\n",
      "140:\ttest: 0.7414634\tbest: 0.7414634 (140)\ttotal: 52.9s\tremaining: 22.1s\n",
      "160:\ttest: 0.7429885\tbest: 0.7429885 (160)\ttotal: 1m 2s\tremaining: 15.1s\n",
      "180:\ttest: 0.7442519\tbest: 0.7442519 (180)\ttotal: 1m 12s\tremaining: 7.58s\n",
      "199:\ttest: 0.7451130\tbest: 0.7451130 (199)\ttotal: 1m 19s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7451130219\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "8:\tloss: 0.7451130\tbest: 0.7494230 (7)\ttotal: 9m 59s\tremaining: 25m 33s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 243ms\tremaining: 48.3s\n",
      "20:\ttest: 0.7183954\tbest: 0.7183954 (20)\ttotal: 7.61s\tremaining: 1m 4s\n",
      "40:\ttest: 0.7324296\tbest: 0.7324296 (40)\ttotal: 15.9s\tremaining: 1m 1s\n",
      "60:\ttest: 0.7379777\tbest: 0.7379777 (60)\ttotal: 23.9s\tremaining: 54.5s\n",
      "80:\ttest: 0.7417054\tbest: 0.7417054 (80)\ttotal: 32.9s\tremaining: 48.3s\n",
      "100:\ttest: 0.7441019\tbest: 0.7441019 (100)\ttotal: 40.1s\tremaining: 39.3s\n",
      "120:\ttest: 0.7451301\tbest: 0.7451361 (119)\ttotal: 47.1s\tremaining: 30.7s\n",
      "140:\ttest: 0.7464575\tbest: 0.7464575 (140)\ttotal: 54.7s\tremaining: 22.9s\n",
      "160:\ttest: 0.7474590\tbest: 0.7474590 (160)\ttotal: 1m 2s\tremaining: 15s\n",
      "180:\ttest: 0.7476996\tbest: 0.7476996 (180)\ttotal: 1m 8s\tremaining: 7.2s\n",
      "199:\ttest: 0.7485463\tbest: 0.7485519 (198)\ttotal: 1m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7485518556\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "9:\tloss: 0.7485519\tbest: 0.7494230 (7)\ttotal: 11m 15s\tremaining: 24m 45s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 244ms\tremaining: 48.6s\n",
      "20:\ttest: 0.7266890\tbest: 0.7266890 (20)\ttotal: 7.72s\tremaining: 1m 5s\n",
      "40:\ttest: 0.7379538\tbest: 0.7379538 (40)\ttotal: 15.8s\tremaining: 1m 1s\n",
      "60:\ttest: 0.7432693\tbest: 0.7432693 (60)\ttotal: 23.2s\tremaining: 52.8s\n",
      "80:\ttest: 0.7452448\tbest: 0.7452633 (79)\ttotal: 30.2s\tremaining: 44.4s\n",
      "100:\ttest: 0.7466005\tbest: 0.7466005 (100)\ttotal: 36.9s\tremaining: 36.2s\n",
      "120:\ttest: 0.7477099\tbest: 0.7477099 (120)\ttotal: 44.1s\tremaining: 28.8s\n",
      "140:\ttest: 0.7484400\tbest: 0.7484400 (140)\ttotal: 50.6s\tremaining: 21.2s\n",
      "160:\ttest: 0.7486040\tbest: 0.7486128 (151)\ttotal: 57.1s\tremaining: 13.8s\n",
      "180:\ttest: 0.7489293\tbest: 0.7489661 (177)\ttotal: 1m 3s\tremaining: 6.69s\n",
      "199:\ttest: 0.7497762\tbest: 0.7497762 (199)\ttotal: 1m 11s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.749776223\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "10:\tloss: 0.7497762\tbest: 0.7497762 (10)\ttotal: 12m 26s\tremaining: 23m 45s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 317ms\tremaining: 1m 3s\n",
      "20:\ttest: 0.7329100\tbest: 0.7329100 (20)\ttotal: 9.23s\tremaining: 1m 18s\n",
      "40:\ttest: 0.7403687\tbest: 0.7403687 (40)\ttotal: 17.3s\tremaining: 1m 7s\n",
      "60:\ttest: 0.7442135\tbest: 0.7442135 (60)\ttotal: 25s\tremaining: 57.1s\n",
      "80:\ttest: 0.7458341\tbest: 0.7458341 (80)\ttotal: 31.9s\tremaining: 46.8s\n",
      "100:\ttest: 0.7469134\tbest: 0.7469134 (100)\ttotal: 38.9s\tremaining: 38.1s\n",
      "120:\ttest: 0.7477579\tbest: 0.7477579 (120)\ttotal: 45.9s\tremaining: 30s\n",
      "140:\ttest: 0.7482154\tbest: 0.7482154 (140)\ttotal: 52.4s\tremaining: 21.9s\n",
      "160:\ttest: 0.7487303\tbest: 0.7487768 (159)\ttotal: 58.8s\tremaining: 14.2s\n",
      "180:\ttest: 0.7493876\tbest: 0.7493876 (180)\ttotal: 1m 6s\tremaining: 6.97s\n",
      "199:\ttest: 0.7495236\tbest: 0.7495931 (195)\ttotal: 1m 13s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7495930754\n",
      "bestIteration = 195\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "11:\tloss: 0.7495931\tbest: 0.7497762 (10)\ttotal: 13m 40s\tremaining: 22m 47s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 298ms\tremaining: 59.4s\n",
      "20:\ttest: 0.7059916\tbest: 0.7059916 (20)\ttotal: 7.86s\tremaining: 1m 6s\n",
      "40:\ttest: 0.7231107\tbest: 0.7231107 (40)\ttotal: 17.1s\tremaining: 1m 6s\n",
      "60:\ttest: 0.7307839\tbest: 0.7307839 (60)\ttotal: 25s\tremaining: 57s\n",
      "80:\ttest: 0.7353967\tbest: 0.7353967 (80)\ttotal: 33.6s\tremaining: 49.4s\n",
      "100:\ttest: 0.7386240\tbest: 0.7386240 (100)\ttotal: 43.1s\tremaining: 42.3s\n",
      "120:\ttest: 0.7410789\tbest: 0.7410789 (120)\ttotal: 51.6s\tremaining: 33.7s\n",
      "140:\ttest: 0.7426301\tbest: 0.7426301 (140)\ttotal: 60s\tremaining: 25.1s\n",
      "160:\ttest: 0.7441156\tbest: 0.7441156 (160)\ttotal: 1m 8s\tremaining: 16.6s\n",
      "180:\ttest: 0.7452739\tbest: 0.7452739 (180)\ttotal: 1m 16s\tremaining: 8.07s\n",
      "199:\ttest: 0.7460072\tbest: 0.7460072 (199)\ttotal: 1m 24s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7460072366\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "12:\tloss: 0.7460072\tbest: 0.7497762 (10)\ttotal: 15m 4s\tremaining: 22m 2s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 291ms\tremaining: 58s\n",
      "20:\ttest: 0.7213168\tbest: 0.7213168 (20)\ttotal: 8.53s\tremaining: 1m 12s\n",
      "40:\ttest: 0.7355076\tbest: 0.7355076 (40)\ttotal: 17.2s\tremaining: 1m 6s\n",
      "60:\ttest: 0.7408197\tbest: 0.7408197 (60)\ttotal: 25.5s\tremaining: 58.1s\n",
      "80:\ttest: 0.7438093\tbest: 0.7438093 (80)\ttotal: 34.2s\tremaining: 50.2s\n",
      "100:\ttest: 0.7455383\tbest: 0.7455383 (100)\ttotal: 44.3s\tremaining: 43.4s\n",
      "120:\ttest: 0.7471059\tbest: 0.7471059 (120)\ttotal: 52.6s\tremaining: 34.4s\n",
      "140:\ttest: 0.7481037\tbest: 0.7481037 (140)\ttotal: 1m\tremaining: 25.3s\n",
      "160:\ttest: 0.7488365\tbest: 0.7488365 (160)\ttotal: 1m 8s\tremaining: 16.5s\n",
      "180:\ttest: 0.7495665\tbest: 0.7495665 (180)\ttotal: 1m 16s\tremaining: 7.99s\n",
      "199:\ttest: 0.7500661\tbest: 0.7500672 (198)\ttotal: 1m 23s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7500671645\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "13:\tloss: 0.7500672\tbest: 0.7500672 (13)\ttotal: 16m 28s\tremaining: 21m 10s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 297ms\tremaining: 59s\n",
      "20:\ttest: 0.7312267\tbest: 0.7312267 (20)\ttotal: 9.32s\tremaining: 1m 19s\n",
      "40:\ttest: 0.7398249\tbest: 0.7398249 (40)\ttotal: 17.3s\tremaining: 1m 7s\n",
      "60:\ttest: 0.7438871\tbest: 0.7438964 (59)\ttotal: 25.1s\tremaining: 57.3s\n",
      "80:\ttest: 0.7454393\tbest: 0.7454461 (78)\ttotal: 32.2s\tremaining: 47.2s\n",
      "100:\ttest: 0.7471704\tbest: 0.7472111 (99)\ttotal: 39.3s\tremaining: 38.5s\n",
      "120:\ttest: 0.7477966\tbest: 0.7477966 (120)\ttotal: 46s\tremaining: 30s\n",
      "140:\ttest: 0.7489173\tbest: 0.7489173 (140)\ttotal: 53.4s\tremaining: 22.3s\n",
      "160:\ttest: 0.7496694\tbest: 0.7496694 (160)\ttotal: 1m 2s\tremaining: 15.1s\n",
      "180:\ttest: 0.7498144\tbest: 0.7500010 (174)\ttotal: 1m 10s\tremaining: 7.39s\n",
      "199:\ttest: 0.7501471\tbest: 0.7501471 (199)\ttotal: 1m 17s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7501471068\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "14:\tloss: 0.7501471\tbest: 0.7501471 (14)\ttotal: 17m 45s\tremaining: 20m 8s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 301ms\tremaining: 60s\n",
      "20:\ttest: 0.7336760\tbest: 0.7336760 (20)\ttotal: 8.55s\tremaining: 1m 12s\n",
      "40:\ttest: 0.7421537\tbest: 0.7421537 (40)\ttotal: 16.5s\tremaining: 1m 3s\n",
      "60:\ttest: 0.7457007\tbest: 0.7457007 (60)\ttotal: 24s\tremaining: 54.7s\n",
      "80:\ttest: 0.7472166\tbest: 0.7472166 (80)\ttotal: 31s\tremaining: 45.5s\n",
      "100:\ttest: 0.7482485\tbest: 0.7482485 (100)\ttotal: 37.8s\tremaining: 37.1s\n",
      "120:\ttest: 0.7489985\tbest: 0.7490256 (119)\ttotal: 45.2s\tremaining: 29.5s\n",
      "140:\ttest: 0.7490803\tbest: 0.7491625 (129)\ttotal: 52.1s\tremaining: 21.8s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7491625186\n",
      "bestIteration = 129\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "15:\tloss: 0.7491625\tbest: 0.7501471 (14)\ttotal: 18m 41s\tremaining: 18m 41s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 223ms\tremaining: 44.3s\n",
      "20:\ttest: 0.6941101\tbest: 0.6941101 (20)\ttotal: 5.75s\tremaining: 49s\n",
      "40:\ttest: 0.7078560\tbest: 0.7078560 (40)\ttotal: 11.6s\tremaining: 45.1s\n",
      "60:\ttest: 0.7169596\tbest: 0.7169596 (60)\ttotal: 17.3s\tremaining: 39.5s\n",
      "80:\ttest: 0.7235050\tbest: 0.7235050 (80)\ttotal: 23.2s\tremaining: 34s\n",
      "100:\ttest: 0.7275101\tbest: 0.7275101 (100)\ttotal: 29s\tremaining: 28.4s\n",
      "120:\ttest: 0.7309723\tbest: 0.7309723 (120)\ttotal: 34.8s\tremaining: 22.7s\n",
      "140:\ttest: 0.7341186\tbest: 0.7341186 (140)\ttotal: 41.1s\tremaining: 17.2s\n",
      "160:\ttest: 0.7363189\tbest: 0.7363189 (160)\ttotal: 46.9s\tremaining: 11.4s\n",
      "180:\ttest: 0.7382495\tbest: 0.7382495 (180)\ttotal: 53s\tremaining: 5.56s\n",
      "199:\ttest: 0.7393601\tbest: 0.7393601 (199)\ttotal: 59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7393601424\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "16:\tloss: 0.7393601\tbest: 0.7501471 (14)\ttotal: 19m 40s\tremaining: 17m 21s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 223ms\tremaining: 44.4s\n",
      "20:\ttest: 0.7110566\tbest: 0.7110566 (20)\ttotal: 6.85s\tremaining: 58.4s\n",
      "40:\ttest: 0.7257202\tbest: 0.7257202 (40)\ttotal: 13.1s\tremaining: 50.6s\n",
      "60:\ttest: 0.7332879\tbest: 0.7332879 (60)\ttotal: 19.6s\tremaining: 44.7s\n",
      "80:\ttest: 0.7373864\tbest: 0.7373864 (80)\ttotal: 26.2s\tremaining: 38.5s\n",
      "100:\ttest: 0.7404107\tbest: 0.7404107 (100)\ttotal: 32.3s\tremaining: 31.6s\n",
      "120:\ttest: 0.7423518\tbest: 0.7423518 (120)\ttotal: 38.2s\tremaining: 25s\n",
      "140:\ttest: 0.7433657\tbest: 0.7433657 (140)\ttotal: 44.1s\tremaining: 18.5s\n",
      "160:\ttest: 0.7448501\tbest: 0.7448501 (160)\ttotal: 50.4s\tremaining: 12.2s\n",
      "180:\ttest: 0.7455125\tbest: 0.7455128 (179)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7459531\tbest: 0.7459531 (199)\ttotal: 1m 1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7459531324\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "17:\tloss: 0.7459531\tbest: 0.7501471 (14)\ttotal: 20m 41s\tremaining: 16m 5s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7185768\tbest: 0.7185768 (20)\ttotal: 6.28s\tremaining: 53.5s\n",
      "40:\ttest: 0.7308254\tbest: 0.7308254 (40)\ttotal: 12.4s\tremaining: 48.1s\n",
      "60:\ttest: 0.7380665\tbest: 0.7380665 (60)\ttotal: 18.7s\tremaining: 42.6s\n",
      "80:\ttest: 0.7411564\tbest: 0.7411564 (80)\ttotal: 25.1s\tremaining: 36.9s\n",
      "100:\ttest: 0.7432658\tbest: 0.7432658 (100)\ttotal: 31.1s\tremaining: 30.5s\n",
      "120:\ttest: 0.7446442\tbest: 0.7446442 (120)\ttotal: 36.9s\tremaining: 24.1s\n",
      "140:\ttest: 0.7451662\tbest: 0.7451662 (140)\ttotal: 42.8s\tremaining: 17.9s\n",
      "160:\ttest: 0.7462863\tbest: 0.7463030 (158)\ttotal: 49.5s\tremaining: 12s\n",
      "180:\ttest: 0.7470859\tbest: 0.7470859 (180)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7474472\tbest: 0.7474523 (197)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7474523159\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "18:\tloss: 0.7474523\tbest: 0.7501471 (14)\ttotal: 21m 44s\tremaining: 14m 52s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 252ms\tremaining: 50.2s\n",
      "20:\ttest: 0.7249919\tbest: 0.7249919 (20)\ttotal: 6.6s\tremaining: 56.3s\n",
      "40:\ttest: 0.7353034\tbest: 0.7353034 (40)\ttotal: 12.8s\tremaining: 49.7s\n",
      "60:\ttest: 0.7392334\tbest: 0.7392334 (60)\ttotal: 18.3s\tremaining: 41.7s\n",
      "80:\ttest: 0.7430480\tbest: 0.7430480 (80)\ttotal: 24.3s\tremaining: 35.8s\n",
      "100:\ttest: 0.7444092\tbest: 0.7444092 (100)\ttotal: 29.9s\tremaining: 29.3s\n",
      "120:\ttest: 0.7458438\tbest: 0.7458438 (120)\ttotal: 35.9s\tremaining: 23.4s\n",
      "140:\ttest: 0.7468424\tbest: 0.7468424 (140)\ttotal: 42s\tremaining: 17.6s\n",
      "160:\ttest: 0.7474859\tbest: 0.7474935 (155)\ttotal: 47.7s\tremaining: 11.6s\n",
      "180:\ttest: 0.7480808\tbest: 0.7480857 (178)\ttotal: 53.5s\tremaining: 5.61s\n",
      "199:\ttest: 0.7482791\tbest: 0.7483154 (197)\ttotal: 58.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7483154481\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "19:\tloss: 0.7483154\tbest: 0.7501471 (14)\ttotal: 22m 43s\tremaining: 13m 38s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7008331\tbest: 0.7008331 (20)\ttotal: 6.51s\tremaining: 55.5s\n",
      "40:\ttest: 0.7134338\tbest: 0.7134338 (40)\ttotal: 13.6s\tremaining: 52.6s\n",
      "60:\ttest: 0.7215962\tbest: 0.7215962 (60)\ttotal: 20s\tremaining: 45.5s\n",
      "80:\ttest: 0.7282169\tbest: 0.7282169 (80)\ttotal: 26.7s\tremaining: 39.2s\n",
      "100:\ttest: 0.7326643\tbest: 0.7326643 (100)\ttotal: 34s\tremaining: 33.3s\n",
      "120:\ttest: 0.7352217\tbest: 0.7352217 (120)\ttotal: 40.8s\tremaining: 26.6s\n",
      "140:\ttest: 0.7377332\tbest: 0.7377332 (140)\ttotal: 47.7s\tremaining: 20s\n",
      "160:\ttest: 0.7396521\tbest: 0.7396521 (160)\ttotal: 54.4s\tremaining: 13.2s\n",
      "180:\ttest: 0.7410868\tbest: 0.7410868 (180)\ttotal: 1m 1s\tremaining: 6.46s\n",
      "199:\ttest: 0.7421636\tbest: 0.7421636 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7421636392\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "20:\tloss: 0.7421636\tbest: 0.7501471 (14)\ttotal: 23m 51s\tremaining: 12m 29s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 233ms\tremaining: 46.3s\n",
      "20:\ttest: 0.7153431\tbest: 0.7153431 (20)\ttotal: 7.19s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7301921\tbest: 0.7301921 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7363345\tbest: 0.7363345 (60)\ttotal: 21.5s\tremaining: 49s\n",
      "80:\ttest: 0.7401659\tbest: 0.7401856 (79)\ttotal: 28.3s\tremaining: 41.6s\n",
      "100:\ttest: 0.7423290\tbest: 0.7423290 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7440772\tbest: 0.7440772 (120)\ttotal: 41.4s\tremaining: 27s\n",
      "140:\ttest: 0.7451441\tbest: 0.7451441 (140)\ttotal: 48.1s\tremaining: 20.1s\n",
      "160:\ttest: 0.7460738\tbest: 0.7460738 (160)\ttotal: 54.6s\tremaining: 13.2s\n",
      "180:\ttest: 0.7466708\tbest: 0.7466708 (180)\ttotal: 1m 1s\tremaining: 6.42s\n",
      "199:\ttest: 0.7470675\tbest: 0.7470706 (198)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7470706135\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "21:\tloss: 0.7470706\tbest: 0.7501471 (14)\ttotal: 24m 58s\tremaining: 11m 21s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 261ms\tremaining: 52s\n",
      "20:\ttest: 0.7225821\tbest: 0.7225821 (20)\ttotal: 6.78s\tremaining: 57.8s\n",
      "40:\ttest: 0.7353872\tbest: 0.7353872 (40)\ttotal: 13.5s\tremaining: 52.5s\n",
      "60:\ttest: 0.7398937\tbest: 0.7398937 (60)\ttotal: 20.3s\tremaining: 46.3s\n",
      "80:\ttest: 0.7433685\tbest: 0.7433778 (78)\ttotal: 27.1s\tremaining: 39.9s\n",
      "100:\ttest: 0.7453004\tbest: 0.7453340 (99)\ttotal: 33.6s\tremaining: 32.9s\n",
      "120:\ttest: 0.7468075\tbest: 0.7468075 (120)\ttotal: 40.1s\tremaining: 26.2s\n",
      "140:\ttest: 0.7474972\tbest: 0.7475054 (139)\ttotal: 46.5s\tremaining: 19.4s\n",
      "160:\ttest: 0.7479784\tbest: 0.7479784 (160)\ttotal: 52.9s\tremaining: 12.8s\n",
      "180:\ttest: 0.7484241\tbest: 0.7484526 (179)\ttotal: 59.3s\tremaining: 6.22s\n",
      "199:\ttest: 0.7487468\tbest: 0.7487472 (198)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7487472057\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "22:\tloss: 0.7487472\tbest: 0.7501471 (14)\ttotal: 26m 3s\tremaining: 10m 11s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 240ms\tremaining: 47.8s\n",
      "20:\ttest: 0.7272551\tbest: 0.7272551 (20)\ttotal: 7.2s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7386984\tbest: 0.7386984 (40)\ttotal: 14.6s\tremaining: 56.8s\n",
      "60:\ttest: 0.7419438\tbest: 0.7419438 (60)\ttotal: 22.5s\tremaining: 51.3s\n",
      "80:\ttest: 0.7448588\tbest: 0.7448588 (80)\ttotal: 29.3s\tremaining: 43s\n",
      "100:\ttest: 0.7466908\tbest: 0.7466908 (100)\ttotal: 36.4s\tremaining: 35.7s\n",
      "120:\ttest: 0.7474343\tbest: 0.7474343 (120)\ttotal: 42.5s\tremaining: 27.8s\n",
      "140:\ttest: 0.7480092\tbest: 0.7480092 (140)\ttotal: 49.1s\tremaining: 20.6s\n",
      "160:\ttest: 0.7484148\tbest: 0.7484318 (159)\ttotal: 55.3s\tremaining: 13.4s\n",
      "180:\ttest: 0.7489792\tbest: 0.7489792 (180)\ttotal: 1m 1s\tremaining: 6.48s\n",
      "199:\ttest: 0.7494230\tbest: 0.7494230 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7494229948\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "23:\tloss: 0.7494230\tbest: 0.7501471 (14)\ttotal: 27m 12s\tremaining: 9m 4s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7034370\tbest: 0.7034370 (20)\ttotal: 6.84s\tremaining: 58.3s\n",
      "40:\ttest: 0.7186187\tbest: 0.7186187 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7279600\tbest: 0.7279600 (60)\ttotal: 22.1s\tremaining: 50.5s\n",
      "80:\ttest: 0.7331008\tbest: 0.7331008 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7367479\tbest: 0.7367479 (100)\ttotal: 40.7s\tremaining: 39.9s\n",
      "120:\ttest: 0.7396022\tbest: 0.7396022 (120)\ttotal: 48.3s\tremaining: 31.6s\n",
      "140:\ttest: 0.7414634\tbest: 0.7414634 (140)\ttotal: 56.3s\tremaining: 23.6s\n",
      "160:\ttest: 0.7429885\tbest: 0.7429885 (160)\ttotal: 1m 3s\tremaining: 15.5s\n",
      "180:\ttest: 0.7442519\tbest: 0.7442519 (180)\ttotal: 1m 11s\tremaining: 7.5s\n",
      "199:\ttest: 0.7451130\tbest: 0.7451130 (199)\ttotal: 1m 18s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7451130219\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "24:\tloss: 0.7451130\tbest: 0.7501471 (14)\ttotal: 28m 30s\tremaining: 7m 59s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 243ms\tremaining: 48.3s\n",
      "20:\ttest: 0.7183954\tbest: 0.7183954 (20)\ttotal: 8.34s\tremaining: 1m 11s\n",
      "40:\ttest: 0.7324296\tbest: 0.7324296 (40)\ttotal: 16s\tremaining: 1m 2s\n",
      "60:\ttest: 0.7379777\tbest: 0.7379777 (60)\ttotal: 24.1s\tremaining: 55s\n",
      "80:\ttest: 0.7417054\tbest: 0.7417054 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7441019\tbest: 0.7441019 (100)\ttotal: 39.6s\tremaining: 38.8s\n",
      "120:\ttest: 0.7451301\tbest: 0.7451361 (119)\ttotal: 46.6s\tremaining: 30.4s\n",
      "140:\ttest: 0.7464575\tbest: 0.7464575 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7474590\tbest: 0.7474590 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7476996\tbest: 0.7476996 (180)\ttotal: 1m 8s\tremaining: 7.14s\n",
      "199:\ttest: 0.7485463\tbest: 0.7485519 (198)\ttotal: 1m 14s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7485518556\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "25:\tloss: 0.7485519\tbest: 0.7501471 (14)\ttotal: 29m 45s\tremaining: 6m 52s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 228ms\tremaining: 45.3s\n",
      "20:\ttest: 0.7266890\tbest: 0.7266890 (20)\ttotal: 7.23s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7379538\tbest: 0.7379538 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7432693\tbest: 0.7432693 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7452448\tbest: 0.7452633 (79)\ttotal: 28.1s\tremaining: 41.3s\n",
      "100:\ttest: 0.7466005\tbest: 0.7466005 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7477099\tbest: 0.7477099 (120)\ttotal: 41.2s\tremaining: 26.9s\n",
      "140:\ttest: 0.7484400\tbest: 0.7484400 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7486040\tbest: 0.7486128 (151)\ttotal: 53.6s\tremaining: 13s\n",
      "180:\ttest: 0.7489293\tbest: 0.7489661 (177)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7497762\tbest: 0.7497762 (199)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.749776223\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "26:\tloss: 0.7497762\tbest: 0.7501471 (14)\ttotal: 30m 51s\tremaining: 5m 42s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 221ms\tremaining: 44s\n",
      "20:\ttest: 0.7329100\tbest: 0.7329100 (20)\ttotal: 7.42s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7403687\tbest: 0.7403687 (40)\ttotal: 14.5s\tremaining: 56.3s\n",
      "60:\ttest: 0.7442135\tbest: 0.7442135 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7458341\tbest: 0.7458341 (80)\ttotal: 27.7s\tremaining: 40.7s\n",
      "100:\ttest: 0.7469134\tbest: 0.7469134 (100)\ttotal: 34.4s\tremaining: 33.7s\n",
      "120:\ttest: 0.7477579\tbest: 0.7477579 (120)\ttotal: 40.9s\tremaining: 26.7s\n",
      "140:\ttest: 0.7482154\tbest: 0.7482154 (140)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7487303\tbest: 0.7487768 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "180:\ttest: 0.7493876\tbest: 0.7493876 (180)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7495236\tbest: 0.7495931 (195)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7495930754\n",
      "bestIteration = 195\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "27:\tloss: 0.7495931\tbest: 0.7501471 (14)\ttotal: 31m 57s\tremaining: 4m 33s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 283ms\tremaining: 56.2s\n",
      "20:\ttest: 0.7059916\tbest: 0.7059916 (20)\ttotal: 6.93s\tremaining: 59s\n",
      "40:\ttest: 0.7231107\tbest: 0.7231107 (40)\ttotal: 14.9s\tremaining: 57.7s\n",
      "60:\ttest: 0.7307839\tbest: 0.7307839 (60)\ttotal: 22.6s\tremaining: 51.4s\n",
      "80:\ttest: 0.7353967\tbest: 0.7353967 (80)\ttotal: 30.3s\tremaining: 44.5s\n",
      "100:\ttest: 0.7386240\tbest: 0.7386240 (100)\ttotal: 38.2s\tremaining: 37.5s\n",
      "120:\ttest: 0.7410789\tbest: 0.7410789 (120)\ttotal: 46.2s\tremaining: 30.2s\n",
      "140:\ttest: 0.7426301\tbest: 0.7426301 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7441156\tbest: 0.7441156 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7452739\tbest: 0.7452739 (180)\ttotal: 1m 9s\tremaining: 7.26s\n",
      "199:\ttest: 0.7460072\tbest: 0.7460072 (199)\ttotal: 1m 16s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7460072366\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "28:\tloss: 0.7460072\tbest: 0.7501471 (14)\ttotal: 33m 13s\tremaining: 3m 26s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 286ms\tremaining: 56.8s\n",
      "20:\ttest: 0.7213168\tbest: 0.7213168 (20)\ttotal: 7.97s\tremaining: 1m 7s\n",
      "40:\ttest: 0.7355076\tbest: 0.7355076 (40)\ttotal: 15.7s\tremaining: 1m\n",
      "60:\ttest: 0.7408197\tbest: 0.7408197 (60)\ttotal: 23.7s\tremaining: 54s\n",
      "80:\ttest: 0.7438093\tbest: 0.7438093 (80)\ttotal: 33s\tremaining: 48.4s\n",
      "100:\ttest: 0.7455383\tbest: 0.7455383 (100)\ttotal: 40.4s\tremaining: 39.6s\n",
      "120:\ttest: 0.7471059\tbest: 0.7471059 (120)\ttotal: 47.8s\tremaining: 31.2s\n",
      "140:\ttest: 0.7481037\tbest: 0.7481037 (140)\ttotal: 54.9s\tremaining: 23s\n",
      "160:\ttest: 0.7488365\tbest: 0.7488365 (160)\ttotal: 1m 1s\tremaining: 15s\n",
      "180:\ttest: 0.7495665\tbest: 0.7495665 (180)\ttotal: 1m 8s\tremaining: 7.22s\n",
      "199:\ttest: 0.7500661\tbest: 0.7500672 (198)\ttotal: 1m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7500671645\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "29:\tloss: 0.7500672\tbest: 0.7501471 (14)\ttotal: 34m 29s\tremaining: 2m 17s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 280ms\tremaining: 55.7s\n",
      "20:\ttest: 0.7312267\tbest: 0.7312267 (20)\ttotal: 8.1s\tremaining: 1m 9s\n",
      "40:\ttest: 0.7398249\tbest: 0.7398249 (40)\ttotal: 16.3s\tremaining: 1m 3s\n",
      "60:\ttest: 0.7438871\tbest: 0.7438964 (59)\ttotal: 25.2s\tremaining: 57.4s\n",
      "80:\ttest: 0.7454393\tbest: 0.7454461 (78)\ttotal: 33.2s\tremaining: 48.7s\n",
      "100:\ttest: 0.7471704\tbest: 0.7472111 (99)\ttotal: 41.4s\tremaining: 40.5s\n",
      "120:\ttest: 0.7477966\tbest: 0.7477966 (120)\ttotal: 48.9s\tremaining: 31.9s\n",
      "140:\ttest: 0.7489173\tbest: 0.7489173 (140)\ttotal: 57.2s\tremaining: 23.9s\n",
      "160:\ttest: 0.7496694\tbest: 0.7496694 (160)\ttotal: 1m 5s\tremaining: 15.8s\n",
      "180:\ttest: 0.7498144\tbest: 0.7500010 (174)\ttotal: 1m 12s\tremaining: 7.66s\n",
      "199:\ttest: 0.7501471\tbest: 0.7501471 (199)\ttotal: 1m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7501471068\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "30:\tloss: 0.7501471\tbest: 0.7501471 (14)\ttotal: 35m 49s\tremaining: 1m 9s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 330ms\tremaining: 1m 5s\n",
      "20:\ttest: 0.7336760\tbest: 0.7336760 (20)\ttotal: 8.78s\tremaining: 1m 14s\n",
      "40:\ttest: 0.7421537\tbest: 0.7421537 (40)\ttotal: 16.9s\tremaining: 1m 5s\n",
      "60:\ttest: 0.7457007\tbest: 0.7457007 (60)\ttotal: 25s\tremaining: 57s\n",
      "80:\ttest: 0.7472166\tbest: 0.7472166 (80)\ttotal: 33.4s\tremaining: 49.1s\n",
      "100:\ttest: 0.7482485\tbest: 0.7482485 (100)\ttotal: 41.5s\tremaining: 40.6s\n",
      "120:\ttest: 0.7489985\tbest: 0.7490256 (119)\ttotal: 49.4s\tremaining: 32.2s\n",
      "140:\ttest: 0.7490803\tbest: 0.7491625 (129)\ttotal: 56.8s\tremaining: 23.7s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7491625186\n",
      "bestIteration = 129\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "31:\tloss: 0.7491625\tbest: 0.7501471 (14)\ttotal: 36m 49s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/3]\n",
      "0:\ttest: 0.5466240\tbest: 0.5466240 (0)\ttotal: 308ms\tremaining: 1m 1s\n",
      "20:\ttest: 0.7280062\tbest: 0.7280062 (20)\ttotal: 7.44s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7372597\tbest: 0.7372597 (40)\ttotal: 14.3s\tremaining: 55.6s\n",
      "60:\ttest: 0.7412573\tbest: 0.7412573 (60)\ttotal: 20.9s\tremaining: 47.5s\n",
      "80:\ttest: 0.7433852\tbest: 0.7433852 (80)\ttotal: 27.2s\tremaining: 40s\n",
      "100:\ttest: 0.7441864\tbest: 0.7441900 (99)\ttotal: 33.5s\tremaining: 32.8s\n",
      "120:\ttest: 0.7447557\tbest: 0.7448117 (117)\ttotal: 39.3s\tremaining: 25.7s\n",
      "140:\ttest: 0.7449382\tbest: 0.7450386 (133)\ttotal: 45.3s\tremaining: 18.9s\n",
      "160:\ttest: 0.7453059\tbest: 0.7453059 (160)\ttotal: 51.4s\tremaining: 12.5s\n",
      "180:\ttest: 0.7456439\tbest: 0.7456526 (176)\ttotal: 57.5s\tremaining: 6.03s\n",
      "199:\ttest: 0.7458717\tbest: 0.7458717 (199)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.745871653\n",
      "bestIteration = 199\n",
      "\n",
      "Training on fold [1/3]\n",
      "0:\ttest: 0.5030965\tbest: 0.5030965 (0)\ttotal: 208ms\tremaining: 41.4s\n",
      "20:\ttest: 0.7269786\tbest: 0.7269786 (20)\ttotal: 6.96s\tremaining: 59.3s\n",
      "40:\ttest: 0.7359736\tbest: 0.7359736 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7395899\tbest: 0.7395899 (60)\ttotal: 20.7s\tremaining: 47.1s\n",
      "80:\ttest: 0.7424179\tbest: 0.7424200 (79)\ttotal: 27.4s\tremaining: 40.2s\n",
      "100:\ttest: 0.7438677\tbest: 0.7438677 (100)\ttotal: 33.8s\tremaining: 33.1s\n",
      "120:\ttest: 0.7449566\tbest: 0.7449770 (117)\ttotal: 40.6s\tremaining: 26.5s\n",
      "140:\ttest: 0.7450859\tbest: 0.7451233 (135)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7453141\tbest: 0.7454346 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "\n",
      "bestTest = 0.7454346082\n",
      "bestIteration = 159\n",
      "\n",
      "Training on fold [2/3]\n",
      "0:\ttest: 0.5472850\tbest: 0.5472850 (0)\ttotal: 283ms\tremaining: 56.3s\n",
      "20:\ttest: 0.7259946\tbest: 0.7259946 (20)\ttotal: 7.25s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7372018\tbest: 0.7372018 (40)\ttotal: 14s\tremaining: 54.1s\n",
      "60:\ttest: 0.7409509\tbest: 0.7409509 (60)\ttotal: 20.2s\tremaining: 46.1s\n",
      "80:\ttest: 0.7430899\tbest: 0.7430904 (79)\ttotal: 26.3s\tremaining: 38.7s\n",
      "100:\ttest: 0.7441000\tbest: 0.7441192 (99)\ttotal: 32.6s\tremaining: 32s\n",
      "120:\ttest: 0.7449599\tbest: 0.7449599 (120)\ttotal: 40.4s\tremaining: 26.4s\n",
      "140:\ttest: 0.7455849\tbest: 0.7455849 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7460361\tbest: 0.7460840 (157)\ttotal: 53.2s\tremaining: 12.9s\n",
      "180:\ttest: 0.7466474\tbest: 0.7466474 (180)\ttotal: 59.4s\tremaining: 6.23s\n",
      "199:\ttest: 0.7467573\tbest: 0.7468616 (186)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7468616149\n",
      "bestIteration = 186\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'min_data_in_leaf': 3,\n",
       "  'depth': 7,\n",
       "  'learning_rate': 0.30000000000000004},\n",
       " 'cv_results': defaultdict(list,\n",
       "             {'iterations': [0,\n",
       "               1,\n",
       "               2,\n",
       "               3,\n",
       "               4,\n",
       "               5,\n",
       "               6,\n",
       "               7,\n",
       "               8,\n",
       "               9,\n",
       "               10,\n",
       "               11,\n",
       "               12,\n",
       "               13,\n",
       "               14,\n",
       "               15,\n",
       "               16,\n",
       "               17,\n",
       "               18,\n",
       "               19,\n",
       "               20,\n",
       "               21,\n",
       "               22,\n",
       "               23,\n",
       "               24,\n",
       "               25,\n",
       "               26,\n",
       "               27,\n",
       "               28,\n",
       "               29,\n",
       "               30,\n",
       "               31,\n",
       "               32,\n",
       "               33,\n",
       "               34,\n",
       "               35,\n",
       "               36,\n",
       "               37,\n",
       "               38,\n",
       "               39,\n",
       "               40,\n",
       "               41,\n",
       "               42,\n",
       "               43,\n",
       "               44,\n",
       "               45,\n",
       "               46,\n",
       "               47,\n",
       "               48,\n",
       "               49,\n",
       "               50,\n",
       "               51,\n",
       "               52,\n",
       "               53,\n",
       "               54,\n",
       "               55,\n",
       "               56,\n",
       "               57,\n",
       "               58,\n",
       "               59,\n",
       "               60,\n",
       "               61,\n",
       "               62,\n",
       "               63,\n",
       "               64,\n",
       "               65,\n",
       "               66,\n",
       "               67,\n",
       "               68,\n",
       "               69,\n",
       "               70,\n",
       "               71,\n",
       "               72,\n",
       "               73,\n",
       "               74,\n",
       "               75,\n",
       "               76,\n",
       "               77,\n",
       "               78,\n",
       "               79,\n",
       "               80,\n",
       "               81,\n",
       "               82,\n",
       "               83,\n",
       "               84,\n",
       "               85,\n",
       "               86,\n",
       "               87,\n",
       "               88,\n",
       "               89,\n",
       "               90,\n",
       "               91,\n",
       "               92,\n",
       "               93,\n",
       "               94,\n",
       "               95,\n",
       "               96,\n",
       "               97,\n",
       "               98,\n",
       "               99,\n",
       "               100,\n",
       "               101,\n",
       "               102,\n",
       "               103,\n",
       "               104,\n",
       "               105,\n",
       "               106,\n",
       "               107,\n",
       "               108,\n",
       "               109,\n",
       "               110,\n",
       "               111,\n",
       "               112,\n",
       "               113,\n",
       "               114,\n",
       "               115,\n",
       "               116,\n",
       "               117,\n",
       "               118,\n",
       "               119,\n",
       "               120,\n",
       "               121,\n",
       "               122,\n",
       "               123,\n",
       "               124,\n",
       "               125,\n",
       "               126,\n",
       "               127,\n",
       "               128,\n",
       "               129,\n",
       "               130,\n",
       "               131,\n",
       "               132,\n",
       "               133,\n",
       "               134,\n",
       "               135,\n",
       "               136,\n",
       "               137,\n",
       "               138,\n",
       "               139,\n",
       "               140,\n",
       "               141,\n",
       "               142,\n",
       "               143,\n",
       "               144,\n",
       "               145,\n",
       "               146,\n",
       "               147,\n",
       "               148,\n",
       "               149,\n",
       "               150,\n",
       "               151,\n",
       "               152,\n",
       "               153,\n",
       "               154,\n",
       "               155,\n",
       "               156,\n",
       "               157,\n",
       "               158,\n",
       "               159,\n",
       "               160,\n",
       "               161,\n",
       "               162,\n",
       "               163,\n",
       "               164,\n",
       "               165,\n",
       "               166,\n",
       "               167,\n",
       "               168,\n",
       "               169,\n",
       "               170,\n",
       "               171,\n",
       "               172,\n",
       "               173,\n",
       "               174,\n",
       "               175,\n",
       "               176,\n",
       "               177,\n",
       "               178,\n",
       "               179,\n",
       "               180,\n",
       "               181,\n",
       "               182,\n",
       "               183,\n",
       "               184,\n",
       "               185,\n",
       "               186,\n",
       "               187,\n",
       "               188,\n",
       "               189,\n",
       "               190,\n",
       "               191,\n",
       "               192,\n",
       "               193,\n",
       "               194,\n",
       "               195,\n",
       "               196,\n",
       "               197,\n",
       "               198,\n",
       "               199],\n",
       "              'test-AUC-mean': [0.5323351871134413,\n",
       "               0.5672485377872802,\n",
       "               0.6073729499291639,\n",
       "               0.6682448148498139,\n",
       "               0.6815865549484652,\n",
       "               0.6905379693067895,\n",
       "               0.697254815238835,\n",
       "               0.6989171525505835,\n",
       "               0.7034855810236008,\n",
       "               0.7081957164191247,\n",
       "               0.712240618753471,\n",
       "               0.7139975265978536,\n",
       "               0.7159752865593644,\n",
       "               0.7175685424543378,\n",
       "               0.7192737969143583,\n",
       "               0.7205048017282761,\n",
       "               0.7221750988457073,\n",
       "               0.7236253837036376,\n",
       "               0.724682731134001,\n",
       "               0.7257887194849176,\n",
       "               0.7269931493303302,\n",
       "               0.7279065089832814,\n",
       "               0.7288224791887279,\n",
       "               0.7293681792534473,\n",
       "               0.7300194439278632,\n",
       "               0.7309084709246666,\n",
       "               0.7314423107118962,\n",
       "               0.7318070239971508,\n",
       "               0.7326287091636717,\n",
       "               0.7330962965236035,\n",
       "               0.7338660846256045,\n",
       "               0.7341638368570527,\n",
       "               0.7347563041881463,\n",
       "               0.7350512845234406,\n",
       "               0.7353708112052444,\n",
       "               0.7355543177924654,\n",
       "               0.7358423387504356,\n",
       "               0.7361120806751931,\n",
       "               0.7363384454684726,\n",
       "               0.7365760315204622,\n",
       "               0.7368117247389444,\n",
       "               0.7370277696510957,\n",
       "               0.7375601115497495,\n",
       "               0.7377754549380248,\n",
       "               0.7378912040874498,\n",
       "               0.7380370290763554,\n",
       "               0.7382350999242915,\n",
       "               0.7384344775060164,\n",
       "               0.7385329708839873,\n",
       "               0.7385593695851984,\n",
       "               0.7387566201175919,\n",
       "               0.7389830207813505,\n",
       "               0.7391369383935688,\n",
       "               0.7393880118340554,\n",
       "               0.7395543786468469,\n",
       "               0.7396444015967685,\n",
       "               0.7398647277582878,\n",
       "               0.7400291683973553,\n",
       "               0.7402085796786584,\n",
       "               0.7403351133767547,\n",
       "               0.7405993644608135,\n",
       "               0.740807386585787,\n",
       "               0.7409894061838483,\n",
       "               0.7411666514507255,\n",
       "               0.741305777489257,\n",
       "               0.7413729131333818,\n",
       "               0.7414738583117991,\n",
       "               0.741625442448362,\n",
       "               0.7417041600133513,\n",
       "               0.7418539191210747,\n",
       "               0.7419784933896265,\n",
       "               0.742097296431333,\n",
       "               0.7422724865920779,\n",
       "               0.7423835930003587,\n",
       "               0.7424318242905521,\n",
       "               0.7425292580388412,\n",
       "               0.742646728928146,\n",
       "               0.7427824920624081,\n",
       "               0.7428386192241708,\n",
       "               0.7429419396670838,\n",
       "               0.7429643327710437,\n",
       "               0.7430798827049051,\n",
       "               0.7431569563374699,\n",
       "               0.7432715937467483,\n",
       "               0.7433677880138255,\n",
       "               0.7434412558096488,\n",
       "               0.7435074239438036,\n",
       "               0.7435534593720735,\n",
       "               0.7435571054726401,\n",
       "               0.7435775020787556,\n",
       "               0.7436365666175054,\n",
       "               0.7437064085778027,\n",
       "               0.7437422225170586,\n",
       "               0.7437764896028108,\n",
       "               0.7438178820318823,\n",
       "               0.74387061624648,\n",
       "               0.7438905727649447,\n",
       "               0.7439927661673247,\n",
       "               0.7440024146013199,\n",
       "               0.7440377978482045,\n",
       "               0.7440513821192641,\n",
       "               0.7441273484926604,\n",
       "               0.7441669408564753,\n",
       "               0.7441858556113946,\n",
       "               0.7442124979466755,\n",
       "               0.7442771576979249,\n",
       "               0.7443005728006197,\n",
       "               0.7443311953439778,\n",
       "               0.7443650456556239,\n",
       "               0.744397027291618,\n",
       "               0.74440267424395,\n",
       "               0.7444382190263493,\n",
       "               0.7445137351926846,\n",
       "               0.7445525301781267,\n",
       "               0.7447270665016884,\n",
       "               0.7447642871083064,\n",
       "               0.7448551349518983,\n",
       "               0.744901713303259,\n",
       "               0.7448661628733549,\n",
       "               0.7448505716325475,\n",
       "               0.7448907389692989,\n",
       "               0.7449061961955286,\n",
       "               0.7449644381774257,\n",
       "               0.744962178409649,\n",
       "               0.7449676824869712,\n",
       "               0.7449661186879172,\n",
       "               0.7450267738808226,\n",
       "               0.7450470288716411,\n",
       "               0.745069257522755,\n",
       "               0.7450743944343543,\n",
       "               0.7450800318421195,\n",
       "               0.7450750130676216,\n",
       "               0.7450995540641578,\n",
       "               0.7451100815027649,\n",
       "               0.7450914218059653,\n",
       "               0.7450837653500435,\n",
       "               0.7450892893651595,\n",
       "               0.7450785936404719,\n",
       "               0.7451303086800137,\n",
       "               0.7452062206328023,\n",
       "               0.7452029918855784,\n",
       "               0.7452044702790896,\n",
       "               0.7452078183243406,\n",
       "               0.7452132176265415,\n",
       "               0.7452586561354283,\n",
       "               0.745277115252616,\n",
       "               0.7453254146410546,\n",
       "               0.7453198801360831,\n",
       "               0.7453331444036664,\n",
       "               0.7453590264265971,\n",
       "               0.7454014770083107,\n",
       "               0.7453813748399947,\n",
       "               0.7454102949284719,\n",
       "               0.7453979245570119,\n",
       "               0.7454442849065378,\n",
       "               0.7454711749955631,\n",
       "               0.7455215502306715,\n",
       "               0.7455523830208541,\n",
       "               0.7455553145576462,\n",
       "               0.7455870789437258,\n",
       "               0.7455520387048767,\n",
       "               0.7455574928939246,\n",
       "               0.7455483202326004,\n",
       "               0.7455471571826099,\n",
       "               0.745548586218574,\n",
       "               0.7455594295195236,\n",
       "               0.7456040841891513,\n",
       "               0.745622476326688,\n",
       "               0.7456657491240811,\n",
       "               0.7456576837885875,\n",
       "               0.7456868405923801,\n",
       "               0.7457052704803594,\n",
       "               0.7456997394776056,\n",
       "               0.7457246870874767,\n",
       "               0.7458033622719472,\n",
       "               0.7458844799535762,\n",
       "               0.7458810888184993,\n",
       "               0.7458754325125145,\n",
       "               0.7458818436495037,\n",
       "               0.7458766029947742,\n",
       "               0.7458869207620135,\n",
       "               0.7458900103891483,\n",
       "               0.7459060247495494,\n",
       "               0.7459091832116601,\n",
       "               0.745936544993763,\n",
       "               0.7459397310864073,\n",
       "               0.7459678665192087,\n",
       "               0.7459554688132831,\n",
       "               0.7459666337707551,\n",
       "               0.7459636588161501,\n",
       "               0.7459680057902269,\n",
       "               0.7459433389010277,\n",
       "               0.7459593686895082,\n",
       "               0.7459496736175785,\n",
       "               0.7459615684481582,\n",
       "               0.7459976774971092,\n",
       "               0.7459951829498932,\n",
       "               0.7459921037133087,\n",
       "               0.7459959742143821,\n",
       "               0.7459994991456217],\n",
       "              'test-AUC-std': [0.02532359239281949,\n",
       "               0.008466858922239156,\n",
       "               0.004585950280540858,\n",
       "               0.004815449956083742,\n",
       "               0.0019913225430410898,\n",
       "               0.0036590378327971533,\n",
       "               0.0010377349078268805,\n",
       "               0.0018098267662474293,\n",
       "               0.0021389364814496147,\n",
       "               0.0010816210541981707,\n",
       "               0.0009109152685680217,\n",
       "               0.002434088331193539,\n",
       "               0.0016368986811836261,\n",
       "               0.0023295211697072406,\n",
       "               0.002577920523229613,\n",
       "               0.0023204793795475248,\n",
       "               0.0015796647918663812,\n",
       "               0.0012675131083679748,\n",
       "               0.0005959262009161987,\n",
       "               0.0006274494740008236,\n",
       "               0.0010058442669958383,\n",
       "               0.0012940433041228297,\n",
       "               0.0013456221322952367,\n",
       "               0.001144895839357038,\n",
       "               0.0012123648314638605,\n",
       "               0.0013366982727912094,\n",
       "               0.001339040058419934,\n",
       "               0.0012871037044102289,\n",
       "               0.0013943295534002281,\n",
       "               0.0012317236773407934,\n",
       "               0.0009226238463269436,\n",
       "               0.0009207596209577093,\n",
       "               0.0008880951451911988,\n",
       "               0.0008277279030819567,\n",
       "               0.0009236325653394494,\n",
       "               0.0008266193498340217,\n",
       "               0.000908486289915637,\n",
       "               0.0008861306885369425,\n",
       "               0.0007098059362100951,\n",
       "               0.0006512860068509332,\n",
       "               0.0007263825197793467,\n",
       "               0.0007253717277105685,\n",
       "               0.0005512314198523541,\n",
       "               0.0007581512258018975,\n",
       "               0.0006857224396737535,\n",
       "               0.0007797304147385075,\n",
       "               0.0008170636530197181,\n",
       "               0.0007581922769637968,\n",
       "               0.0009294312946726416,\n",
       "               0.0009398192095068884,\n",
       "               0.0009126326959371571,\n",
       "               0.0009698487471650067,\n",
       "               0.0009380196943498978,\n",
       "               0.0007515349890663632,\n",
       "               0.0006187828199820805,\n",
       "               0.0005057435706896258,\n",
       "               0.0005871467477125114,\n",
       "               0.000615019317967743,\n",
       "               0.0007190485263371711,\n",
       "               0.0008866482515136522,\n",
       "               0.0008875667375294412,\n",
       "               0.0007285008398668062,\n",
       "               0.0008960573508490511,\n",
       "               0.0008079691274163551,\n",
       "               0.0007062063761600933,\n",
       "               0.00069487865013244,\n",
       "               0.0007233722926929515,\n",
       "               0.0005597041835083738,\n",
       "               0.0005264861194173963,\n",
       "               0.0004724843053420606,\n",
       "               0.0005093057362766326,\n",
       "               0.0005173542554170649,\n",
       "               0.0004808240967711858,\n",
       "               0.0004109066501969464,\n",
       "               0.00044570281115598973,\n",
       "               0.0004272792413529454,\n",
       "               0.000439054745604249,\n",
       "               0.00041803362579330325,\n",
       "               0.0004773632261178682,\n",
       "               0.00046583858069193973,\n",
       "               0.0004957508939481096,\n",
       "               0.0005727322649158489,\n",
       "               0.000561120993353562,\n",
       "               0.00042532409476542867,\n",
       "               0.0003669174434163326,\n",
       "               0.00037835918011397277,\n",
       "               0.0003135152491979665,\n",
       "               0.0003497234273534862,\n",
       "               0.00027304021948943374,\n",
       "               0.0002696544315423864,\n",
       "               0.0003023229851023641,\n",
       "               0.00025026774289106294,\n",
       "               0.0002469934991097206,\n",
       "               0.00022811245710394585,\n",
       "               0.00022213018620281731,\n",
       "               0.00020629942162416544,\n",
       "               0.00017864512502352914,\n",
       "               0.00017977635554250856,\n",
       "               0.0001847724069356753,\n",
       "               0.0002053234473095896,\n",
       "               0.0001648434464886782,\n",
       "               0.00020761385073964593,\n",
       "               0.00017284493789659886,\n",
       "               0.00017687275828017155,\n",
       "               0.00017746676757125648,\n",
       "               0.0002437725109907605,\n",
       "               0.0002940307387516383,\n",
       "               0.0003006814677872576,\n",
       "               0.0003304338159713527,\n",
       "               0.00032571608564376794,\n",
       "               0.00031200116755029656,\n",
       "               0.0003233719214620155,\n",
       "               0.00021271755789855547,\n",
       "               0.00019601258827327953,\n",
       "               5.5581490153552954e-05,\n",
       "               6.25634462612514e-05,\n",
       "               4.728296292219027e-05,\n",
       "               8.362111438530978e-05,\n",
       "               7.324550546186718e-05,\n",
       "               8.548065158074559e-05,\n",
       "               0.00011692009061906363,\n",
       "               0.000138881805328732,\n",
       "               0.000159372486809559,\n",
       "               0.00016631320414211336,\n",
       "               0.0001540334681279224,\n",
       "               0.00015879743220322478,\n",
       "               0.00013113605028878332,\n",
       "               0.0001007652880926456,\n",
       "               0.00011658553193894731,\n",
       "               0.00011585040775047589,\n",
       "               6.494702932034623e-05,\n",
       "               7.290157957630871e-05,\n",
       "               9.225930628721633e-05,\n",
       "               6.856129893881642e-05,\n",
       "               8.783686234356395e-05,\n",
       "               9.637590039608928e-05,\n",
       "               9.994698282443942e-05,\n",
       "               6.393479474443715e-05,\n",
       "               0.00013651364132303686,\n",
       "               0.0003143939839207871,\n",
       "               0.000338909681777747,\n",
       "               0.0003266314328202843,\n",
       "               0.0003432787309968498,\n",
       "               0.00037432162328763184,\n",
       "               0.00034363456493125894,\n",
       "               0.00035450351200104084,\n",
       "               0.00035323490055288895,\n",
       "               0.00036865662560145657,\n",
       "               0.0003782092031153797,\n",
       "               0.0003926083943459441,\n",
       "               0.0003963172431402646,\n",
       "               0.00039696883222072743,\n",
       "               0.00038464737381002504,\n",
       "               0.0003971747478911386,\n",
       "               0.00040905672666516406,\n",
       "               0.0004183232031727523,\n",
       "               0.0004381367373615701,\n",
       "               0.0004672035306302684,\n",
       "               0.0004367392241883043,\n",
       "               0.00040946734858020973,\n",
       "               0.0004192579104515046,\n",
       "               0.0004193533568755244,\n",
       "               0.00041762369170116943,\n",
       "               0.0004192615712828445,\n",
       "               0.0004362101258684112,\n",
       "               0.00042009784981146027,\n",
       "               0.00041748720069425047,\n",
       "               0.00040888224065514,\n",
       "               0.00038802859342702844,\n",
       "               0.00037026441296878425,\n",
       "               0.00037861962453010386,\n",
       "               0.00037692481417933614,\n",
       "               0.00043027335014486477,\n",
       "               0.00046121400920888083,\n",
       "               0.0005446723221166341,\n",
       "               0.0006442071833715171,\n",
       "               0.0006351865003248502,\n",
       "               0.0006310578465817754,\n",
       "               0.0006551561172386533,\n",
       "               0.0006502535147695251,\n",
       "               0.0006727224133376267,\n",
       "               0.0006781888070109002,\n",
       "               0.0007001067016156205,\n",
       "               0.0007298574057930118,\n",
       "               0.0007459932046994607,\n",
       "               0.0007589191745506932,\n",
       "               0.0007886918261306194,\n",
       "               0.0007713032934364513,\n",
       "               0.0007866913976575951,\n",
       "               0.0007817532214235002,\n",
       "               0.0007707687804994566,\n",
       "               0.0007440474526188147,\n",
       "               0.00072979931181204,\n",
       "               0.0007106698589331393,\n",
       "               0.0007112330868961144,\n",
       "               0.0007289071317548428,\n",
       "               0.0007106591616577817,\n",
       "               0.0007046394371559195,\n",
       "               0.0007053257487551718,\n",
       "               0.0007026838108733379],\n",
       "              'test-Logloss-mean': [0.35180698124470894,\n",
       "               0.23026996637385042,\n",
       "               0.18397858416083512,\n",
       "               0.16388931064488058,\n",
       "               0.15536707191661384,\n",
       "               0.1510823448581409,\n",
       "               0.14897560631064996,\n",
       "               0.1478944726648073,\n",
       "               0.14696395529728792,\n",
       "               0.14631252901941508,\n",
       "               0.145841463446266,\n",
       "               0.14553385428737553,\n",
       "               0.14517442685501203,\n",
       "               0.1449521560323742,\n",
       "               0.1447005467928781,\n",
       "               0.1445106367788774,\n",
       "               0.14432068138816234,\n",
       "               0.14414496820162906,\n",
       "               0.1439847337617169,\n",
       "               0.14387398888328623,\n",
       "               0.14372076145221693,\n",
       "               0.14362783120004263,\n",
       "               0.1435155113638539,\n",
       "               0.14343977554118434,\n",
       "               0.14335047017673552,\n",
       "               0.14324950377697632,\n",
       "               0.14318476583632203,\n",
       "               0.14314203109113346,\n",
       "               0.14304399774300544,\n",
       "               0.1429828010495902,\n",
       "               0.1428858103828851,\n",
       "               0.1428420702822026,\n",
       "               0.14277711119100542,\n",
       "               0.1427274278415724,\n",
       "               0.14268195075713838,\n",
       "               0.14264702864002174,\n",
       "               0.14261450467344372,\n",
       "               0.1425712541522943,\n",
       "               0.14254550909796357,\n",
       "               0.1425100209513985,\n",
       "               0.14248380388225937,\n",
       "               0.14245558471070216,\n",
       "               0.1423873084659136,\n",
       "               0.1423520416533882,\n",
       "               0.1423287957389005,\n",
       "               0.14231113232960577,\n",
       "               0.14228566315011004,\n",
       "               0.14225297869098777,\n",
       "               0.14224203548224248,\n",
       "               0.14223670194442095,\n",
       "               0.14221032597081132,\n",
       "               0.14217898327163894,\n",
       "               0.14216172290761173,\n",
       "               0.14213419209220823,\n",
       "               0.14211192770888062,\n",
       "               0.14210231526719985,\n",
       "               0.1420770087522377,\n",
       "               0.14204410282256663,\n",
       "               0.14202606162162001,\n",
       "               0.14201045814390814,\n",
       "               0.14197721330507398,\n",
       "               0.1419531007127323,\n",
       "               0.14192575821498002,\n",
       "               0.14190428045063205,\n",
       "               0.14189037205556174,\n",
       "               0.14188098536520286,\n",
       "               0.14186819654531244,\n",
       "               0.14184966432416232,\n",
       "               0.14184200720921683,\n",
       "               0.14182600533948445,\n",
       "               0.14181088796536323,\n",
       "               0.14180080846969337,\n",
       "               0.14178081205392776,\n",
       "               0.14176719688894732,\n",
       "               0.14175871266596737,\n",
       "               0.1417443225176791,\n",
       "               0.14173009605088135,\n",
       "               0.1417141639351123,\n",
       "               0.1417039128790523,\n",
       "               0.14168981376374737,\n",
       "               0.14168606369182637,\n",
       "               0.14167251170840303,\n",
       "               0.14166279303869245,\n",
       "               0.1416498733847369,\n",
       "               0.14163891132959164,\n",
       "               0.1416314475128556,\n",
       "               0.14162375164882954,\n",
       "               0.141617063539044,\n",
       "               0.14161587546664803,\n",
       "               0.14161372009561105,\n",
       "               0.14160998769666766,\n",
       "               0.14159501406094213,\n",
       "               0.14159161971921572,\n",
       "               0.14158785351263456,\n",
       "               0.14158697940660855,\n",
       "               0.1415806387129128,\n",
       "               0.1415760637177954,\n",
       "               0.14155996488760939,\n",
       "               0.14155938399343068,\n",
       "               0.14155387493436364,\n",
       "               0.14155017932230232,\n",
       "               0.1415419595162399,\n",
       "               0.14153822758747733,\n",
       "               0.1415350291456736,\n",
       "               0.1415312814443288,\n",
       "               0.14152165555152485,\n",
       "               0.14151630013793481,\n",
       "               0.14151476059711635,\n",
       "               0.14150941452489504,\n",
       "               0.14150600003735156,\n",
       "               0.1415052428427487,\n",
       "               0.14149906230146095,\n",
       "               0.1414926589488338,\n",
       "               0.14148659525263066,\n",
       "               0.1414660278632282,\n",
       "               0.14146043397263794,\n",
       "               0.14144873823631485,\n",
       "               0.14144002355354315,\n",
       "               0.14144415240665806,\n",
       "               0.14144604054162202,\n",
       "               0.14144125860299242,\n",
       "               0.14143835166125726,\n",
       "               0.14142670970835183,\n",
       "               0.1414250409164651,\n",
       "               0.1414200430209583,\n",
       "               0.14142080227367038,\n",
       "               0.1414180805128696,\n",
       "               0.14141618674546222,\n",
       "               0.14141078615690797,\n",
       "               0.14141248458233396,\n",
       "               0.1414112349496702,\n",
       "               0.14141413588947427,\n",
       "               0.14141265949537735,\n",
       "               0.14141145199939856,\n",
       "               0.14141178290805975,\n",
       "               0.14141114333444463,\n",
       "               0.14141080390410607,\n",
       "               0.14141310715010907,\n",
       "               0.1414108689820429,\n",
       "               0.14139944188870576,\n",
       "               0.1413977009161523,\n",
       "               0.1413963499663271,\n",
       "               0.1413958386481126,\n",
       "               0.14139585639743146,\n",
       "               0.14138849366175707,\n",
       "               0.14138650872046887,\n",
       "               0.1413791420381603,\n",
       "               0.1413770472558733,\n",
       "               0.14137833331128102,\n",
       "               0.14137638136198225,\n",
       "               0.14137064295410784,\n",
       "               0.14137072728782887,\n",
       "               0.14136202210448945,\n",
       "               0.14136268221242856,\n",
       "               0.14135602703029135,\n",
       "               0.141349377286439,\n",
       "               0.14134609326019285,\n",
       "               0.14134338895287124,\n",
       "               0.14134162195493039,\n",
       "               0.14133863938359015,\n",
       "               0.14133996281159514,\n",
       "               0.14133855952276983,\n",
       "               0.14133807029450488,\n",
       "               0.1413378928908927,\n",
       "               0.1413373025229666,\n",
       "               0.14133475574633034,\n",
       "               0.14133037028239848,\n",
       "               0.1413275942109404,\n",
       "               0.1413251591967822,\n",
       "               0.14132296669682873,\n",
       "               0.14131770421067583,\n",
       "               0.14131642574302705,\n",
       "               0.14131841985522717,\n",
       "               0.1413192589439878,\n",
       "               0.14130964437704643,\n",
       "               0.1413012829024133,\n",
       "               0.1412990966681225,\n",
       "               0.1413001065928621,\n",
       "               0.1412967970987309,\n",
       "               0.1412967750989941,\n",
       "               0.14129504703454443,\n",
       "               0.1412946356937097,\n",
       "               0.14129258051240134,\n",
       "               0.14129059960092724,\n",
       "               0.1412848249743339,\n",
       "               0.14128466619544164,\n",
       "               0.14128254634476312,\n",
       "               0.1412836482114419,\n",
       "               0.14128390733264062,\n",
       "               0.14128330202817227,\n",
       "               0.14128161916479465,\n",
       "               0.14128313583350016,\n",
       "               0.1412802605935267,\n",
       "               0.14128111301307322,\n",
       "               0.14128100561449025,\n",
       "               0.14127810296945267,\n",
       "               0.14127796915402568,\n",
       "               0.1412775856065105,\n",
       "               0.14127725974326685,\n",
       "               0.14127669077226337],\n",
       "              'test-Logloss-std': [0.0005837574209663514,\n",
       "               0.0002172022677762946,\n",
       "               0.0002969095093022844,\n",
       "               0.0002555372726815721,\n",
       "               0.00021283460587267903,\n",
       "               0.0004202418831078934,\n",
       "               0.00015571945343389128,\n",
       "               0.00029774229598745,\n",
       "               0.000255313187081981,\n",
       "               7.651501357620769e-05,\n",
       "               6.601226108440212e-05,\n",
       "               0.00015527991153946248,\n",
       "               9.569326022227246e-05,\n",
       "               0.00013837819247912122,\n",
       "               0.00017732068608738,\n",
       "               0.00014250898480069082,\n",
       "               8.814574624388235e-05,\n",
       "               7.139889431199906e-05,\n",
       "               1.607344990252169e-05,\n",
       "               2.866477445658262e-05,\n",
       "               5.356389392858374e-05,\n",
       "               8.259948087794028e-05,\n",
       "               7.444936079662396e-05,\n",
       "               7.77515981070996e-05,\n",
       "               5.640494310542395e-05,\n",
       "               7.643880984889307e-05,\n",
       "               7.105094830057104e-05,\n",
       "               7.496255602883405e-05,\n",
       "               0.00011145816238551958,\n",
       "               9.38104898945748e-05,\n",
       "               8.372438310563817e-05,\n",
       "               8.986669510752975e-05,\n",
       "               0.00012254943227647165,\n",
       "               0.00012187171824515203,\n",
       "               0.00013883099459294484,\n",
       "               0.00013211310021562464,\n",
       "               0.00014585429006628737,\n",
       "               0.00015549452387443895,\n",
       "               0.00013651174479778572,\n",
       "               0.00012415571057365965,\n",
       "               0.00014041173981286227,\n",
       "               0.00013620109597409572,\n",
       "               0.00011235786649752728,\n",
       "               0.00011653202054386406,\n",
       "               0.00010210442476872522,\n",
       "               0.00011273146632724944,\n",
       "               0.00011687462910787654,\n",
       "               0.00010945650691776421,\n",
       "               0.00012728800618267363,\n",
       "               0.00012925052853179211,\n",
       "               0.00013983451997903303,\n",
       "               0.0001523510697805438,\n",
       "               0.0001594896129564806,\n",
       "               0.00013648011382222085,\n",
       "               0.00012407535496673214,\n",
       "               0.00011141986347715864,\n",
       "               0.00011830915317039878,\n",
       "               0.0001259481216724747,\n",
       "               0.0001439758503337618,\n",
       "               0.00015771086775209515,\n",
       "               0.00014812792851109184,\n",
       "               0.00012962203553671252,\n",
       "               0.00015078396066287738,\n",
       "               0.0001423358562330602,\n",
       "               0.0001306194204554875,\n",
       "               0.00012721313461129983,\n",
       "               0.00013043100454917224,\n",
       "               0.00010932449382016015,\n",
       "               0.0001090088811735865,\n",
       "               0.00010094318949279182,\n",
       "               0.00010260780813623869,\n",
       "               9.950202183128705e-05,\n",
       "               0.00010097808467949662,\n",
       "               9.654888975527773e-05,\n",
       "               0.00010431894850519347,\n",
       "               0.0001017464208828689,\n",
       "               9.387029485048853e-05,\n",
       "               9.550527731257697e-05,\n",
       "               0.00010193501529597636,\n",
       "               0.00010983615484675611,\n",
       "               0.00011384966341538812,\n",
       "               0.0001230273342997177,\n",
       "               0.00012328518005062745,\n",
       "               0.00010278953997408501,\n",
       "               9.378491064992746e-05,\n",
       "               9.297002195327092e-05,\n",
       "               8.983828348207946e-05,\n",
       "               9.493597249738022e-05,\n",
       "               8.726891692369035e-05,\n",
       "               8.7130003331294e-05,\n",
       "               9.427663044962366e-05,\n",
       "               7.995372419299508e-05,\n",
       "               8.324923770092814e-05,\n",
       "               7.775548428555403e-05,\n",
       "               7.583759418347922e-05,\n",
       "               7.289448918584842e-05,\n",
       "               7.089323722283244e-05,\n",
       "               6.264991983508518e-05,\n",
       "               6.407420304045691e-05,\n",
       "               6.463002371439278e-05,\n",
       "               5.7335849856998854e-05,\n",
       "               6.246125505097976e-05,\n",
       "               5.979969665840396e-05,\n",
       "               6.046755540919396e-05,\n",
       "               5.980628880092118e-05,\n",
       "               6.610711467160652e-05,\n",
       "               7.404103454735078e-05,\n",
       "               7.557026894425236e-05,\n",
       "               8.150631755721151e-05,\n",
       "               7.22909314737312e-05,\n",
       "               7.319811159014766e-05,\n",
       "               7.563359492100841e-05,\n",
       "               6.068805251699489e-05,\n",
       "               5.890877355035975e-05,\n",
       "               4.38688628517993e-05,\n",
       "               4.427185538906972e-05,\n",
       "               4.487312522128361e-05,\n",
       "               3.4624741940115545e-05,\n",
       "               3.259873988833714e-05,\n",
       "               3.7310097541288e-05,\n",
       "               3.811389415132559e-05,\n",
       "               4.411542118701477e-05,\n",
       "               4.9759621052660846e-05,\n",
       "               4.981362418877432e-05,\n",
       "               4.71560938041917e-05,\n",
       "               4.879135435237901e-05,\n",
       "               4.798959782481898e-05,\n",
       "               4.6648647541788485e-05,\n",
       "               5.0882543414887834e-05,\n",
       "               5.205288624310396e-05,\n",
       "               4.891886052195218e-05,\n",
       "               4.8611591989005545e-05,\n",
       "               4.9077509982445886e-05,\n",
       "               4.737288865673105e-05,\n",
       "               4.779436275417745e-05,\n",
       "               4.5352558244441845e-05,\n",
       "               4.648221319766166e-05,\n",
       "               4.377061058012832e-05,\n",
       "               5.118881145351792e-05,\n",
       "               7.594619011424548e-05,\n",
       "               7.849900553625912e-05,\n",
       "               7.949688173696476e-05,\n",
       "               7.972078999701114e-05,\n",
       "               8.311798386173363e-05,\n",
       "               7.79851652020845e-05,\n",
       "               7.98279527995231e-05,\n",
       "               7.818287299813165e-05,\n",
       "               8.494478098622199e-05,\n",
       "               8.578208589125797e-05,\n",
       "               8.590563320240281e-05,\n",
       "               8.454305358862779e-05,\n",
       "               8.616326299728145e-05,\n",
       "               8.33815643417997e-05,\n",
       "               8.381480126212499e-05,\n",
       "               8.817837957241318e-05,\n",
       "               8.455625796519155e-05,\n",
       "               8.696261266478662e-05,\n",
       "               8.961305868389429e-05,\n",
       "               8.57325047236974e-05,\n",
       "               8.362210604100771e-05,\n",
       "               8.892350729529429e-05,\n",
       "               8.844021293350916e-05,\n",
       "               8.862459583290053e-05,\n",
       "               8.899522253085028e-05,\n",
       "               8.992056784820182e-05,\n",
       "               8.915130025969569e-05,\n",
       "               8.925661120271388e-05,\n",
       "               8.804937829189448e-05,\n",
       "               8.870027412321713e-05,\n",
       "               9.095366683031001e-05,\n",
       "               9.165466389866536e-05,\n",
       "               9.289472580313485e-05,\n",
       "               9.52925465615794e-05,\n",
       "               0.00010282733124660046,\n",
       "               0.00011196968165069219,\n",
       "               0.0001191153698187999,\n",
       "               0.00012134300142199919,\n",
       "               0.00012053436089811714,\n",
       "               0.00012762531319839942,\n",
       "               0.0001259248579265282,\n",
       "               0.0001302077458216833,\n",
       "               0.00013105892259699574,\n",
       "               0.00013410723834397532,\n",
       "               0.00013876626870233406,\n",
       "               0.00014404384456570665,\n",
       "               0.00014480416500771662,\n",
       "               0.0001472706684137815,\n",
       "               0.0001452581528960754,\n",
       "               0.0001443353205326422,\n",
       "               0.00014390817413346057,\n",
       "               0.00014502682958959478,\n",
       "               0.00014262283077907367,\n",
       "               0.0001414876946954318,\n",
       "               0.00013962905829049602,\n",
       "               0.00013851266644080633,\n",
       "               0.00013938581278184973,\n",
       "               0.0001385702005323355,\n",
       "               0.00013838900160632086,\n",
       "               0.00013823319820230331,\n",
       "               0.00013797027325572947],\n",
       "              'train-Logloss-mean': [0.35176721840712855,\n",
       "               0.23018842093806988,\n",
       "               0.1838362830940481,\n",
       "               0.1637560661806244,\n",
       "               0.15521621418876286,\n",
       "               0.15093659878292745,\n",
       "               0.1488188462995693,\n",
       "               0.1476920105594722,\n",
       "               0.14671708789146032,\n",
       "               0.14605019227412705,\n",
       "               0.14556231998984911,\n",
       "               0.1452189806667257,\n",
       "               0.14483764258838752,\n",
       "               0.14458950136337978,\n",
       "               0.1443172819396642,\n",
       "               0.14410078484716118,\n",
       "               0.14390084996005137,\n",
       "               0.14370604154415495,\n",
       "               0.14353251937575573,\n",
       "               0.14340012470374497,\n",
       "               0.14322025600933078,\n",
       "               0.14310482505853225,\n",
       "               0.14297094016168024,\n",
       "               0.14287329100760862,\n",
       "               0.14275130106845982,\n",
       "               0.14262723412591133,\n",
       "               0.1425432461616484,\n",
       "               0.14248055831343437,\n",
       "               0.1423648128953731,\n",
       "               0.1422773597415666,\n",
       "               0.14216179724717595,\n",
       "               0.14209819480826724,\n",
       "               0.1420227139142685,\n",
       "               0.14193682403856936,\n",
       "               0.14186934286402508,\n",
       "               0.1418026651434272,\n",
       "               0.14174519012985098,\n",
       "               0.14167588021524805,\n",
       "               0.14163163263175252,\n",
       "               0.1415760647990761,\n",
       "               0.14152758080569702,\n",
       "               0.14148548714575404,\n",
       "               0.14139658523078957,\n",
       "               0.14134049592515532,\n",
       "               0.1412918696079497,\n",
       "               0.1412541544053633,\n",
       "               0.1412048862507497,\n",
       "               0.1411464863801517,\n",
       "               0.1411136668722227,\n",
       "               0.14109406663470267,\n",
       "               0.14104309364786013,\n",
       "               0.1409899621362947,\n",
       "               0.14094532052216216,\n",
       "               0.14089379790577983,\n",
       "               0.14084314509024884,\n",
       "               0.14081988885124908,\n",
       "               0.14076198231224032,\n",
       "               0.14071287725196743,\n",
       "               0.14067329311589327,\n",
       "               0.1406303483791496,\n",
       "               0.14056224316823054,\n",
       "               0.14051547619762605,\n",
       "               0.1404605995826689,\n",
       "               0.14042185860137638,\n",
       "               0.14038286198638272,\n",
       "               0.14034686979815425,\n",
       "               0.14031191238563992,\n",
       "               0.14027631371489327,\n",
       "               0.14025295150255293,\n",
       "               0.14021773805975285,\n",
       "               0.14018570402470198,\n",
       "               0.140149593603721,\n",
       "               0.14009985433640054,\n",
       "               0.14006119981030782,\n",
       "               0.14003182976801853,\n",
       "               0.140000544512758,\n",
       "               0.139964987252702,\n",
       "               0.13992762671854178,\n",
       "               0.13989929046056523,\n",
       "               0.13986530571484754,\n",
       "               0.13984549887885372,\n",
       "               0.13980110006972193,\n",
       "               0.13976558407486048,\n",
       "               0.13971955001567735,\n",
       "               0.139691095773846,\n",
       "               0.13965934458884802,\n",
       "               0.13962680430399296,\n",
       "               0.13959602711289984,\n",
       "               0.13956875061736038,\n",
       "               0.13955078360140794,\n",
       "               0.1395212324693135,\n",
       "               0.1394914874166521,\n",
       "               0.1394671521363924,\n",
       "               0.13944711552922892,\n",
       "               0.13942401330689383,\n",
       "               0.13938733043924798,\n",
       "               0.13936795548748163,\n",
       "               0.13932981536095262,\n",
       "               0.13930586524010427,\n",
       "               0.13928601266442842,\n",
       "               0.1392610478298087,\n",
       "               0.13923770436125366,\n",
       "               0.13921257494781888,\n",
       "               0.13919281800977,\n",
       "               0.13916664335970874,\n",
       "               0.13914009683618428,\n",
       "               0.13911115037410074,\n",
       "               0.13908892379008056,\n",
       "               0.1390572420808804,\n",
       "               0.13902560863172353,\n",
       "               0.13901676431024837,\n",
       "               0.13899053672650707,\n",
       "               0.13896310717817936,\n",
       "               0.13893024044937322,\n",
       "               0.1388802017174836,\n",
       "               0.1388501502047339,\n",
       "               0.13881533933858872,\n",
       "               0.13878572017969776,\n",
       "               0.13877051537350063,\n",
       "               0.13874405253161692,\n",
       "               0.13873303992089822,\n",
       "               0.13871209658056027,\n",
       "               0.13868508877185384,\n",
       "               0.13866600846156332,\n",
       "               0.1386463450327977,\n",
       "               0.13862644474617528,\n",
       "               0.13860084722938792,\n",
       "               0.138577370653178,\n",
       "               0.1385567784350801,\n",
       "               0.1385466444498232,\n",
       "               0.13852269145485283,\n",
       "               0.1385029150149172,\n",
       "               0.13848510504995346,\n",
       "               0.13846949528800026,\n",
       "               0.1384584285571925,\n",
       "               0.13844391243212137,\n",
       "               0.13842507419017588,\n",
       "               0.13840730331161372,\n",
       "               0.13838153326715827,\n",
       "               0.1383463701971577,\n",
       "               0.1383259651487725,\n",
       "               0.13830732996701875,\n",
       "               0.13828988481385085,\n",
       "               0.13827020096039208,\n",
       "               0.1382551670503943,\n",
       "               0.1382335031033536,\n",
       "               0.13821220569502998,\n",
       "               0.1381878223719961,\n",
       "               0.13817164396648865,\n",
       "               0.1381550512735098,\n",
       "               0.13813050478242536,\n",
       "               0.13810830184237036,\n",
       "               0.1380880724420219,\n",
       "               0.1380751897656275,\n",
       "               0.13805856440065714,\n",
       "               0.1380311861421408,\n",
       "               0.13800259229750325,\n",
       "               0.13798666800783746,\n",
       "               0.13796589764485132,\n",
       "               0.13794644316558324,\n",
       "               0.13792268394644283,\n",
       "               0.13790889415935423,\n",
       "               0.1378919428130947,\n",
       "               0.1378868830103335,\n",
       "               0.13787497480088065,\n",
       "               0.13786192435724057,\n",
       "               0.13784064026862777,\n",
       "               0.1378169141196284,\n",
       "               0.13779313403588592,\n",
       "               0.13776636564750858,\n",
       "               0.13774503746505687,\n",
       "               0.1377277387857395,\n",
       "               0.13770511218496997,\n",
       "               0.13768097015880446,\n",
       "               0.13765684393063704,\n",
       "               0.1376279556886996,\n",
       "               0.13760999495604478,\n",
       "               0.13759731793082552,\n",
       "               0.1375712740771714,\n",
       "               0.13755815944580205,\n",
       "               0.1375484091235318,\n",
       "               0.1375426207381006,\n",
       "               0.13753019131851948,\n",
       "               0.13750704657677051,\n",
       "               0.13749691127694194,\n",
       "               0.13749264866577232,\n",
       "               0.1374833582061873,\n",
       "               0.13747181164986075,\n",
       "               0.13746590979443882,\n",
       "               0.13745502502730517,\n",
       "               0.1374432703113095,\n",
       "               0.13742796269885318,\n",
       "               0.13741876916890677,\n",
       "               0.1374118154860651,\n",
       "               0.137396334060938,\n",
       "               0.13738451172119714,\n",
       "               0.1373745758659598,\n",
       "               0.13736175204492085,\n",
       "               0.13735508492162815,\n",
       "               0.13734589424327126],\n",
       "              'train-Logloss-std': [0.0005285859497856227,\n",
       "               0.0001724951039144888,\n",
       "               0.00025956986131828104,\n",
       "               0.00022496022328593243,\n",
       "               0.00017580056977787765,\n",
       "               0.000404254940537632,\n",
       "               0.00010263781081431873,\n",
       "               0.0002396043237037392,\n",
       "               0.00018537336205053955,\n",
       "               3.078236545027917e-05,\n",
       "               6.054536488274788e-05,\n",
       "               0.00018359831556467136,\n",
       "               0.00012219361960540188,\n",
       "               0.00017086153499480818,\n",
       "               0.00020493159309817086,\n",
       "               0.00021026966137538161,\n",
       "               0.00018015468961708593,\n",
       "               0.00012592144578431837,\n",
       "               0.00010886510033230686,\n",
       "               0.00013486777613450692,\n",
       "               0.0001517120456242865,\n",
       "               0.00017043539010006407,\n",
       "               0.00014277172835272417,\n",
       "               0.00010586724348178574,\n",
       "               0.00012855019253002226,\n",
       "               0.00013250272593265047,\n",
       "               0.00014365708490960213,\n",
       "               0.00014764553679245032,\n",
       "               0.00015706758987282682,\n",
       "               0.00016343054351362194,\n",
       "               0.0001250883894274553,\n",
       "               0.00012314908357763914,\n",
       "               0.00011373022629257952,\n",
       "               8.720021132652935e-05,\n",
       "               9.512227837272311e-05,\n",
       "               0.00010298573060979864,\n",
       "               0.00010210478354556449,\n",
       "               0.00011165049279775778,\n",
       "               9.919094220391805e-05,\n",
       "               0.00011476459250603898,\n",
       "               0.00010429643747693835,\n",
       "               0.00010747334385549215,\n",
       "               0.00010872582070796802,\n",
       "               0.00014759805544408986,\n",
       "               0.00013835707583457728,\n",
       "               0.00014369213401962274,\n",
       "               0.0001531440216473359,\n",
       "               0.00013794673515662837,\n",
       "               0.00015988839949567516,\n",
       "               0.00017167276068744556,\n",
       "               0.00016570646891429455,\n",
       "               0.0001687709561524094,\n",
       "               0.0001703388236619904,\n",
       "               0.00014801970688812496,\n",
       "               0.00014255119325574781,\n",
       "               0.00013368997163518083,\n",
       "               0.00014699790508239093,\n",
       "               0.0001583273981988634,\n",
       "               0.00014421152377796647,\n",
       "               0.0001414881304748178,\n",
       "               0.0001475405174773958,\n",
       "               0.00014011290550204307,\n",
       "               0.00016302141963962888,\n",
       "               0.00016506028837294404,\n",
       "               0.00015960354969256945,\n",
       "               0.00015837030470253264,\n",
       "               0.00016129532735844248,\n",
       "               0.00017576909584325756,\n",
       "               0.00015749762776877315,\n",
       "               0.00017057821534079948,\n",
       "               0.00017862188807734952,\n",
       "               0.00018632728693156094,\n",
       "               0.00018295208480445294,\n",
       "               0.00017851389874293152,\n",
       "               0.00018423079033022812,\n",
       "               0.00017470720181221545,\n",
       "               0.0001901626697785432,\n",
       "               0.00017640376277095471,\n",
       "               0.00017515635672423017,\n",
       "               0.0001675944834156824,\n",
       "               0.00018357587697156706,\n",
       "               0.00017683939379386005,\n",
       "               0.0001836948618113667,\n",
       "               0.00019138473447048303,\n",
       "               0.00017907168233227555,\n",
       "               0.00019789970337782947,\n",
       "               0.00018435570859666516,\n",
       "               0.00018607527514636314,\n",
       "               0.00018633498226311394,\n",
       "               0.0001758855477547297,\n",
       "               0.0001794105903620079,\n",
       "               0.00019684341721436135,\n",
       "               0.00019852254585758577,\n",
       "               0.0002030731157898976,\n",
       "               0.0001954318704669913,\n",
       "               0.0001984955201909476,\n",
       "               0.00021185374080156226,\n",
       "               0.00020923688937042048,\n",
       "               0.00020728690377439876,\n",
       "               0.0002181194354089805,\n",
       "               0.00023324779019839813,\n",
       "               0.00023041861747999338,\n",
       "               0.00022630355483092837,\n",
       "               0.00021378628281811104,\n",
       "               0.00020316378328769137,\n",
       "               0.0002087339823524504,\n",
       "               0.0002054782555150741,\n",
       "               0.00020174810729570747,\n",
       "               0.00018284297622297448,\n",
       "               0.00018156079277095215,\n",
       "               0.0001852505014592404,\n",
       "               0.00018831531482320075,\n",
       "               0.0001895226257879383,\n",
       "               0.00018574296058357015,\n",
       "               0.00020940967566002462,\n",
       "               0.0002060273670065533,\n",
       "               0.00020890962798296775,\n",
       "               0.00021763963831431806,\n",
       "               0.00021621835775528322,\n",
       "               0.0002152636010268415,\n",
       "               0.00020973500285833568,\n",
       "               0.0002069539105045894,\n",
       "               0.000204181371300185,\n",
       "               0.000212509182965831,\n",
       "               0.0002220682723092787,\n",
       "               0.0002218799302961182,\n",
       "               0.0002344005912472272,\n",
       "               0.00023016101737716052,\n",
       "               0.0002211406087114977,\n",
       "               0.0002254788764410636,\n",
       "               0.0002240109546059361,\n",
       "               0.00023209509987769312,\n",
       "               0.0002344647253573436,\n",
       "               0.0002319471504334184,\n",
       "               0.000238856316368723,\n",
       "               0.0002454106374958728,\n",
       "               0.00024529926536625993,\n",
       "               0.00022927148652566196,\n",
       "               0.00023205889954256708,\n",
       "               0.00019884828728649337,\n",
       "               0.00019092767080716974,\n",
       "               0.0001886237265973517,\n",
       "               0.00019387594709236407,\n",
       "               0.00019519344917595232,\n",
       "               0.00020405856922089215,\n",
       "               0.00019682226767867082,\n",
       "               0.00020467508591132303,\n",
       "               0.00019663791117165762,\n",
       "               0.00019474684810997917,\n",
       "               0.00018947796872728422,\n",
       "               0.00020531358145854428,\n",
       "               0.00019619821151415232,\n",
       "               0.00020301899479277,\n",
       "               0.00021297488576562632,\n",
       "               0.00020380194652188142,\n",
       "               0.0002174354205637758,\n",
       "               0.00020784304602385712,\n",
       "               0.00019932892111356495,\n",
       "               0.00020144450209196764,\n",
       "               0.00020832523764318948,\n",
       "               0.0002176971013620358,\n",
       "               0.00021110341340097887,\n",
       "               0.00021148644755659637,\n",
       "               0.00020928166214866553,\n",
       "               0.0001974080143002658,\n",
       "               0.0002025729334612479,\n",
       "               0.000209322297519319,\n",
       "               0.00021196267951857837,\n",
       "               0.0002159513316942829,\n",
       "               0.00019919760231606662,\n",
       "               0.0001965326132326443,\n",
       "               0.00020401979417468709,\n",
       "               0.0001977303475661469,\n",
       "               0.00019648439620748983,\n",
       "               0.0001786854586575178,\n",
       "               0.00017762549001625177,\n",
       "               0.00017113203497562922,\n",
       "               0.00017897356096562895,\n",
       "               0.00017204671894432164,\n",
       "               0.0001728103842133515,\n",
       "               0.00016827360301073754,\n",
       "               0.0001599470137579489,\n",
       "               0.00014074053987737543,\n",
       "               0.00012075480184389592,\n",
       "               0.0001179179718607274,\n",
       "               0.00011241224121733347,\n",
       "               0.0001098845986741196,\n",
       "               0.0001038341059744456,\n",
       "               9.874429818183713e-05,\n",
       "               9.884504529195414e-05,\n",
       "               9.552246400539639e-05,\n",
       "               9.185434477057187e-05,\n",
       "               0.00010214981000028083,\n",
       "               9.471013252230616e-05,\n",
       "               0.00010220054873369587,\n",
       "               0.00010819572680575705,\n",
       "               0.00011406853932196002,\n",
       "               0.00012411586485373656,\n",
       "               0.00012553951198232088,\n",
       "               0.00013564530033628817]})}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:\ttest: 0.6941101\tbest: 0.6941101 (20)\ttotal: 5.75s\tremaining: 49s\n",
      "40:\ttest: 0.7078560\tbest: 0.7078560 (40)\ttotal: 11.6s\tremaining: 45.1s\n",
      "60:\ttest: 0.7169596\tbest: 0.7169596 (60)\ttotal: 17.3s\tremaining: 39.5s\n",
      "80:\ttest: 0.7235050\tbest: 0.7235050 (80)\ttotal: 23.2s\tremaining: 34s\n",
      "100:\ttest: 0.7275101\tbest: 0.7275101 (100)\ttotal: 29s\tremaining: 28.4s\n",
      "120:\ttest: 0.7309723\tbest: 0.7309723 (120)\ttotal: 34.8s\tremaining: 22.7s\n",
      "140:\ttest: 0.7341186\tbest: 0.7341186 (140)\ttotal: 41.1s\tremaining: 17.2s\n",
      "160:\ttest: 0.7363189\tbest: 0.7363189 (160)\ttotal: 46.9s\tremaining: 11.4s\n",
      "180:\ttest: 0.7382495\tbest: 0.7382495 (180)\ttotal: 53s\tremaining: 5.56s\n",
      "199:\ttest: 0.7393601\tbest: 0.7393601 (199)\ttotal: 59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7393601424\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "16:\tloss: 0.7393601\tbest: 0.7501471 (14)\ttotal: 19m 40s\tremaining: 17m 21s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 223ms\tremaining: 44.4s\n",
      "20:\ttest: 0.7110566\tbest: 0.7110566 (20)\ttotal: 6.85s\tremaining: 58.4s\n",
      "40:\ttest: 0.7257202\tbest: 0.7257202 (40)\ttotal: 13.1s\tremaining: 50.6s\n",
      "60:\ttest: 0.7332879\tbest: 0.7332879 (60)\ttotal: 19.6s\tremaining: 44.7s\n",
      "80:\ttest: 0.7373864\tbest: 0.7373864 (80)\ttotal: 26.2s\tremaining: 38.5s\n",
      "100:\ttest: 0.7404107\tbest: 0.7404107 (100)\ttotal: 32.3s\tremaining: 31.6s\n",
      "120:\ttest: 0.7423518\tbest: 0.7423518 (120)\ttotal: 38.2s\tremaining: 25s\n",
      "140:\ttest: 0.7433657\tbest: 0.7433657 (140)\ttotal: 44.1s\tremaining: 18.5s\n",
      "160:\ttest: 0.7448501\tbest: 0.7448501 (160)\ttotal: 50.4s\tremaining: 12.2s\n",
      "180:\ttest: 0.7455125\tbest: 0.7455128 (179)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7459531\tbest: 0.7459531 (199)\ttotal: 1m 1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7459531324\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "17:\tloss: 0.7459531\tbest: 0.7501471 (14)\ttotal: 20m 41s\tremaining: 16m 5s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7185768\tbest: 0.7185768 (20)\ttotal: 6.28s\tremaining: 53.5s\n",
      "40:\ttest: 0.7308254\tbest: 0.7308254 (40)\ttotal: 12.4s\tremaining: 48.1s\n",
      "60:\ttest: 0.7380665\tbest: 0.7380665 (60)\ttotal: 18.7s\tremaining: 42.6s\n",
      "80:\ttest: 0.7411564\tbest: 0.7411564 (80)\ttotal: 25.1s\tremaining: 36.9s\n",
      "100:\ttest: 0.7432658\tbest: 0.7432658 (100)\ttotal: 31.1s\tremaining: 30.5s\n",
      "120:\ttest: 0.7446442\tbest: 0.7446442 (120)\ttotal: 36.9s\tremaining: 24.1s\n",
      "140:\ttest: 0.7451662\tbest: 0.7451662 (140)\ttotal: 42.8s\tremaining: 17.9s\n",
      "160:\ttest: 0.7462863\tbest: 0.7463030 (158)\ttotal: 49.5s\tremaining: 12s\n",
      "180:\ttest: 0.7470859\tbest: 0.7470859 (180)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7474472\tbest: 0.7474523 (197)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7474523159\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "18:\tloss: 0.7474523\tbest: 0.7501471 (14)\ttotal: 21m 44s\tremaining: 14m 52s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 252ms\tremaining: 50.2s\n",
      "20:\ttest: 0.7249919\tbest: 0.7249919 (20)\ttotal: 6.6s\tremaining: 56.3s\n",
      "40:\ttest: 0.7353034\tbest: 0.7353034 (40)\ttotal: 12.8s\tremaining: 49.7s\n",
      "60:\ttest: 0.7392334\tbest: 0.7392334 (60)\ttotal: 18.3s\tremaining: 41.7s\n",
      "80:\ttest: 0.7430480\tbest: 0.7430480 (80)\ttotal: 24.3s\tremaining: 35.8s\n",
      "100:\ttest: 0.7444092\tbest: 0.7444092 (100)\ttotal: 29.9s\tremaining: 29.3s\n",
      "120:\ttest: 0.7458438\tbest: 0.7458438 (120)\ttotal: 35.9s\tremaining: 23.4s\n",
      "140:\ttest: 0.7468424\tbest: 0.7468424 (140)\ttotal: 42s\tremaining: 17.6s\n",
      "160:\ttest: 0.7474859\tbest: 0.7474935 (155)\ttotal: 47.7s\tremaining: 11.6s\n",
      "180:\ttest: 0.7480808\tbest: 0.7480857 (178)\ttotal: 53.5s\tremaining: 5.61s\n",
      "199:\ttest: 0.7482791\tbest: 0.7483154 (197)\ttotal: 58.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7483154481\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "19:\tloss: 0.7483154\tbest: 0.7501471 (14)\ttotal: 22m 43s\tremaining: 13m 38s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7008331\tbest: 0.7008331 (20)\ttotal: 6.51s\tremaining: 55.5s\n",
      "40:\ttest: 0.7134338\tbest: 0.7134338 (40)\ttotal: 13.6s\tremaining: 52.6s\n",
      "60:\ttest: 0.7215962\tbest: 0.7215962 (60)\ttotal: 20s\tremaining: 45.5s\n",
      "80:\ttest: 0.7282169\tbest: 0.7282169 (80)\ttotal: 26.7s\tremaining: 39.2s\n",
      "100:\ttest: 0.7326643\tbest: 0.7326643 (100)\ttotal: 34s\tremaining: 33.3s\n",
      "120:\ttest: 0.7352217\tbest: 0.7352217 (120)\ttotal: 40.8s\tremaining: 26.6s\n",
      "140:\ttest: 0.7377332\tbest: 0.7377332 (140)\ttotal: 47.7s\tremaining: 20s\n",
      "160:\ttest: 0.7396521\tbest: 0.7396521 (160)\ttotal: 54.4s\tremaining: 13.2s\n",
      "180:\ttest: 0.7410868\tbest: 0.7410868 (180)\ttotal: 1m 1s\tremaining: 6.46s\n",
      "199:\ttest: 0.7421636\tbest: 0.7421636 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7421636392\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "20:\tloss: 0.7421636\tbest: 0.7501471 (14)\ttotal: 23m 51s\tremaining: 12m 29s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 233ms\tremaining: 46.3s\n",
      "20:\ttest: 0.7153431\tbest: 0.7153431 (20)\ttotal: 7.19s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7301921\tbest: 0.7301921 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7363345\tbest: 0.7363345 (60)\ttotal: 21.5s\tremaining: 49s\n",
      "80:\ttest: 0.7401659\tbest: 0.7401856 (79)\ttotal: 28.3s\tremaining: 41.6s\n",
      "100:\ttest: 0.7423290\tbest: 0.7423290 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7440772\tbest: 0.7440772 (120)\ttotal: 41.4s\tremaining: 27s\n",
      "140:\ttest: 0.7451441\tbest: 0.7451441 (140)\ttotal: 48.1s\tremaining: 20.1s\n",
      "160:\ttest: 0.7460738\tbest: 0.7460738 (160)\ttotal: 54.6s\tremaining: 13.2s\n",
      "180:\ttest: 0.7466708\tbest: 0.7466708 (180)\ttotal: 1m 1s\tremaining: 6.42s\n",
      "199:\ttest: 0.7470675\tbest: 0.7470706 (198)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7470706135\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "21:\tloss: 0.7470706\tbest: 0.7501471 (14)\ttotal: 24m 58s\tremaining: 11m 21s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 261ms\tremaining: 52s\n",
      "20:\ttest: 0.7225821\tbest: 0.7225821 (20)\ttotal: 6.78s\tremaining: 57.8s\n",
      "40:\ttest: 0.7353872\tbest: 0.7353872 (40)\ttotal: 13.5s\tremaining: 52.5s\n",
      "60:\ttest: 0.7398937\tbest: 0.7398937 (60)\ttotal: 20.3s\tremaining: 46.3s\n",
      "80:\ttest: 0.7433685\tbest: 0.7433778 (78)\ttotal: 27.1s\tremaining: 39.9s\n",
      "100:\ttest: 0.7453004\tbest: 0.7453340 (99)\ttotal: 33.6s\tremaining: 32.9s\n",
      "120:\ttest: 0.7468075\tbest: 0.7468075 (120)\ttotal: 40.1s\tremaining: 26.2s\n",
      "140:\ttest: 0.7474972\tbest: 0.7475054 (139)\ttotal: 46.5s\tremaining: 19.4s\n",
      "160:\ttest: 0.7479784\tbest: 0.7479784 (160)\ttotal: 52.9s\tremaining: 12.8s\n",
      "180:\ttest: 0.7484241\tbest: 0.7484526 (179)\ttotal: 59.3s\tremaining: 6.22s\n",
      "199:\ttest: 0.7487468\tbest: 0.7487472 (198)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7487472057\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "22:\tloss: 0.7487472\tbest: 0.7501471 (14)\ttotal: 26m 3s\tremaining: 10m 11s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 240ms\tremaining: 47.8s\n",
      "20:\ttest: 0.7272551\tbest: 0.7272551 (20)\ttotal: 7.2s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7386984\tbest: 0.7386984 (40)\ttotal: 14.6s\tremaining: 56.8s\n",
      "60:\ttest: 0.7419438\tbest: 0.7419438 (60)\ttotal: 22.5s\tremaining: 51.3s\n",
      "80:\ttest: 0.7448588\tbest: 0.7448588 (80)\ttotal: 29.3s\tremaining: 43s\n",
      "100:\ttest: 0.7466908\tbest: 0.7466908 (100)\ttotal: 36.4s\tremaining: 35.7s\n",
      "120:\ttest: 0.7474343\tbest: 0.7474343 (120)\ttotal: 42.5s\tremaining: 27.8s\n",
      "140:\ttest: 0.7480092\tbest: 0.7480092 (140)\ttotal: 49.1s\tremaining: 20.6s\n",
      "160:\ttest: 0.7484148\tbest: 0.7484318 (159)\ttotal: 55.3s\tremaining: 13.4s\n",
      "180:\ttest: 0.7489792\tbest: 0.7489792 (180)\ttotal: 1m 1s\tremaining: 6.48s\n",
      "199:\ttest: 0.7494230\tbest: 0.7494230 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7494229948\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "23:\tloss: 0.7494230\tbest: 0.7501471 (14)\ttotal: 27m 12s\tremaining: 9m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7034370\tbest: 0.7034370 (20)\ttotal: 6.84s\tremaining: 58.3s\n",
      "40:\ttest: 0.7186187\tbest: 0.7186187 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7279600\tbest: 0.7279600 (60)\ttotal: 22.1s\tremaining: 50.5s\n",
      "80:\ttest: 0.7331008\tbest: 0.7331008 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7367479\tbest: 0.7367479 (100)\ttotal: 40.7s\tremaining: 39.9s\n",
      "120:\ttest: 0.7396022\tbest: 0.7396022 (120)\ttotal: 48.3s\tremaining: 31.6s\n",
      "140:\ttest: 0.7414634\tbest: 0.7414634 (140)\ttotal: 56.3s\tremaining: 23.6s\n",
      "160:\ttest: 0.7429885\tbest: 0.7429885 (160)\ttotal: 1m 3s\tremaining: 15.5s\n",
      "180:\ttest: 0.7442519\tbest: 0.7442519 (180)\ttotal: 1m 11s\tremaining: 7.5s\n",
      "199:\ttest: 0.7451130\tbest: 0.7451130 (199)\ttotal: 1m 18s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7451130219\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "24:\tloss: 0.7451130\tbest: 0.7501471 (14)\ttotal: 28m 30s\tremaining: 7m 59s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 243ms\tremaining: 48.3s\n",
      "20:\ttest: 0.7183954\tbest: 0.7183954 (20)\ttotal: 8.34s\tremaining: 1m 11s\n",
      "40:\ttest: 0.7324296\tbest: 0.7324296 (40)\ttotal: 16s\tremaining: 1m 2s\n",
      "60:\ttest: 0.7379777\tbest: 0.7379777 (60)\ttotal: 24.1s\tremaining: 55s\n",
      "80:\ttest: 0.7417054\tbest: 0.7417054 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7441019\tbest: 0.7441019 (100)\ttotal: 39.6s\tremaining: 38.8s\n",
      "120:\ttest: 0.7451301\tbest: 0.7451361 (119)\ttotal: 46.6s\tremaining: 30.4s\n",
      "140:\ttest: 0.7464575\tbest: 0.7464575 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7474590\tbest: 0.7474590 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7476996\tbest: 0.7476996 (180)\ttotal: 1m 8s\tremaining: 7.14s\n",
      "199:\ttest: 0.7485463\tbest: 0.7485519 (198)\ttotal: 1m 14s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7485518556\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "25:\tloss: 0.7485519\tbest: 0.7501471 (14)\ttotal: 29m 45s\tremaining: 6m 52s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 228ms\tremaining: 45.3s\n",
      "20:\ttest: 0.7266890\tbest: 0.7266890 (20)\ttotal: 7.23s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7379538\tbest: 0.7379538 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7432693\tbest: 0.7432693 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7452448\tbest: 0.7452633 (79)\ttotal: 28.1s\tremaining: 41.3s\n",
      "100:\ttest: 0.7466005\tbest: 0.7466005 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7477099\tbest: 0.7477099 (120)\ttotal: 41.2s\tremaining: 26.9s\n",
      "140:\ttest: 0.7484400\tbest: 0.7484400 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7486040\tbest: 0.7486128 (151)\ttotal: 53.6s\tremaining: 13s\n",
      "180:\ttest: 0.7489293\tbest: 0.7489661 (177)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7497762\tbest: 0.7497762 (199)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.749776223\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "26:\tloss: 0.7497762\tbest: 0.7501471 (14)\ttotal: 30m 51s\tremaining: 5m 42s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 221ms\tremaining: 44s\n",
      "20:\ttest: 0.7329100\tbest: 0.7329100 (20)\ttotal: 7.42s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7403687\tbest: 0.7403687 (40)\ttotal: 14.5s\tremaining: 56.3s\n",
      "60:\ttest: 0.7442135\tbest: 0.7442135 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7458341\tbest: 0.7458341 (80)\ttotal: 27.7s\tremaining: 40.7s\n",
      "100:\ttest: 0.7469134\tbest: 0.7469134 (100)\ttotal: 34.4s\tremaining: 33.7s\n",
      "120:\ttest: 0.7477579\tbest: 0.7477579 (120)\ttotal: 40.9s\tremaining: 26.7s\n",
      "140:\ttest: 0.7482154\tbest: 0.7482154 (140)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7487303\tbest: 0.7487768 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "180:\ttest: 0.7493876\tbest: 0.7493876 (180)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7495236\tbest: 0.7495931 (195)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7495930754\n",
      "bestIteration = 195\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "27:\tloss: 0.7495931\tbest: 0.7501471 (14)\ttotal: 31m 57s\tremaining: 4m 33s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 283ms\tremaining: 56.2s\n",
      "20:\ttest: 0.7059916\tbest: 0.7059916 (20)\ttotal: 6.93s\tremaining: 59s\n",
      "40:\ttest: 0.7231107\tbest: 0.7231107 (40)\ttotal: 14.9s\tremaining: 57.7s\n",
      "60:\ttest: 0.7307839\tbest: 0.7307839 (60)\ttotal: 22.6s\tremaining: 51.4s\n",
      "80:\ttest: 0.7353967\tbest: 0.7353967 (80)\ttotal: 30.3s\tremaining: 44.5s\n",
      "100:\ttest: 0.7386240\tbest: 0.7386240 (100)\ttotal: 38.2s\tremaining: 37.5s\n",
      "120:\ttest: 0.7410789\tbest: 0.7410789 (120)\ttotal: 46.2s\tremaining: 30.2s\n",
      "140:\ttest: 0.7426301\tbest: 0.7426301 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7441156\tbest: 0.7441156 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7452739\tbest: 0.7452739 (180)\ttotal: 1m 9s\tremaining: 7.26s\n",
      "199:\ttest: 0.7460072\tbest: 0.7460072 (199)\ttotal: 1m 16s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7460072366\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "28:\tloss: 0.7460072\tbest: 0.7501471 (14)\ttotal: 33m 13s\tremaining: 3m 26s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 286ms\tremaining: 56.8s\n",
      "20:\ttest: 0.7213168\tbest: 0.7213168 (20)\ttotal: 7.97s\tremaining: 1m 7s\n",
      "40:\ttest: 0.7355076\tbest: 0.7355076 (40)\ttotal: 15.7s\tremaining: 1m\n",
      "60:\ttest: 0.7408197\tbest: 0.7408197 (60)\ttotal: 23.7s\tremaining: 54s\n",
      "80:\ttest: 0.7438093\tbest: 0.7438093 (80)\ttotal: 33s\tremaining: 48.4s\n",
      "100:\ttest: 0.7455383\tbest: 0.7455383 (100)\ttotal: 40.4s\tremaining: 39.6s\n",
      "120:\ttest: 0.7471059\tbest: 0.7471059 (120)\ttotal: 47.8s\tremaining: 31.2s\n",
      "140:\ttest: 0.7481037\tbest: 0.7481037 (140)\ttotal: 54.9s\tremaining: 23s\n",
      "160:\ttest: 0.7488365\tbest: 0.7488365 (160)\ttotal: 1m 1s\tremaining: 15s\n",
      "180:\ttest: 0.7495665\tbest: 0.7495665 (180)\ttotal: 1m 8s\tremaining: 7.22s\n",
      "199:\ttest: 0.7500661\tbest: 0.7500672 (198)\ttotal: 1m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7500671645\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "29:\tloss: 0.7500672\tbest: 0.7501471 (14)\ttotal: 34m 29s\tremaining: 2m 17s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 280ms\tremaining: 55.7s\n",
      "20:\ttest: 0.7312267\tbest: 0.7312267 (20)\ttotal: 8.1s\tremaining: 1m 9s\n",
      "40:\ttest: 0.7398249\tbest: 0.7398249 (40)\ttotal: 16.3s\tremaining: 1m 3s\n",
      "60:\ttest: 0.7438871\tbest: 0.7438964 (59)\ttotal: 25.2s\tremaining: 57.4s\n",
      "80:\ttest: 0.7454393\tbest: 0.7454461 (78)\ttotal: 33.2s\tremaining: 48.7s\n",
      "100:\ttest: 0.7471704\tbest: 0.7472111 (99)\ttotal: 41.4s\tremaining: 40.5s\n",
      "120:\ttest: 0.7477966\tbest: 0.7477966 (120)\ttotal: 48.9s\tremaining: 31.9s\n",
      "140:\ttest: 0.7489173\tbest: 0.7489173 (140)\ttotal: 57.2s\tremaining: 23.9s\n",
      "160:\ttest: 0.7496694\tbest: 0.7496694 (160)\ttotal: 1m 5s\tremaining: 15.8s\n",
      "180:\ttest: 0.7498144\tbest: 0.7500010 (174)\ttotal: 1m 12s\tremaining: 7.66s\n",
      "199:\ttest: 0.7501471\tbest: 0.7501471 (199)\ttotal: 1m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7501471068\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "30:\tloss: 0.7501471\tbest: 0.7501471 (14)\ttotal: 35m 49s\tremaining: 1m 9s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 330ms\tremaining: 1m 5s\n",
      "20:\ttest: 0.7336760\tbest: 0.7336760 (20)\ttotal: 8.78s\tremaining: 1m 14s\n",
      "40:\ttest: 0.7421537\tbest: 0.7421537 (40)\ttotal: 16.9s\tremaining: 1m 5s\n",
      "60:\ttest: 0.7457007\tbest: 0.7457007 (60)\ttotal: 25s\tremaining: 57s\n",
      "80:\ttest: 0.7472166\tbest: 0.7472166 (80)\ttotal: 33.4s\tremaining: 49.1s\n",
      "100:\ttest: 0.7482485\tbest: 0.7482485 (100)\ttotal: 41.5s\tremaining: 40.6s\n",
      "120:\ttest: 0.7489985\tbest: 0.7490256 (119)\ttotal: 49.4s\tremaining: 32.2s\n",
      "140:\ttest: 0.7490803\tbest: 0.7491625 (129)\ttotal: 56.8s\tremaining: 23.7s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7491625186\n",
      "bestIteration = 129\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "31:\tloss: 0.7491625\tbest: 0.7501471 (14)\ttotal: 36m 49s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5466240\tbest: 0.5466240 (0)\ttotal: 308ms\tremaining: 1m 1s\n",
      "20:\ttest: 0.7280062\tbest: 0.7280062 (20)\ttotal: 7.44s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7372597\tbest: 0.7372597 (40)\ttotal: 14.3s\tremaining: 55.6s\n",
      "60:\ttest: 0.7412573\tbest: 0.7412573 (60)\ttotal: 20.9s\tremaining: 47.5s\n",
      "80:\ttest: 0.7433852\tbest: 0.7433852 (80)\ttotal: 27.2s\tremaining: 40s\n",
      "100:\ttest: 0.7441864\tbest: 0.7441900 (99)\ttotal: 33.5s\tremaining: 32.8s\n",
      "120:\ttest: 0.7447557\tbest: 0.7448117 (117)\ttotal: 39.3s\tremaining: 25.7s\n",
      "140:\ttest: 0.7449382\tbest: 0.7450386 (133)\ttotal: 45.3s\tremaining: 18.9s\n",
      "160:\ttest: 0.7453059\tbest: 0.7453059 (160)\ttotal: 51.4s\tremaining: 12.5s\n",
      "180:\ttest: 0.7456439\tbest: 0.7456526 (176)\ttotal: 57.5s\tremaining: 6.03s\n",
      "199:\ttest: 0.7458717\tbest: 0.7458717 (199)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.745871653\n",
      "bestIteration = 199\n",
      "\n",
      "Training on fold [1/3]\n",
      "0:\ttest: 0.5030965\tbest: 0.5030965 (0)\ttotal: 208ms\tremaining: 41.4s\n",
      "20:\ttest: 0.7269786\tbest: 0.7269786 (20)\ttotal: 6.96s\tremaining: 59.3s\n",
      "40:\ttest: 0.7359736\tbest: 0.7359736 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7395899\tbest: 0.7395899 (60)\ttotal: 20.7s\tremaining: 47.1s\n",
      "80:\ttest: 0.7424179\tbest: 0.7424200 (79)\ttotal: 27.4s\tremaining: 40.2s\n",
      "100:\ttest: 0.7438677\tbest: 0.7438677 (100)\ttotal: 33.8s\tremaining: 33.1s\n",
      "120:\ttest: 0.7449566\tbest: 0.7449770 (117)\ttotal: 40.6s\tremaining: 26.5s\n",
      "140:\ttest: 0.7450859\tbest: 0.7451233 (135)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7453141\tbest: 0.7454346 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "\n",
      "bestTest = 0.7454346082\n",
      "bestIteration = 159\n",
      "\n",
      "Training on fold [2/3]\n",
      "0:\ttest: 0.5472850\tbest: 0.5472850 (0)\ttotal: 283ms\tremaining: 56.3s\n",
      "20:\ttest: 0.7259946\tbest: 0.7259946 (20)\ttotal: 7.25s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7372018\tbest: 0.7372018 (40)\ttotal: 14s\tremaining: 54.1s\n",
      "60:\ttest: 0.7409509\tbest: 0.7409509 (60)\ttotal: 20.2s\tremaining: 46.1s\n",
      "80:\ttest: 0.7430899\tbest: 0.7430904 (79)\ttotal: 26.3s\tremaining: 38.7s\n",
      "100:\ttest: 0.7441000\tbest: 0.7441192 (99)\ttotal: 32.6s\tremaining: 32s\n",
      "120:\ttest: 0.7449599\tbest: 0.7449599 (120)\ttotal: 40.4s\tremaining: 26.4s\n",
      "140:\ttest: 0.7455849\tbest: 0.7455849 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7460361\tbest: 0.7460840 (157)\ttotal: 53.2s\tremaining: 12.9s\n",
      "180:\ttest: 0.7466474\tbest: 0.7466474 (180)\ttotal: 59.4s\tremaining: 6.23s\n",
      "199:\ttest: 0.7467573\tbest: 0.7468616 (186)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7468616149\n",
      "bestIteration = 186\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'min_data_in_leaf': 3,\n",
       "  'depth': 7,\n",
       "  'learning_rate': 0.30000000000000004},\n",
       " 'cv_results': defaultdict(list,\n",
       "             {'iterations': [0,\n",
       "               1,\n",
       "               2,\n",
       "               3,\n",
       "               4,\n",
       "               5,\n",
       "               6,\n",
       "               7,\n",
       "               8,\n",
       "               9,\n",
       "               10,\n",
       "               11,\n",
       "               12,\n",
       "               13,\n",
       "               14,\n",
       "               15,\n",
       "               16,\n",
       "               17,\n",
       "               18,\n",
       "               19,\n",
       "               20,\n",
       "               21,\n",
       "               22,\n",
       "               23,\n",
       "               24,\n",
       "               25,\n",
       "               26,\n",
       "               27,\n",
       "               28,\n",
       "               29,\n",
       "               30,\n",
       "               31,\n",
       "               32,\n",
       "               33,\n",
       "               34,\n",
       "               35,\n",
       "               36,\n",
       "               37,\n",
       "               38,\n",
       "               39,\n",
       "               40,\n",
       "               41,\n",
       "               42,\n",
       "               43,\n",
       "               44,\n",
       "               45,\n",
       "               46,\n",
       "               47,\n",
       "               48,\n",
       "               49,\n",
       "               50,\n",
       "               51,\n",
       "               52,\n",
       "               53,\n",
       "               54,\n",
       "               55,\n",
       "               56,\n",
       "               57,\n",
       "               58,\n",
       "               59,\n",
       "               60,\n",
       "               61,\n",
       "               62,\n",
       "               63,\n",
       "               64,\n",
       "               65,\n",
       "               66,\n",
       "               67,\n",
       "               68,\n",
       "               69,\n",
       "               70,\n",
       "               71,\n",
       "               72,\n",
       "               73,\n",
       "               74,\n",
       "               75,\n",
       "               76,\n",
       "               77,\n",
       "               78,\n",
       "               79,\n",
       "               80,\n",
       "               81,\n",
       "               82,\n",
       "               83,\n",
       "               84,\n",
       "               85,\n",
       "               86,\n",
       "               87,\n",
       "               88,\n",
       "               89,\n",
       "               90,\n",
       "               91,\n",
       "               92,\n",
       "               93,\n",
       "               94,\n",
       "               95,\n",
       "               96,\n",
       "               97,\n",
       "               98,\n",
       "               99,\n",
       "               100,\n",
       "               101,\n",
       "               102,\n",
       "               103,\n",
       "               104,\n",
       "               105,\n",
       "               106,\n",
       "               107,\n",
       "               108,\n",
       "               109,\n",
       "               110,\n",
       "               111,\n",
       "               112,\n",
       "               113,\n",
       "               114,\n",
       "               115,\n",
       "               116,\n",
       "               117,\n",
       "               118,\n",
       "               119,\n",
       "               120,\n",
       "               121,\n",
       "               122,\n",
       "               123,\n",
       "               124,\n",
       "               125,\n",
       "               126,\n",
       "               127,\n",
       "               128,\n",
       "               129,\n",
       "               130,\n",
       "               131,\n",
       "               132,\n",
       "               133,\n",
       "               134,\n",
       "               135,\n",
       "               136,\n",
       "               137,\n",
       "               138,\n",
       "               139,\n",
       "               140,\n",
       "               141,\n",
       "               142,\n",
       "               143,\n",
       "               144,\n",
       "               145,\n",
       "               146,\n",
       "               147,\n",
       "               148,\n",
       "               149,\n",
       "               150,\n",
       "               151,\n",
       "               152,\n",
       "               153,\n",
       "               154,\n",
       "               155,\n",
       "               156,\n",
       "               157,\n",
       "               158,\n",
       "               159,\n",
       "               160,\n",
       "               161,\n",
       "               162,\n",
       "               163,\n",
       "               164,\n",
       "               165,\n",
       "               166,\n",
       "               167,\n",
       "               168,\n",
       "               169,\n",
       "               170,\n",
       "               171,\n",
       "               172,\n",
       "               173,\n",
       "               174,\n",
       "               175,\n",
       "               176,\n",
       "               177,\n",
       "               178,\n",
       "               179,\n",
       "               180,\n",
       "               181,\n",
       "               182,\n",
       "               183,\n",
       "               184,\n",
       "               185,\n",
       "               186,\n",
       "               187,\n",
       "               188,\n",
       "               189,\n",
       "               190,\n",
       "               191,\n",
       "               192,\n",
       "               193,\n",
       "               194,\n",
       "               195,\n",
       "               196,\n",
       "               197,\n",
       "               198,\n",
       "               199],\n",
       "              'test-AUC-mean': [0.5323351871134413,\n",
       "               0.5672485377872802,\n",
       "               0.6073729499291639,\n",
       "               0.6682448148498139,\n",
       "               0.6815865549484652,\n",
       "               0.6905379693067895,\n",
       "               0.697254815238835,\n",
       "               0.6989171525505835,\n",
       "               0.7034855810236008,\n",
       "               0.7081957164191247,\n",
       "               0.712240618753471,\n",
       "               0.7139975265978536,\n",
       "               0.7159752865593644,\n",
       "               0.7175685424543378,\n",
       "               0.7192737969143583,\n",
       "               0.7205048017282761,\n",
       "               0.7221750988457073,\n",
       "               0.7236253837036376,\n",
       "               0.724682731134001,\n",
       "               0.7257887194849176,\n",
       "               0.7269931493303302,\n",
       "               0.7279065089832814,\n",
       "               0.7288224791887279,\n",
       "               0.7293681792534473,\n",
       "               0.7300194439278632,\n",
       "               0.7309084709246666,\n",
       "               0.7314423107118962,\n",
       "               0.7318070239971508,\n",
       "               0.7326287091636717,\n",
       "               0.7330962965236035,\n",
       "               0.7338660846256045,\n",
       "               0.7341638368570527,\n",
       "               0.7347563041881463,\n",
       "               0.7350512845234406,\n",
       "               0.7353708112052444,\n",
       "               0.7355543177924654,\n",
       "               0.7358423387504356,\n",
       "               0.7361120806751931,\n",
       "               0.7363384454684726,\n",
       "               0.7365760315204622,\n",
       "               0.7368117247389444,\n",
       "               0.7370277696510957,\n",
       "               0.7375601115497495,\n",
       "               0.7377754549380248,\n",
       "               0.7378912040874498,\n",
       "               0.7380370290763554,\n",
       "               0.7382350999242915,\n",
       "               0.7384344775060164,\n",
       "               0.7385329708839873,\n",
       "               0.7385593695851984,\n",
       "               0.7387566201175919,\n",
       "               0.7389830207813505,\n",
       "               0.7391369383935688,\n",
       "               0.7393880118340554,\n",
       "               0.7395543786468469,\n",
       "               0.7396444015967685,\n",
       "               0.7398647277582878,\n",
       "               0.7400291683973553,\n",
       "               0.7402085796786584,\n",
       "               0.7403351133767547,\n",
       "               0.7405993644608135,\n",
       "               0.740807386585787,\n",
       "               0.7409894061838483,\n",
       "               0.7411666514507255,\n",
       "               0.741305777489257,\n",
       "               0.7413729131333818,\n",
       "               0.7414738583117991,\n",
       "               0.741625442448362,\n",
       "               0.7417041600133513,\n",
       "               0.7418539191210747,\n",
       "               0.7419784933896265,\n",
       "               0.742097296431333,\n",
       "               0.7422724865920779,\n",
       "               0.7423835930003587,\n",
       "               0.7424318242905521,\n",
       "               0.7425292580388412,\n",
       "               0.742646728928146,\n",
       "               0.7427824920624081,\n",
       "               0.7428386192241708,\n",
       "               0.7429419396670838,\n",
       "               0.7429643327710437,\n",
       "               0.7430798827049051,\n",
       "               0.7431569563374699,\n",
       "               0.7432715937467483,\n",
       "               0.7433677880138255,\n",
       "               0.7434412558096488,\n",
       "               0.7435074239438036,\n",
       "               0.7435534593720735,\n",
       "               0.7435571054726401,\n",
       "               0.7435775020787556,\n",
       "               0.7436365666175054,\n",
       "               0.7437064085778027,\n",
       "               0.7437422225170586,\n",
       "               0.7437764896028108,\n",
       "               0.7438178820318823,\n",
       "               0.74387061624648,\n",
       "               0.7438905727649447,\n",
       "               0.7439927661673247,\n",
       "               0.7440024146013199,\n",
       "               0.7440377978482045,\n",
       "               0.7440513821192641,\n",
       "               0.7441273484926604,\n",
       "               0.7441669408564753,\n",
       "               0.7441858556113946,\n",
       "               0.7442124979466755,\n",
       "               0.7442771576979249,\n",
       "               0.7443005728006197,\n",
       "               0.7443311953439778,\n",
       "               0.7443650456556239,\n",
       "               0.744397027291618,\n",
       "               0.74440267424395,\n",
       "               0.7444382190263493,\n",
       "               0.7445137351926846,\n",
       "               0.7445525301781267,\n",
       "               0.7447270665016884,\n",
       "               0.7447642871083064,\n",
       "               0.7448551349518983,\n",
       "               0.744901713303259,\n",
       "               0.7448661628733549,\n",
       "               0.7448505716325475,\n",
       "               0.7448907389692989,\n",
       "               0.7449061961955286,\n",
       "               0.7449644381774257,\n",
       "               0.744962178409649,\n",
       "               0.7449676824869712,\n",
       "               0.7449661186879172,\n",
       "               0.7450267738808226,\n",
       "               0.7450470288716411,\n",
       "               0.745069257522755,\n",
       "               0.7450743944343543,\n",
       "               0.7450800318421195,\n",
       "               0.7450750130676216,\n",
       "               0.7450995540641578,\n",
       "               0.7451100815027649,\n",
       "               0.7450914218059653,\n",
       "               0.7450837653500435,\n",
       "               0.7450892893651595,\n",
       "               0.7450785936404719,\n",
       "               0.7451303086800137,\n",
       "               0.7452062206328023,\n",
       "               0.7452029918855784,\n",
       "               0.7452044702790896,\n",
       "               0.7452078183243406,\n",
       "               0.7452132176265415,\n",
       "               0.7452586561354283,\n",
       "               0.745277115252616,\n",
       "               0.7453254146410546,\n",
       "               0.7453198801360831,\n",
       "               0.7453331444036664,\n",
       "               0.7453590264265971,\n",
       "               0.7454014770083107,\n",
       "               0.7453813748399947,\n",
       "               0.7454102949284719,\n",
       "               0.7453979245570119,\n",
       "               0.7454442849065378,\n",
       "               0.7454711749955631,\n",
       "               0.7455215502306715,\n",
       "               0.7455523830208541,\n",
       "               0.7455553145576462,\n",
       "               0.7455870789437258,\n",
       "               0.7455520387048767,\n",
       "               0.7455574928939246,\n",
       "               0.7455483202326004,\n",
       "               0.7455471571826099,\n",
       "               0.745548586218574,\n",
       "               0.7455594295195236,\n",
       "               0.7456040841891513,\n",
       "               0.745622476326688,\n",
       "               0.7456657491240811,\n",
       "               0.7456576837885875,\n",
       "               0.7456868405923801,\n",
       "               0.7457052704803594,\n",
       "               0.7456997394776056,\n",
       "               0.7457246870874767,\n",
       "               0.7458033622719472,\n",
       "               0.7458844799535762,\n",
       "               0.7458810888184993,\n",
       "               0.7458754325125145,\n",
       "               0.7458818436495037,\n",
       "               0.7458766029947742,\n",
       "               0.7458869207620135,\n",
       "               0.7458900103891483,\n",
       "               0.7459060247495494,\n",
       "               0.7459091832116601,\n",
       "               0.745936544993763,\n",
       "               0.7459397310864073,\n",
       "               0.7459678665192087,\n",
       "               0.7459554688132831,\n",
       "               0.7459666337707551,\n",
       "               0.7459636588161501,\n",
       "               0.7459680057902269,\n",
       "               0.7459433389010277,\n",
       "               0.7459593686895082,\n",
       "               0.7459496736175785,\n",
       "               0.7459615684481582,\n",
       "               0.7459976774971092,\n",
       "               0.7459951829498932,\n",
       "               0.7459921037133087,\n",
       "               0.7459959742143821,\n",
       "               0.7459994991456217],\n",
       "              'test-AUC-std': [0.02532359239281949,\n",
       "               0.008466858922239156,\n",
       "               0.004585950280540858,\n",
       "               0.004815449956083742,\n",
       "               0.0019913225430410898,\n",
       "               0.0036590378327971533,\n",
       "               0.0010377349078268805,\n",
       "               0.0018098267662474293,\n",
       "               0.0021389364814496147,\n",
       "               0.0010816210541981707,\n",
       "               0.0009109152685680217,\n",
       "               0.002434088331193539,\n",
       "               0.0016368986811836261,\n",
       "               0.0023295211697072406,\n",
       "               0.002577920523229613,\n",
       "               0.0023204793795475248,\n",
       "               0.0015796647918663812,\n",
       "               0.0012675131083679748,\n",
       "               0.0005959262009161987,\n",
       "               0.0006274494740008236,\n",
       "               0.0010058442669958383,\n",
       "               0.0012940433041228297,\n",
       "               0.0013456221322952367,\n",
       "               0.001144895839357038,\n",
       "               0.0012123648314638605,\n",
       "               0.0013366982727912094,\n",
       "               0.001339040058419934,\n",
       "               0.0012871037044102289,\n",
       "               0.0013943295534002281,\n",
       "               0.0012317236773407934,\n",
       "               0.0009226238463269436,\n",
       "               0.0009207596209577093,\n",
       "               0.0008880951451911988,\n",
       "               0.0008277279030819567,\n",
       "               0.0009236325653394494,\n",
       "               0.0008266193498340217,\n",
       "               0.000908486289915637,\n",
       "               0.0008861306885369425,\n",
       "               0.0007098059362100951,\n",
       "               0.0006512860068509332,\n",
       "               0.0007263825197793467,\n",
       "               0.0007253717277105685,\n",
       "               0.0005512314198523541,\n",
       "               0.0007581512258018975,\n",
       "               0.0006857224396737535,\n",
       "               0.0007797304147385075,\n",
       "               0.0008170636530197181,\n",
       "               0.0007581922769637968,\n",
       "               0.0009294312946726416,\n",
       "               0.0009398192095068884,\n",
       "               0.0009126326959371571,\n",
       "               0.0009698487471650067,\n",
       "               0.0009380196943498978,\n",
       "               0.0007515349890663632,\n",
       "               0.0006187828199820805,\n",
       "               0.0005057435706896258,\n",
       "               0.0005871467477125114,\n",
       "               0.000615019317967743,\n",
       "               0.0007190485263371711,\n",
       "               0.0008866482515136522,\n",
       "               0.0008875667375294412,\n",
       "               0.0007285008398668062,\n",
       "               0.0008960573508490511,\n",
       "               0.0008079691274163551,\n",
       "               0.0007062063761600933,\n",
       "               0.00069487865013244,\n",
       "               0.0007233722926929515,\n",
       "               0.0005597041835083738,\n",
       "               0.0005264861194173963,\n",
       "               0.0004724843053420606,\n",
       "               0.0005093057362766326,\n",
       "               0.0005173542554170649,\n",
       "               0.0004808240967711858,\n",
       "               0.0004109066501969464,\n",
       "               0.00044570281115598973,\n",
       "               0.0004272792413529454,\n",
       "               0.000439054745604249,\n",
       "               0.00041803362579330325,\n",
       "               0.0004773632261178682,\n",
       "               0.00046583858069193973,\n",
       "               0.0004957508939481096,\n",
       "               0.0005727322649158489,\n",
       "               0.000561120993353562,\n",
       "               0.00042532409476542867,\n",
       "               0.0003669174434163326,\n",
       "               0.00037835918011397277,\n",
       "               0.0003135152491979665,\n",
       "               0.0003497234273534862,\n",
       "               0.00027304021948943374,\n",
       "               0.0002696544315423864,\n",
       "               0.0003023229851023641,\n",
       "               0.00025026774289106294,\n",
       "               0.0002469934991097206,\n",
       "               0.00022811245710394585,\n",
       "               0.00022213018620281731,\n",
       "               0.00020629942162416544,\n",
       "               0.00017864512502352914,\n",
       "               0.00017977635554250856,\n",
       "               0.0001847724069356753,\n",
       "               0.0002053234473095896,\n",
       "               0.0001648434464886782,\n",
       "               0.00020761385073964593,\n",
       "               0.00017284493789659886,\n",
       "               0.00017687275828017155,\n",
       "               0.00017746676757125648,\n",
       "               0.0002437725109907605,\n",
       "               0.0002940307387516383,\n",
       "               0.0003006814677872576,\n",
       "               0.0003304338159713527,\n",
       "               0.00032571608564376794,\n",
       "               0.00031200116755029656,\n",
       "               0.0003233719214620155,\n",
       "               0.00021271755789855547,\n",
       "               0.00019601258827327953,\n",
       "               5.5581490153552954e-05,\n",
       "               6.25634462612514e-05,\n",
       "               4.728296292219027e-05,\n",
       "               8.362111438530978e-05,\n",
       "               7.324550546186718e-05,\n",
       "               8.548065158074559e-05,\n",
       "               0.00011692009061906363,\n",
       "               0.000138881805328732,\n",
       "               0.000159372486809559,\n",
       "               0.00016631320414211336,\n",
       "               0.0001540334681279224,\n",
       "               0.00015879743220322478,\n",
       "               0.00013113605028878332,\n",
       "               0.0001007652880926456,\n",
       "               0.00011658553193894731,\n",
       "               0.00011585040775047589,\n",
       "               6.494702932034623e-05,\n",
       "               7.290157957630871e-05,\n",
       "               9.225930628721633e-05,\n",
       "               6.856129893881642e-05,\n",
       "               8.783686234356395e-05,\n",
       "               9.637590039608928e-05,\n",
       "               9.994698282443942e-05,\n",
       "               6.393479474443715e-05,\n",
       "               0.00013651364132303686,\n",
       "               0.0003143939839207871,\n",
       "               0.000338909681777747,\n",
       "               0.0003266314328202843,\n",
       "               0.0003432787309968498,\n",
       "               0.00037432162328763184,\n",
       "               0.00034363456493125894,\n",
       "               0.00035450351200104084,\n",
       "               0.00035323490055288895,\n",
       "               0.00036865662560145657,\n",
       "               0.0003782092031153797,\n",
       "               0.0003926083943459441,\n",
       "               0.0003963172431402646,\n",
       "               0.00039696883222072743,\n",
       "               0.00038464737381002504,\n",
       "               0.0003971747478911386,\n",
       "               0.00040905672666516406,\n",
       "               0.0004183232031727523,\n",
       "               0.0004381367373615701,\n",
       "               0.0004672035306302684,\n",
       "               0.0004367392241883043,\n",
       "               0.00040946734858020973,\n",
       "               0.0004192579104515046,\n",
       "               0.0004193533568755244,\n",
       "               0.00041762369170116943,\n",
       "               0.0004192615712828445,\n",
       "               0.0004362101258684112,\n",
       "               0.00042009784981146027,\n",
       "               0.00041748720069425047,\n",
       "               0.00040888224065514,\n",
       "               0.00038802859342702844,\n",
       "               0.00037026441296878425,\n",
       "               0.00037861962453010386,\n",
       "               0.00037692481417933614,\n",
       "               0.00043027335014486477,\n",
       "               0.00046121400920888083,\n",
       "               0.0005446723221166341,\n",
       "               0.0006442071833715171,\n",
       "               0.0006351865003248502,\n",
       "               0.0006310578465817754,\n",
       "               0.0006551561172386533,\n",
       "               0.0006502535147695251,\n",
       "               0.0006727224133376267,\n",
       "               0.0006781888070109002,\n",
       "               0.0007001067016156205,\n",
       "               0.0007298574057930118,\n",
       "               0.0007459932046994607,\n",
       "               0.0007589191745506932,\n",
       "               0.0007886918261306194,\n",
       "               0.0007713032934364513,\n",
       "               0.0007866913976575951,\n",
       "               0.0007817532214235002,\n",
       "               0.0007707687804994566,\n",
       "               0.0007440474526188147,\n",
       "               0.00072979931181204,\n",
       "               0.0007106698589331393,\n",
       "               0.0007112330868961144,\n",
       "               0.0007289071317548428,\n",
       "               0.0007106591616577817,\n",
       "               0.0007046394371559195,\n",
       "               0.0007053257487551718,\n",
       "               0.0007026838108733379],\n",
       "              'test-Logloss-mean': [0.35180698124470894,\n",
       "               0.23026996637385042,\n",
       "               0.18397858416083512,\n",
       "               0.16388931064488058,\n",
       "               0.15536707191661384,\n",
       "               0.1510823448581409,\n",
       "               0.14897560631064996,\n",
       "               0.1478944726648073,\n",
       "               0.14696395529728792,\n",
       "               0.14631252901941508,\n",
       "               0.145841463446266,\n",
       "               0.14553385428737553,\n",
       "               0.14517442685501203,\n",
       "               0.1449521560323742,\n",
       "               0.1447005467928781,\n",
       "               0.1445106367788774,\n",
       "               0.14432068138816234,\n",
       "               0.14414496820162906,\n",
       "               0.1439847337617169,\n",
       "               0.14387398888328623,\n",
       "               0.14372076145221693,\n",
       "               0.14362783120004263,\n",
       "               0.1435155113638539,\n",
       "               0.14343977554118434,\n",
       "               0.14335047017673552,\n",
       "               0.14324950377697632,\n",
       "               0.14318476583632203,\n",
       "               0.14314203109113346,\n",
       "               0.14304399774300544,\n",
       "               0.1429828010495902,\n",
       "               0.1428858103828851,\n",
       "               0.1428420702822026,\n",
       "               0.14277711119100542,\n",
       "               0.1427274278415724,\n",
       "               0.14268195075713838,\n",
       "               0.14264702864002174,\n",
       "               0.14261450467344372,\n",
       "               0.1425712541522943,\n",
       "               0.14254550909796357,\n",
       "               0.1425100209513985,\n",
       "               0.14248380388225937,\n",
       "               0.14245558471070216,\n",
       "               0.1423873084659136,\n",
       "               0.1423520416533882,\n",
       "               0.1423287957389005,\n",
       "               0.14231113232960577,\n",
       "               0.14228566315011004,\n",
       "               0.14225297869098777,\n",
       "               0.14224203548224248,\n",
       "               0.14223670194442095,\n",
       "               0.14221032597081132,\n",
       "               0.14217898327163894,\n",
       "               0.14216172290761173,\n",
       "               0.14213419209220823,\n",
       "               0.14211192770888062,\n",
       "               0.14210231526719985,\n",
       "               0.1420770087522377,\n",
       "               0.14204410282256663,\n",
       "               0.14202606162162001,\n",
       "               0.14201045814390814,\n",
       "               0.14197721330507398,\n",
       "               0.1419531007127323,\n",
       "               0.14192575821498002,\n",
       "               0.14190428045063205,\n",
       "               0.14189037205556174,\n",
       "               0.14188098536520286,\n",
       "               0.14186819654531244,\n",
       "               0.14184966432416232,\n",
       "               0.14184200720921683,\n",
       "               0.14182600533948445,\n",
       "               0.14181088796536323,\n",
       "               0.14180080846969337,\n",
       "               0.14178081205392776,\n",
       "               0.14176719688894732,\n",
       "               0.14175871266596737,\n",
       "               0.1417443225176791,\n",
       "               0.14173009605088135,\n",
       "               0.1417141639351123,\n",
       "               0.1417039128790523,\n",
       "               0.14168981376374737,\n",
       "               0.14168606369182637,\n",
       "               0.14167251170840303,\n",
       "               0.14166279303869245,\n",
       "               0.1416498733847369,\n",
       "               0.14163891132959164,\n",
       "               0.1416314475128556,\n",
       "               0.14162375164882954,\n",
       "               0.141617063539044,\n",
       "               0.14161587546664803,\n",
       "               0.14161372009561105,\n",
       "               0.14160998769666766,\n",
       "               0.14159501406094213,\n",
       "               0.14159161971921572,\n",
       "               0.14158785351263456,\n",
       "               0.14158697940660855,\n",
       "               0.1415806387129128,\n",
       "               0.1415760637177954,\n",
       "               0.14155996488760939,\n",
       "               0.14155938399343068,\n",
       "               0.14155387493436364,\n",
       "               0.14155017932230232,\n",
       "               0.1415419595162399,\n",
       "               0.14153822758747733,\n",
       "               0.1415350291456736,\n",
       "               0.1415312814443288,\n",
       "               0.14152165555152485,\n",
       "               0.14151630013793481,\n",
       "               0.14151476059711635,\n",
       "               0.14150941452489504,\n",
       "               0.14150600003735156,\n",
       "               0.1415052428427487,\n",
       "               0.14149906230146095,\n",
       "               0.1414926589488338,\n",
       "               0.14148659525263066,\n",
       "               0.1414660278632282,\n",
       "               0.14146043397263794,\n",
       "               0.14144873823631485,\n",
       "               0.14144002355354315,\n",
       "               0.14144415240665806,\n",
       "               0.14144604054162202,\n",
       "               0.14144125860299242,\n",
       "               0.14143835166125726,\n",
       "               0.14142670970835183,\n",
       "               0.1414250409164651,\n",
       "               0.1414200430209583,\n",
       "               0.14142080227367038,\n",
       "               0.1414180805128696,\n",
       "               0.14141618674546222,\n",
       "               0.14141078615690797,\n",
       "               0.14141248458233396,\n",
       "               0.1414112349496702,\n",
       "               0.14141413588947427,\n",
       "               0.14141265949537735,\n",
       "               0.14141145199939856,\n",
       "               0.14141178290805975,\n",
       "               0.14141114333444463,\n",
       "               0.14141080390410607,\n",
       "               0.14141310715010907,\n",
       "               0.1414108689820429,\n",
       "               0.14139944188870576,\n",
       "               0.1413977009161523,\n",
       "               0.1413963499663271,\n",
       "               0.1413958386481126,\n",
       "               0.14139585639743146,\n",
       "               0.14138849366175707,\n",
       "               0.14138650872046887,\n",
       "               0.1413791420381603,\n",
       "               0.1413770472558733,\n",
       "               0.14137833331128102,\n",
       "               0.14137638136198225,\n",
       "               0.14137064295410784,\n",
       "               0.14137072728782887,\n",
       "               0.14136202210448945,\n",
       "               0.14136268221242856,\n",
       "               0.14135602703029135,\n",
       "               0.141349377286439,\n",
       "               0.14134609326019285,\n",
       "               0.14134338895287124,\n",
       "               0.14134162195493039,\n",
       "               0.14133863938359015,\n",
       "               0.14133996281159514,\n",
       "               0.14133855952276983,\n",
       "               0.14133807029450488,\n",
       "               0.1413378928908927,\n",
       "               0.1413373025229666,\n",
       "               0.14133475574633034,\n",
       "               0.14133037028239848,\n",
       "               0.1413275942109404,\n",
       "               0.1413251591967822,\n",
       "               0.14132296669682873,\n",
       "               0.14131770421067583,\n",
       "               0.14131642574302705,\n",
       "               0.14131841985522717,\n",
       "               0.1413192589439878,\n",
       "               0.14130964437704643,\n",
       "               0.1413012829024133,\n",
       "               0.1412990966681225,\n",
       "               0.1413001065928621,\n",
       "               0.1412967970987309,\n",
       "               0.1412967750989941,\n",
       "               0.14129504703454443,\n",
       "               0.1412946356937097,\n",
       "               0.14129258051240134,\n",
       "               0.14129059960092724,\n",
       "               0.1412848249743339,\n",
       "               0.14128466619544164,\n",
       "               0.14128254634476312,\n",
       "               0.1412836482114419,\n",
       "               0.14128390733264062,\n",
       "               0.14128330202817227,\n",
       "               0.14128161916479465,\n",
       "               0.14128313583350016,\n",
       "               0.1412802605935267,\n",
       "               0.14128111301307322,\n",
       "               0.14128100561449025,\n",
       "               0.14127810296945267,\n",
       "               0.14127796915402568,\n",
       "               0.1412775856065105,\n",
       "               0.14127725974326685,\n",
       "               0.14127669077226337],\n",
       "              'test-Logloss-std': [0.0005837574209663514,\n",
       "               0.0002172022677762946,\n",
       "               0.0002969095093022844,\n",
       "               0.0002555372726815721,\n",
       "               0.00021283460587267903,\n",
       "               0.0004202418831078934,\n",
       "               0.00015571945343389128,\n",
       "               0.00029774229598745,\n",
       "               0.000255313187081981,\n",
       "               7.651501357620769e-05,\n",
       "               6.601226108440212e-05,\n",
       "               0.00015527991153946248,\n",
       "               9.569326022227246e-05,\n",
       "               0.00013837819247912122,\n",
       "               0.00017732068608738,\n",
       "               0.00014250898480069082,\n",
       "               8.814574624388235e-05,\n",
       "               7.139889431199906e-05,\n",
       "               1.607344990252169e-05,\n",
       "               2.866477445658262e-05,\n",
       "               5.356389392858374e-05,\n",
       "               8.259948087794028e-05,\n",
       "               7.444936079662396e-05,\n",
       "               7.77515981070996e-05,\n",
       "               5.640494310542395e-05,\n",
       "               7.643880984889307e-05,\n",
       "               7.105094830057104e-05,\n",
       "               7.496255602883405e-05,\n",
       "               0.00011145816238551958,\n",
       "               9.38104898945748e-05,\n",
       "               8.372438310563817e-05,\n",
       "               8.986669510752975e-05,\n",
       "               0.00012254943227647165,\n",
       "               0.00012187171824515203,\n",
       "               0.00013883099459294484,\n",
       "               0.00013211310021562464,\n",
       "               0.00014585429006628737,\n",
       "               0.00015549452387443895,\n",
       "               0.00013651174479778572,\n",
       "               0.00012415571057365965,\n",
       "               0.00014041173981286227,\n",
       "               0.00013620109597409572,\n",
       "               0.00011235786649752728,\n",
       "               0.00011653202054386406,\n",
       "               0.00010210442476872522,\n",
       "               0.00011273146632724944,\n",
       "               0.00011687462910787654,\n",
       "               0.00010945650691776421,\n",
       "               0.00012728800618267363,\n",
       "               0.00012925052853179211,\n",
       "               0.00013983451997903303,\n",
       "               0.0001523510697805438,\n",
       "               0.0001594896129564806,\n",
       "               0.00013648011382222085,\n",
       "               0.00012407535496673214,\n",
       "               0.00011141986347715864,\n",
       "               0.00011830915317039878,\n",
       "               0.0001259481216724747,\n",
       "               0.0001439758503337618,\n",
       "               0.00015771086775209515,\n",
       "               0.00014812792851109184,\n",
       "               0.00012962203553671252,\n",
       "               0.00015078396066287738,\n",
       "               0.0001423358562330602,\n",
       "               0.0001306194204554875,\n",
       "               0.00012721313461129983,\n",
       "               0.00013043100454917224,\n",
       "               0.00010932449382016015,\n",
       "               0.0001090088811735865,\n",
       "               0.00010094318949279182,\n",
       "               0.00010260780813623869,\n",
       "               9.950202183128705e-05,\n",
       "               0.00010097808467949662,\n",
       "               9.654888975527773e-05,\n",
       "               0.00010431894850519347,\n",
       "               0.0001017464208828689,\n",
       "               9.387029485048853e-05,\n",
       "               9.550527731257697e-05,\n",
       "               0.00010193501529597636,\n",
       "               0.00010983615484675611,\n",
       "               0.00011384966341538812,\n",
       "               0.0001230273342997177,\n",
       "               0.00012328518005062745,\n",
       "               0.00010278953997408501,\n",
       "               9.378491064992746e-05,\n",
       "               9.297002195327092e-05,\n",
       "               8.983828348207946e-05,\n",
       "               9.493597249738022e-05,\n",
       "               8.726891692369035e-05,\n",
       "               8.7130003331294e-05,\n",
       "               9.427663044962366e-05,\n",
       "               7.995372419299508e-05,\n",
       "               8.324923770092814e-05,\n",
       "               7.775548428555403e-05,\n",
       "               7.583759418347922e-05,\n",
       "               7.289448918584842e-05,\n",
       "               7.089323722283244e-05,\n",
       "               6.264991983508518e-05,\n",
       "               6.407420304045691e-05,\n",
       "               6.463002371439278e-05,\n",
       "               5.7335849856998854e-05,\n",
       "               6.246125505097976e-05,\n",
       "               5.979969665840396e-05,\n",
       "               6.046755540919396e-05,\n",
       "               5.980628880092118e-05,\n",
       "               6.610711467160652e-05,\n",
       "               7.404103454735078e-05,\n",
       "               7.557026894425236e-05,\n",
       "               8.150631755721151e-05,\n",
       "               7.22909314737312e-05,\n",
       "               7.319811159014766e-05,\n",
       "               7.563359492100841e-05,\n",
       "               6.068805251699489e-05,\n",
       "               5.890877355035975e-05,\n",
       "               4.38688628517993e-05,\n",
       "               4.427185538906972e-05,\n",
       "               4.487312522128361e-05,\n",
       "               3.4624741940115545e-05,\n",
       "               3.259873988833714e-05,\n",
       "               3.7310097541288e-05,\n",
       "               3.811389415132559e-05,\n",
       "               4.411542118701477e-05,\n",
       "               4.9759621052660846e-05,\n",
       "               4.981362418877432e-05,\n",
       "               4.71560938041917e-05,\n",
       "               4.879135435237901e-05,\n",
       "               4.798959782481898e-05,\n",
       "               4.6648647541788485e-05,\n",
       "               5.0882543414887834e-05,\n",
       "               5.205288624310396e-05,\n",
       "               4.891886052195218e-05,\n",
       "               4.8611591989005545e-05,\n",
       "               4.9077509982445886e-05,\n",
       "               4.737288865673105e-05,\n",
       "               4.779436275417745e-05,\n",
       "               4.5352558244441845e-05,\n",
       "               4.648221319766166e-05,\n",
       "               4.377061058012832e-05,\n",
       "               5.118881145351792e-05,\n",
       "               7.594619011424548e-05,\n",
       "               7.849900553625912e-05,\n",
       "               7.949688173696476e-05,\n",
       "               7.972078999701114e-05,\n",
       "               8.311798386173363e-05,\n",
       "               7.79851652020845e-05,\n",
       "               7.98279527995231e-05,\n",
       "               7.818287299813165e-05,\n",
       "               8.494478098622199e-05,\n",
       "               8.578208589125797e-05,\n",
       "               8.590563320240281e-05,\n",
       "               8.454305358862779e-05,\n",
       "               8.616326299728145e-05,\n",
       "               8.33815643417997e-05,\n",
       "               8.381480126212499e-05,\n",
       "               8.817837957241318e-05,\n",
       "               8.455625796519155e-05,\n",
       "               8.696261266478662e-05,\n",
       "               8.961305868389429e-05,\n",
       "               8.57325047236974e-05,\n",
       "               8.362210604100771e-05,\n",
       "               8.892350729529429e-05,\n",
       "               8.844021293350916e-05,\n",
       "               8.862459583290053e-05,\n",
       "               8.899522253085028e-05,\n",
       "               8.992056784820182e-05,\n",
       "               8.915130025969569e-05,\n",
       "               8.925661120271388e-05,\n",
       "               8.804937829189448e-05,\n",
       "               8.870027412321713e-05,\n",
       "               9.095366683031001e-05,\n",
       "               9.165466389866536e-05,\n",
       "               9.289472580313485e-05,\n",
       "               9.52925465615794e-05,\n",
       "               0.00010282733124660046,\n",
       "               0.00011196968165069219,\n",
       "               0.0001191153698187999,\n",
       "               0.00012134300142199919,\n",
       "               0.00012053436089811714,\n",
       "               0.00012762531319839942,\n",
       "               0.0001259248579265282,\n",
       "               0.0001302077458216833,\n",
       "               0.00013105892259699574,\n",
       "               0.00013410723834397532,\n",
       "               0.00013876626870233406,\n",
       "               0.00014404384456570665,\n",
       "               0.00014480416500771662,\n",
       "               0.0001472706684137815,\n",
       "               0.0001452581528960754,\n",
       "               0.0001443353205326422,\n",
       "               0.00014390817413346057,\n",
       "               0.00014502682958959478,\n",
       "               0.00014262283077907367,\n",
       "               0.0001414876946954318,\n",
       "               0.00013962905829049602,\n",
       "               0.00013851266644080633,\n",
       "               0.00013938581278184973,\n",
       "               0.0001385702005323355,\n",
       "               0.00013838900160632086,\n",
       "               0.00013823319820230331,\n",
       "               0.00013797027325572947],\n",
       "              'train-Logloss-mean': [0.35176721840712855,\n",
       "               0.23018842093806988,\n",
       "               0.1838362830940481,\n",
       "               0.1637560661806244,\n",
       "               0.15521621418876286,\n",
       "               0.15093659878292745,\n",
       "               0.1488188462995693,\n",
       "               0.1476920105594722,\n",
       "               0.14671708789146032,\n",
       "               0.14605019227412705,\n",
       "               0.14556231998984911,\n",
       "               0.1452189806667257,\n",
       "               0.14483764258838752,\n",
       "               0.14458950136337978,\n",
       "               0.1443172819396642,\n",
       "               0.14410078484716118,\n",
       "               0.14390084996005137,\n",
       "               0.14370604154415495,\n",
       "               0.14353251937575573,\n",
       "               0.14340012470374497,\n",
       "               0.14322025600933078,\n",
       "               0.14310482505853225,\n",
       "               0.14297094016168024,\n",
       "               0.14287329100760862,\n",
       "               0.14275130106845982,\n",
       "               0.14262723412591133,\n",
       "               0.1425432461616484,\n",
       "               0.14248055831343437,\n",
       "               0.1423648128953731,\n",
       "               0.1422773597415666,\n",
       "               0.14216179724717595,\n",
       "               0.14209819480826724,\n",
       "               0.1420227139142685,\n",
       "               0.14193682403856936,\n",
       "               0.14186934286402508,\n",
       "               0.1418026651434272,\n",
       "               0.14174519012985098,\n",
       "               0.14167588021524805,\n",
       "               0.14163163263175252,\n",
       "               0.1415760647990761,\n",
       "               0.14152758080569702,\n",
       "               0.14148548714575404,\n",
       "               0.14139658523078957,\n",
       "               0.14134049592515532,\n",
       "               0.1412918696079497,\n",
       "               0.1412541544053633,\n",
       "               0.1412048862507497,\n",
       "               0.1411464863801517,\n",
       "               0.1411136668722227,\n",
       "               0.14109406663470267,\n",
       "               0.14104309364786013,\n",
       "               0.1409899621362947,\n",
       "               0.14094532052216216,\n",
       "               0.14089379790577983,\n",
       "               0.14084314509024884,\n",
       "               0.14081988885124908,\n",
       "               0.14076198231224032,\n",
       "               0.14071287725196743,\n",
       "               0.14067329311589327,\n",
       "               0.1406303483791496,\n",
       "               0.14056224316823054,\n",
       "               0.14051547619762605,\n",
       "               0.1404605995826689,\n",
       "               0.14042185860137638,\n",
       "               0.14038286198638272,\n",
       "               0.14034686979815425,\n",
       "               0.14031191238563992,\n",
       "               0.14027631371489327,\n",
       "               0.14025295150255293,\n",
       "               0.14021773805975285,\n",
       "               0.14018570402470198,\n",
       "               0.140149593603721,\n",
       "               0.14009985433640054,\n",
       "               0.14006119981030782,\n",
       "               0.14003182976801853,\n",
       "               0.140000544512758,\n",
       "               0.139964987252702,\n",
       "               0.13992762671854178,\n",
       "               0.13989929046056523,\n",
       "               0.13986530571484754,\n",
       "               0.13984549887885372,\n",
       "               0.13980110006972193,\n",
       "               0.13976558407486048,\n",
       "               0.13971955001567735,\n",
       "               0.139691095773846,\n",
       "               0.13965934458884802,\n",
       "               0.13962680430399296,\n",
       "               0.13959602711289984,\n",
       "               0.13956875061736038,\n",
       "               0.13955078360140794,\n",
       "               0.1395212324693135,\n",
       "               0.1394914874166521,\n",
       "               0.1394671521363924,\n",
       "               0.13944711552922892,\n",
       "               0.13942401330689383,\n",
       "               0.13938733043924798,\n",
       "               0.13936795548748163,\n",
       "               0.13932981536095262,\n",
       "               0.13930586524010427,\n",
       "               0.13928601266442842,\n",
       "               0.1392610478298087,\n",
       "               0.13923770436125366,\n",
       "               0.13921257494781888,\n",
       "               0.13919281800977,\n",
       "               0.13916664335970874,\n",
       "               0.13914009683618428,\n",
       "               0.13911115037410074,\n",
       "               0.13908892379008056,\n",
       "               0.1390572420808804,\n",
       "               0.13902560863172353,\n",
       "               0.13901676431024837,\n",
       "               0.13899053672650707,\n",
       "               0.13896310717817936,\n",
       "               0.13893024044937322,\n",
       "               0.1388802017174836,\n",
       "               0.1388501502047339,\n",
       "               0.13881533933858872,\n",
       "               0.13878572017969776,\n",
       "               0.13877051537350063,\n",
       "               0.13874405253161692,\n",
       "               0.13873303992089822,\n",
       "               0.13871209658056027,\n",
       "               0.13868508877185384,\n",
       "               0.13866600846156332,\n",
       "               0.1386463450327977,\n",
       "               0.13862644474617528,\n",
       "               0.13860084722938792,\n",
       "               0.138577370653178,\n",
       "               0.1385567784350801,\n",
       "               0.1385466444498232,\n",
       "               0.13852269145485283,\n",
       "               0.1385029150149172,\n",
       "               0.13848510504995346,\n",
       "               0.13846949528800026,\n",
       "               0.1384584285571925,\n",
       "               0.13844391243212137,\n",
       "               0.13842507419017588,\n",
       "               0.13840730331161372,\n",
       "               0.13838153326715827,\n",
       "               0.1383463701971577,\n",
       "               0.1383259651487725,\n",
       "               0.13830732996701875,\n",
       "               0.13828988481385085,\n",
       "               0.13827020096039208,\n",
       "               0.1382551670503943,\n",
       "               0.1382335031033536,\n",
       "               0.13821220569502998,\n",
       "               0.1381878223719961,\n",
       "               0.13817164396648865,\n",
       "               0.1381550512735098,\n",
       "               0.13813050478242536,\n",
       "               0.13810830184237036,\n",
       "               0.1380880724420219,\n",
       "               0.1380751897656275,\n",
       "               0.13805856440065714,\n",
       "               0.1380311861421408,\n",
       "               0.13800259229750325,\n",
       "               0.13798666800783746,\n",
       "               0.13796589764485132,\n",
       "               0.13794644316558324,\n",
       "               0.13792268394644283,\n",
       "               0.13790889415935423,\n",
       "               0.1378919428130947,\n",
       "               0.1378868830103335,\n",
       "               0.13787497480088065,\n",
       "               0.13786192435724057,\n",
       "               0.13784064026862777,\n",
       "               0.1378169141196284,\n",
       "               0.13779313403588592,\n",
       "               0.13776636564750858,\n",
       "               0.13774503746505687,\n",
       "               0.1377277387857395,\n",
       "               0.13770511218496997,\n",
       "               0.13768097015880446,\n",
       "               0.13765684393063704,\n",
       "               0.1376279556886996,\n",
       "               0.13760999495604478,\n",
       "               0.13759731793082552,\n",
       "               0.1375712740771714,\n",
       "               0.13755815944580205,\n",
       "               0.1375484091235318,\n",
       "               0.1375426207381006,\n",
       "               0.13753019131851948,\n",
       "               0.13750704657677051,\n",
       "               0.13749691127694194,\n",
       "               0.13749264866577232,\n",
       "               0.1374833582061873,\n",
       "               0.13747181164986075,\n",
       "               0.13746590979443882,\n",
       "               0.13745502502730517,\n",
       "               0.1374432703113095,\n",
       "               0.13742796269885318,\n",
       "               0.13741876916890677,\n",
       "               0.1374118154860651,\n",
       "               0.137396334060938,\n",
       "               0.13738451172119714,\n",
       "               0.1373745758659598,\n",
       "               0.13736175204492085,\n",
       "               0.13735508492162815,\n",
       "               0.13734589424327126],\n",
       "              'train-Logloss-std': [0.0005285859497856227,\n",
       "               0.0001724951039144888,\n",
       "               0.00025956986131828104,\n",
       "               0.00022496022328593243,\n",
       "               0.00017580056977787765,\n",
       "               0.000404254940537632,\n",
       "               0.00010263781081431873,\n",
       "               0.0002396043237037392,\n",
       "               0.00018537336205053955,\n",
       "               3.078236545027917e-05,\n",
       "               6.054536488274788e-05,\n",
       "               0.00018359831556467136,\n",
       "               0.00012219361960540188,\n",
       "               0.00017086153499480818,\n",
       "               0.00020493159309817086,\n",
       "               0.00021026966137538161,\n",
       "               0.00018015468961708593,\n",
       "               0.00012592144578431837,\n",
       "               0.00010886510033230686,\n",
       "               0.00013486777613450692,\n",
       "               0.0001517120456242865,\n",
       "               0.00017043539010006407,\n",
       "               0.00014277172835272417,\n",
       "               0.00010586724348178574,\n",
       "               0.00012855019253002226,\n",
       "               0.00013250272593265047,\n",
       "               0.00014365708490960213,\n",
       "               0.00014764553679245032,\n",
       "               0.00015706758987282682,\n",
       "               0.00016343054351362194,\n",
       "               0.0001250883894274553,\n",
       "               0.00012314908357763914,\n",
       "               0.00011373022629257952,\n",
       "               8.720021132652935e-05,\n",
       "               9.512227837272311e-05,\n",
       "               0.00010298573060979864,\n",
       "               0.00010210478354556449,\n",
       "               0.00011165049279775778,\n",
       "               9.919094220391805e-05,\n",
       "               0.00011476459250603898,\n",
       "               0.00010429643747693835,\n",
       "               0.00010747334385549215,\n",
       "               0.00010872582070796802,\n",
       "               0.00014759805544408986,\n",
       "               0.00013835707583457728,\n",
       "               0.00014369213401962274,\n",
       "               0.0001531440216473359,\n",
       "               0.00013794673515662837,\n",
       "               0.00015988839949567516,\n",
       "               0.00017167276068744556,\n",
       "               0.00016570646891429455,\n",
       "               0.0001687709561524094,\n",
       "               0.0001703388236619904,\n",
       "               0.00014801970688812496,\n",
       "               0.00014255119325574781,\n",
       "               0.00013368997163518083,\n",
       "               0.00014699790508239093,\n",
       "               0.0001583273981988634,\n",
       "               0.00014421152377796647,\n",
       "               0.0001414881304748178,\n",
       "               0.0001475405174773958,\n",
       "               0.00014011290550204307,\n",
       "               0.00016302141963962888,\n",
       "               0.00016506028837294404,\n",
       "               0.00015960354969256945,\n",
       "               0.00015837030470253264,\n",
       "               0.00016129532735844248,\n",
       "               0.00017576909584325756,\n",
       "               0.00015749762776877315,\n",
       "               0.00017057821534079948,\n",
       "               0.00017862188807734952,\n",
       "               0.00018632728693156094,\n",
       "               0.00018295208480445294,\n",
       "               0.00017851389874293152,\n",
       "               0.00018423079033022812,\n",
       "               0.00017470720181221545,\n",
       "               0.0001901626697785432,\n",
       "               0.00017640376277095471,\n",
       "               0.00017515635672423017,\n",
       "               0.0001675944834156824,\n",
       "               0.00018357587697156706,\n",
       "               0.00017683939379386005,\n",
       "               0.0001836948618113667,\n",
       "               0.00019138473447048303,\n",
       "               0.00017907168233227555,\n",
       "               0.00019789970337782947,\n",
       "               0.00018435570859666516,\n",
       "               0.00018607527514636314,\n",
       "               0.00018633498226311394,\n",
       "               0.0001758855477547297,\n",
       "               0.0001794105903620079,\n",
       "               0.00019684341721436135,\n",
       "               0.00019852254585758577,\n",
       "               0.0002030731157898976,\n",
       "               0.0001954318704669913,\n",
       "               0.0001984955201909476,\n",
       "               0.00021185374080156226,\n",
       "               0.00020923688937042048,\n",
       "               0.00020728690377439876,\n",
       "               0.0002181194354089805,\n",
       "               0.00023324779019839813,\n",
       "               0.00023041861747999338,\n",
       "               0.00022630355483092837,\n",
       "               0.00021378628281811104,\n",
       "               0.00020316378328769137,\n",
       "               0.0002087339823524504,\n",
       "               0.0002054782555150741,\n",
       "               0.00020174810729570747,\n",
       "               0.00018284297622297448,\n",
       "               0.00018156079277095215,\n",
       "               0.0001852505014592404,\n",
       "               0.00018831531482320075,\n",
       "               0.0001895226257879383,\n",
       "               0.00018574296058357015,\n",
       "               0.00020940967566002462,\n",
       "               0.0002060273670065533,\n",
       "               0.00020890962798296775,\n",
       "               0.00021763963831431806,\n",
       "               0.00021621835775528322,\n",
       "               0.0002152636010268415,\n",
       "               0.00020973500285833568,\n",
       "               0.0002069539105045894,\n",
       "               0.000204181371300185,\n",
       "               0.000212509182965831,\n",
       "               0.0002220682723092787,\n",
       "               0.0002218799302961182,\n",
       "               0.0002344005912472272,\n",
       "               0.00023016101737716052,\n",
       "               0.0002211406087114977,\n",
       "               0.0002254788764410636,\n",
       "               0.0002240109546059361,\n",
       "               0.00023209509987769312,\n",
       "               0.0002344647253573436,\n",
       "               0.0002319471504334184,\n",
       "               0.000238856316368723,\n",
       "               0.0002454106374958728,\n",
       "               0.00024529926536625993,\n",
       "               0.00022927148652566196,\n",
       "               0.00023205889954256708,\n",
       "               0.00019884828728649337,\n",
       "               0.00019092767080716974,\n",
       "               0.0001886237265973517,\n",
       "               0.00019387594709236407,\n",
       "               0.00019519344917595232,\n",
       "               0.00020405856922089215,\n",
       "               0.00019682226767867082,\n",
       "               0.00020467508591132303,\n",
       "               0.00019663791117165762,\n",
       "               0.00019474684810997917,\n",
       "               0.00018947796872728422,\n",
       "               0.00020531358145854428,\n",
       "               0.00019619821151415232,\n",
       "               0.00020301899479277,\n",
       "               0.00021297488576562632,\n",
       "               0.00020380194652188142,\n",
       "               0.0002174354205637758,\n",
       "               0.00020784304602385712,\n",
       "               0.00019932892111356495,\n",
       "               0.00020144450209196764,\n",
       "               0.00020832523764318948,\n",
       "               0.0002176971013620358,\n",
       "               0.00021110341340097887,\n",
       "               0.00021148644755659637,\n",
       "               0.00020928166214866553,\n",
       "               0.0001974080143002658,\n",
       "               0.0002025729334612479,\n",
       "               0.000209322297519319,\n",
       "               0.00021196267951857837,\n",
       "               0.0002159513316942829,\n",
       "               0.00019919760231606662,\n",
       "               0.0001965326132326443,\n",
       "               0.00020401979417468709,\n",
       "               0.0001977303475661469,\n",
       "               0.00019648439620748983,\n",
       "               0.0001786854586575178,\n",
       "               0.00017762549001625177,\n",
       "               0.00017113203497562922,\n",
       "               0.00017897356096562895,\n",
       "               0.00017204671894432164,\n",
       "               0.0001728103842133515,\n",
       "               0.00016827360301073754,\n",
       "               0.0001599470137579489,\n",
       "               0.00014074053987737543,\n",
       "               0.00012075480184389592,\n",
       "               0.0001179179718607274,\n",
       "               0.00011241224121733347,\n",
       "               0.0001098845986741196,\n",
       "               0.0001038341059744456,\n",
       "               9.874429818183713e-05,\n",
       "               9.884504529195414e-05,\n",
       "               9.552246400539639e-05,\n",
       "               9.185434477057187e-05,\n",
       "               0.00010214981000028083,\n",
       "               9.471013252230616e-05,\n",
       "               0.00010220054873369587,\n",
       "               0.00010819572680575705,\n",
       "               0.00011406853932196002,\n",
       "               0.00012411586485373656,\n",
       "               0.00012553951198232088,\n",
       "               0.00013564530033628817]})}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_model = catboost.CatBoostClassifier(n_estimators=200, learning_rate=0.3, eval_metric='AUC',\n",
    "                                             early_stopping_rounds=20, verbose=20, random_seed=42)\n",
    "\n",
    "params = {'depth': np.arange(4, 8, 1), 'learning_rate': np.arange(0.1, 0.5, 0.1), 'min_data_in_leaf': np.arange(3, 5, 1)}\n",
    "\n",
    "boosting_model.grid_search(params, X_train, y_train, cv=3, search_by_train_test_split=True, calc_cv_statistics=True,\n",
    "                           plot=True, refit=True, stratified=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan_mode': 'Min',\n",
       " 'eval_metric': 'AUC',\n",
       " 'iterations': 200,\n",
       " 'sampling_frequency': 'PerTree',\n",
       " 'leaf_estimation_method': 'Newton',\n",
       " 'od_pval': 0,\n",
       " 'random_score_type': 'NormalWithModelSizeDecrease',\n",
       " 'grow_policy': 'SymmetricTree',\n",
       " 'penalties_coefficient': 1,\n",
       " 'boosting_type': 'Plain',\n",
       " 'model_shrink_mode': 'Constant',\n",
       " 'feature_border_type': 'GreedyLogSum',\n",
       " 'bayesian_matrix_reg': 0.10000000149011612,\n",
       " 'eval_fraction': 0,\n",
       " 'force_unit_auto_pair_weights': False,\n",
       " 'l2_leaf_reg': 3,\n",
       " 'random_strength': 1,\n",
       " 'od_type': 'Iter',\n",
       " 'rsm': 1,\n",
       " 'boost_from_average': False,\n",
       " 'model_size_reg': 0.5,\n",
       " 'pool_metainfo_options': {'tags': {}},\n",
       " 'subsample': 0.800000011920929,\n",
       " 'use_best_model': True,\n",
       " 'od_wait': 20,\n",
       " 'class_names': [0, 1],\n",
       " 'random_seed': 42,\n",
       " 'depth': 7,\n",
       " 'posterior_sampling': False,\n",
       " 'border_count': 254,\n",
       " 'classes_count': 0,\n",
       " 'auto_class_weights': 'None',\n",
       " 'sparse_features_conflict_fraction': 0,\n",
       " 'leaf_estimation_backtracking': 'AnyImprovement',\n",
       " 'best_model_min_trees': 1,\n",
       " 'model_shrink_rate': 0,\n",
       " 'min_data_in_leaf': 3,\n",
       " 'loss_function': 'Logloss',\n",
       " 'learning_rate': 0.30000001192092896,\n",
       " 'score_function': 'Cosine',\n",
       " 'task_type': 'CPU',\n",
       " 'leaf_estimation_iterations': 10,\n",
       " 'bootstrap_type': 'MVS',\n",
       " 'max_leaves': 128}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = boosting_model.get_all_params()\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42aee1a4c3ef4ec5bfdd245316647369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5403323\tbest: 0.5403323 (0)\ttotal: 449ms\tremaining: 3m 44s\n",
      "50:\ttest: 0.7406433\tbest: 0.7406433 (50)\ttotal: 22.4s\tremaining: 3m 17s\n",
      "100:\ttest: 0.7458627\tbest: 0.7458772 (99)\ttotal: 43.7s\tremaining: 2m 52s\n",
      "150:\ttest: 0.7473500\tbest: 0.7473639 (149)\ttotal: 1m 4s\tremaining: 2m 29s\n",
      "200:\ttest: 0.7481973\tbest: 0.7482015 (199)\ttotal: 1m 25s\tremaining: 2m 7s\n",
      "250:\ttest: 0.7485814\tbest: 0.7485814 (250)\ttotal: 1m 47s\tremaining: 1m 46s\n",
      "300:\ttest: 0.7489793\tbest: 0.7489793 (300)\ttotal: 2m 12s\tremaining: 1m 27s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7491580894\n",
      "bestIteration = 308\n",
      "\n",
      "Shrink model to first 309 iterations.\n",
      "train:  0.7693149192237118\n",
      "test:  0.7491580893562763\n"
     ]
    }
   ],
   "source": [
    "boosting_model_2 = catboost.CatBoostClassifier(n_estimators=500, learning_rate=0.3, eval_metric='AUC',\n",
    "                                             early_stopping_rounds=20, verbose=20, random_seed=42,\n",
    "                                            depth=7, min_data_in_leaf=3)\n",
    "\n",
    "boosting_model_2.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=50, plot=True)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, boosting_model_2.predict_proba(X_train)[:, 1]))\n",
    "print('test: ', roc_auc_score(y_test, boosting_model_2.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Id</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre_util_3</td>\n",
       "      <td>8.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is_zero_loans530</td>\n",
       "      <td>7.547831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pre_loans_credit_limit_2</td>\n",
       "      <td>6.612090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pre_util_6</td>\n",
       "      <td>5.342999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enc_paym_22_3</td>\n",
       "      <td>4.650208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>pre_loans3060_6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>pre_loans3060_9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>pre_loans6090_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>pre_over2limit_18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>enc_loans_account_cur_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Feature Id  Importances\n",
       "0                  pre_util_3     8.154100\n",
       "1            is_zero_loans530     7.547831\n",
       "2    pre_loans_credit_limit_2     6.612090\n",
       "3                  pre_util_6     5.342999\n",
       "4               enc_paym_22_3     4.650208\n",
       "..                        ...          ...\n",
       "190           pre_loans3060_6     0.000000\n",
       "191           pre_loans3060_9     0.000000\n",
       "192           pre_loans6090_3     0.000000\n",
       "193         pre_over2limit_18     0.000000\n",
       "194   enc_loans_account_cur_3     0.000000\n",
       "\n",
       "[195 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_model_2.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp = pd.DataFrame(boosting_model_2.get_feature_importance(prettified=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_imp.loc[df_imp['Importances']>0]['Feature Id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#убираю лишние фичи\n",
    "df_imp = pd.DataFrame(boosting_model_2.get_feature_importance(prettified=True))\n",
    "new_cols = df_imp.loc[df_imp['Importances']>0]['Feature Id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001418, 175)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[new_cols], df['target'], stratify=df['target'], train_size=0.7)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5719056103704d2cb394d90dbf72c277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5947286\tbest: 0.5947286 (0)\ttotal: 345ms\tremaining: 2m 52s\n",
      "50:\ttest: 0.7426050\tbest: 0.7426050 (50)\ttotal: 21s\tremaining: 3m 4s\n",
      "100:\ttest: 0.7477287\tbest: 0.7477287 (100)\ttotal: 40.5s\tremaining: 2m 39s\n",
      "150:\ttest: 0.7498418\tbest: 0.7498418 (150)\ttotal: 1m\tremaining: 2m 18s\n",
      "200:\ttest: 0.7507420\tbest: 0.7507819 (198)\ttotal: 1m 19s\tremaining: 1m 58s\n",
      "250:\ttest: 0.7513661\tbest: 0.7513809 (249)\ttotal: 1m 38s\tremaining: 1m 37s\n",
      "300:\ttest: 0.7518082\tbest: 0.7518316 (299)\ttotal: 1m 57s\tremaining: 1m 17s\n",
      "350:\ttest: 0.7521967\tbest: 0.7522053 (349)\ttotal: 2m 17s\tremaining: 58.4s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7522874505\n",
      "bestIteration = 357\n",
      "\n",
      "Shrink model to first 358 iterations.\n",
      "train:  0.7709935366016526\n",
      "test:  0.7522874505188494\n"
     ]
    }
   ],
   "source": [
    "boosting_model_2 = catboost.CatBoostClassifier(n_estimators=500, learning_rate=0.3, eval_metric='AUC',\n",
    "                                             early_stopping_rounds=20, verbose=20, random_seed=42,\n",
    "                                            depth=7, min_data_in_leaf=3)\n",
    "\n",
    "boosting_model_2.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=50, plot=True)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, boosting_model_2.predict_proba(X_train)[:, 1]))\n",
    "print('test: ', roc_auc_score(y_test, boosting_model_2.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e9837c97a14db19a1df037bcf0601d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_loss 0.1836996316200211\n",
      "train:  0.7304729559196442\n",
      "curr_loss 0.1411219557126363\n",
      "train:  0.7460666199059471\n",
      "curr_loss 0.13972157956588835\n",
      "train:  0.7513843278067072\n",
      "curr_loss 0.13877129334779012\n",
      "train:  0.7570041933580814\n",
      "curr_loss 0.13819176458886692\n",
      "train:  0.7608164091577205\n",
      "curr_loss 0.1378076552989937\n",
      "train:  0.7627375672049768\n",
      "curr_loss 0.1374118372797966\n",
      "train:  0.7656595506650496\n",
      "curr_loss 0.13705579155967348\n",
      "train:  0.7690076909951372\n",
      "curr_loss 0.13685141099350792\n",
      "train:  0.7686768545830803\n",
      "curr_loss 0.1365907577176889\n",
      "train:  0.7713685655896559\n",
      "curr_loss 0.13625181246371496\n",
      "train:  0.774605643066052\n",
      "curr_loss 0.13589044099762326\n",
      "train:  0.7757127840407803\n",
      "curr_loss 0.13552879478250232\n",
      "train:  0.77796611830665\n",
      "curr_loss 0.13521782124326343\n",
      "train:  0.7794071986940345\n",
      "curr_loss 0.13482717013075238\n",
      "train:  0.7825171128905027\n",
      "train:  0.7825188101411484\n",
      "test:  0.7527168482160213\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], \n",
    "                                                    train_size=0.7, random_state=1)\n",
    "\n",
    "\n",
    "X_train_t =  torch.FloatTensor(X_train.values)\n",
    "y_train_t =  torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_t =  torch.FloatTensor(X_test.values)\n",
    "y_test_t =  torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(X_train_t, y_train_t)), batch_size=10000, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(X_test_t, y_test_t)), batch_size=10000, shuffle=False)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.fc1 = nn.Linear(361, 216, bias=True)\n",
    "        self.fc2 = nn.Linear(216, 130, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(130, 78, bias=True)\n",
    "        self.fc4 = nn.Linear(78, 39, bias=True)\n",
    "        self.fc5 = nn.Linear(39, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "nn_model = Net()\n",
    "\n",
    "def train_stochastic(model, loader, criterion, optimazer, num_epoch, X_train_t, y_train_t):\n",
    "    \n",
    "    losses = []\n",
    "    roc_auc_metrics = []\n",
    "    for t in tqdm(range(num_epoch)):\n",
    "        epoch_loss = []\n",
    "        metrics = []\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                     \n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        print(\"curr_loss\", np.mean(epoch_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn_prediction_train = model(X_train_t).tolist()\n",
    "            roc_auc_metrics.append(roc_auc_score(y_train_t, nn_prediction_train))\n",
    "            print('train: ', roc_auc_score(y_train_t, nn_prediction_train))\n",
    "\n",
    "    return model, losses, roc_auc_metrics\n",
    "    \n",
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "\n",
    "model, losses, metrics = train_stochastic(nn_model, train_loader, loss, optimizer, 15, X_train_t, y_train_t)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_prediction_train = model(X_train_t)\n",
    "    nn_prediction_train = nn_prediction_train.tolist() \n",
    "    \n",
    "    nn_prediction_test = model(X_test_t)\n",
    "    nn_prediction_test = nn_prediction_test.tolist()\n",
    "\n",
    "    print('train: ', roc_auc_score(y_train, nn_prediction_train))\n",
    "    print('test: ', roc_auc_score(y_test, nn_prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trainig(losses, metrics):\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.plot(losses, label=\"train_loss\")\n",
    "    #plt.plot(valid_losses, label=\"valid_loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.plot(metrics, label=\"train_roc-auc\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_trainig(losses, metrics)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ad15a495b554f6988a6bf8034b47c87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23e710f9ded3410fb7739bf2f708765b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "241ce64f1fc34f74a8f34aa59ad826f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b69b7d615c44ee184e7411f134801a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb33ae64d7304648907fca36c15f3b21",
       "IPY_MODEL_4a2acf6316dc48f9bce1d99d87fc9dea",
       "IPY_MODEL_741177c6570644628034c5aa05eabc77"
      ],
      "layout": "IPY_MODEL_1ad15a495b554f6988a6bf8034b47c87"
     }
    },
    "4a2acf6316dc48f9bce1d99d87fc9dea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ba69de817e745e78d82a73d59292a7f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_23e710f9ded3410fb7739bf2f708765b",
      "value": 2
     }
    },
    "4b611a125375404e860f4d20d62a23ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ba69de817e745e78d82a73d59292a7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "519903cf72e44d38a31dab1b35e63db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55b69718f08f497e80f3e04e6a48df72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fa48124bf95428290882f8b2e6613d3",
      "placeholder": "​",
      "style": "IPY_MODEL_241ce64f1fc34f74a8f34aa59ad826f0",
      "value": "Transforming transactions data: 100%"
     }
    },
    "65e640c2bdc142e38063637901fecdd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7078f2a7e69140baade16d892b921a52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741177c6570644628034c5aa05eabc77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5120879dec04920a9520f48990108f5",
      "placeholder": "​",
      "style": "IPY_MODEL_f1951d6bdf53462cb1237cf4973dd1d0",
      "value": " 2/2 [00:03&lt;00:00,  1.50s/it]"
     }
    },
    "7e321bc6d2ab43e09f084cf5c56871e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fa48124bf95428290882f8b2e6613d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ee0775d5224e63bebed82b6da365ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65e640c2bdc142e38063637901fecdd1",
      "placeholder": "​",
      "style": "IPY_MODEL_519903cf72e44d38a31dab1b35e63db2",
      "value": " 1/1 [00:15&lt;00:00, 15.05s/it]"
     }
    },
    "a6e783c7b06c4729ac354630a6d7de37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55b69718f08f497e80f3e04e6a48df72",
       "IPY_MODEL_fd892497bcc1499995d753e3a5a57bfc",
       "IPY_MODEL_a0ee0775d5224e63bebed82b6da365ef"
      ],
      "layout": "IPY_MODEL_4b611a125375404e860f4d20d62a23ca"
     }
    },
    "c5120879dec04920a9520f48990108f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ceb12588680f454490dd102c0f6763d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1951d6bdf53462cb1237cf4973dd1d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8f7460fde7f4a37a3aedba33c1d1def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb33ae64d7304648907fca36c15f3b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e321bc6d2ab43e09f084cf5c56871e8",
      "placeholder": "​",
      "style": "IPY_MODEL_7078f2a7e69140baade16d892b921a52",
      "value": "Reading dataset with pandas: 100%"
     }
    },
    "fd892497bcc1499995d753e3a5a57bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ceb12588680f454490dd102c0f6763d1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8f7460fde7f4a37a3aedba33c1d1def",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
