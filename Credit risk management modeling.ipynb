{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import catboost\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torcheval.metrics import BinaryAUROC\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from torch.optim import Adam, Adadelta, RMSprop\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, LinearLR, MultiStepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and viewing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:\n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                              if filename.startswith('train')])\n",
    "    print(dataset_paths)\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "    if verbose:\n",
    "        print('Reading chunks:\\n')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        print('chunk_path', chunk_path)\n",
    "        chunk = pd.read_parquet(chunk_path,columns=columns)\n",
    "        res.append(chunk)\n",
    "\n",
    "    return pd.concat(res).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_dataset(path):\n",
    "    for i, elem in enumerate(['df_'+str(a) for a in range(0, 12)]):\n",
    "        elem = read_parquet_dataset_from_local(path, i, 1)\n",
    "        if i==0:\n",
    "            df = elem.copy()\n",
    "        else:\n",
    "            df = pd.concat([df, elem], ignore_index=True, axis=0)\n",
    "        del elem\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cbc181ff554cbbbde656901d779515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_0.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7661f00338e04dba8c5efb89b1238b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_1.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d4b05de73046b5a95af26501afb84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_10.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c927345a744d359bf6fe73c305979f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_11.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ee6d89b8a94d0ab08139ce159ea970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_2.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f3e29a14494e0089226c3dd115ca5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_3.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfad172a285b4e58b7d257310305b030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_4.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d07273b8fe74dcca027aeccea1644b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_5.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25a78bfa74e4850a131bdcd462e49bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_6.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734e306706294cf08f949095208e4b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_7.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c12e3d918e94b1c84f6656f00a0890d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_8.pq\n",
      "['train_data/train_data_0.pq', 'train_data/train_data_1.pq', 'train_data/train_data_10.pq', 'train_data/train_data_11.pq', 'train_data/train_data_2.pq', 'train_data/train_data_3.pq', 'train_data/train_data_4.pq', 'train_data/train_data_5.pq', 'train_data/train_data_6.pq', 'train_data/train_data_7.pq', 'train_data/train_data_8.pq', 'train_data/train_data_9.pq']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17c047ae06854cffb2a4c4132203db04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading dataset with pandas:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_path train_data/train_data_9.pq\n"
     ]
    }
   ],
   "source": [
    "path = 'train_data/'\n",
    "df = create_dataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rn</th>\n",
       "      <th>pre_since_opened</th>\n",
       "      <th>pre_since_confirmed</th>\n",
       "      <th>pre_pterm</th>\n",
       "      <th>pre_fterm</th>\n",
       "      <th>pre_till_pclose</th>\n",
       "      <th>pre_till_fclose</th>\n",
       "      <th>pre_loans_credit_limit</th>\n",
       "      <th>pre_loans_next_pay_summ</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_paym_21</th>\n",
       "      <th>enc_paym_22</th>\n",
       "      <th>enc_paym_23</th>\n",
       "      <th>enc_paym_24</th>\n",
       "      <th>enc_loans_account_holder_type</th>\n",
       "      <th>enc_loans_credit_status</th>\n",
       "      <th>enc_loans_credit_type</th>\n",
       "      <th>enc_loans_account_cur</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  rn  pre_since_opened  pre_since_confirmed  pre_pterm  pre_fterm  \\\n",
       "0   0   1                18                    9          2          3   \n",
       "1   0   2                18                    9         14         14   \n",
       "2   0   3                18                    9          4          8   \n",
       "3   0   4                 4                    1          9         12   \n",
       "4   0   5                 5                   12         15          2   \n",
       "\n",
       "   pre_till_pclose  pre_till_fclose  pre_loans_credit_limit  \\\n",
       "0               16               10                      11   \n",
       "1               12               12                       0   \n",
       "2                1               11                      11   \n",
       "3               16                7                      12   \n",
       "4               11               12                      10   \n",
       "\n",
       "   pre_loans_next_pay_summ  ...  enc_paym_21  enc_paym_22  enc_paym_23  \\\n",
       "0                        3  ...            3            3            3   \n",
       "1                        3  ...            0            0            0   \n",
       "2                        0  ...            0            0            0   \n",
       "3                        2  ...            3            3            3   \n",
       "4                        2  ...            3            3            3   \n",
       "\n",
       "   enc_paym_24  enc_loans_account_holder_type  enc_loans_credit_status  \\\n",
       "0            4                              1                        3   \n",
       "1            4                              1                        3   \n",
       "2            4                              1                        2   \n",
       "3            4                              1                        3   \n",
       "4            4                              1                        3   \n",
       "\n",
       "   enc_loans_credit_type  enc_loans_account_cur  pclose_flag  fclose_flag  \n",
       "0                      4                      1            0            0  \n",
       "1                      4                      1            0            0  \n",
       "2                      3                      1            1            1  \n",
       "3                      1                      1            0            0  \n",
       "4                      4                      1            0            0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26162717, 61)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999995</th>\n",
       "      <td>2999995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999996</th>\n",
       "      <td>2999996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999997</th>\n",
       "      <td>2999997</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999998</th>\n",
       "      <td>2999998</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999999</th>\n",
       "      <td>2999999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  flag\n",
       "0              0     0\n",
       "1              1     0\n",
       "2              2     0\n",
       "3              3     0\n",
       "4              4     0\n",
       "...          ...   ...\n",
       "2999995  2999995     0\n",
       "2999996  2999996     0\n",
       "2999997  2999997     0\n",
       "2999998  2999998     0\n",
       "2999999  2999999     0\n",
       "\n",
       "[3000000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = pd.read_csv('train_target.csv')\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['pre_since_opened', 'pre_since_confirmed', 'pre_fterm', 'pre_till_pclose', \n",
    "                   'pre_till_fclose', 'pre_loans_outstanding', 'pre_loans_total_overdue',\n",
    "                   'pre_loans_max_overdue_sum', 'pre_loans90', #'is_zero_loans90', \n",
    "                   'is_zero_util', 'is_zero_over2limit', 'is_zero_maxover2limit', \n",
    "                   'enc_paym_0', 'enc_paym_1', 'enc_paym_2', 'enc_paym_3', 'enc_paym_4', 'enc_paym_5', \n",
    "                   'enc_paym_6', 'enc_paym_7', 'enc_paym_8', 'enc_paym_9', 'enc_paym_10', 'enc_paym_11', \n",
    "                   'enc_paym_12', 'enc_paym_13', 'enc_paym_14', 'enc_paym_15', 'enc_paym_16', 'enc_paym_17', \n",
    "                   'enc_paym_18', 'enc_paym_19', 'enc_paym_20','enc_paym_21']\n",
    "df.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                               0\n",
       "rn                               0\n",
       "pre_pterm                        0\n",
       "pre_loans_credit_limit           0\n",
       "pre_loans_next_pay_summ          0\n",
       "pre_loans_credit_cost_rate       0\n",
       "pre_loans5                       0\n",
       "pre_loans530                     0\n",
       "pre_loans3060                    0\n",
       "pre_loans6090                    0\n",
       "is_zero_loans5                   0\n",
       "is_zero_loans530                 0\n",
       "is_zero_loans3060                0\n",
       "is_zero_loans6090                0\n",
       "is_zero_loans90                  0\n",
       "pre_util                         0\n",
       "pre_over2limit                   0\n",
       "pre_maxover2limit                0\n",
       "enc_paym_22                      0\n",
       "enc_paym_23                      0\n",
       "enc_paym_24                      0\n",
       "enc_loans_account_holder_type    0\n",
       "enc_loans_credit_status          0\n",
       "enc_loans_credit_type            0\n",
       "enc_loans_account_cur            0\n",
       "pclose_flag                      0\n",
       "fclose_flag                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['pre_pterm', 'pre_loans_credit_limit',  'pre_loans_next_pay_summ', 'pre_loans_credit_cost_rate',\n",
    "              'pre_loans5', 'pre_loans530', 'pre_loans3060', 'pre_loans6090', 'pre_util', 'pre_over2limit', 'is_zero_loans90', \n",
    "               'pre_maxover2limit', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24', 'enc_loans_account_holder_type',\n",
    "               'enc_loans_credit_status', 'enc_loans_credit_type', 'enc_loans_account_cur']\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore', dtype='int8')\n",
    "df[ohe.get_feature_names_out()] = ohe.fit_transform(df[cat_columns])\n",
    "df.drop(columns=cat_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rn</th>\n",
       "      <th>is_zero_loans5</th>\n",
       "      <th>is_zero_loans530</th>\n",
       "      <th>is_zero_loans3060</th>\n",
       "      <th>is_zero_loans6090</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "      <th>pre_pterm_1</th>\n",
       "      <th>pre_pterm_2</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_loans_credit_type_1</th>\n",
       "      <th>enc_loans_credit_type_2</th>\n",
       "      <th>enc_loans_credit_type_3</th>\n",
       "      <th>enc_loans_credit_type_4</th>\n",
       "      <th>enc_loans_credit_type_5</th>\n",
       "      <th>enc_loans_credit_type_6</th>\n",
       "      <th>enc_loans_credit_type_7</th>\n",
       "      <th>enc_loans_account_cur_1</th>\n",
       "      <th>enc_loans_account_cur_2</th>\n",
       "      <th>enc_loans_account_cur_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  rn  is_zero_loans5  is_zero_loans530  is_zero_loans3060  \\\n",
       "0   0   1               1                 1                  1   \n",
       "1   0   2               1                 1                  1   \n",
       "2   0   3               1                 1                  1   \n",
       "3   0   4               0                 1                  1   \n",
       "4   0   5               1                 1                  1   \n",
       "\n",
       "   is_zero_loans6090  pclose_flag  fclose_flag  pre_pterm_1  pre_pterm_2  ...  \\\n",
       "0                  1            0            0            0            1  ...   \n",
       "1                  1            0            0            0            0  ...   \n",
       "2                  1            1            1            0            0  ...   \n",
       "3                  1            0            0            0            0  ...   \n",
       "4                  1            0            0            0            0  ...   \n",
       "\n",
       "   enc_loans_credit_type_1  enc_loans_credit_type_2  enc_loans_credit_type_3  \\\n",
       "0                        0                        0                        0   \n",
       "1                        0                        0                        0   \n",
       "2                        0                        0                        1   \n",
       "3                        1                        0                        0   \n",
       "4                        0                        0                        0   \n",
       "\n",
       "   enc_loans_credit_type_4  enc_loans_credit_type_5  enc_loans_credit_type_6  \\\n",
       "0                        1                        0                        0   \n",
       "1                        1                        0                        0   \n",
       "2                        0                        0                        0   \n",
       "3                        0                        0                        0   \n",
       "4                        1                        0                        0   \n",
       "\n",
       "   enc_loans_credit_type_7  enc_loans_account_cur_1  enc_loans_account_cur_2  \\\n",
       "0                        0                        1                        0   \n",
       "1                        0                        1                        0   \n",
       "2                        0                        1                        0   \n",
       "3                        0                        1                        0   \n",
       "4                        0                        1                        0   \n",
       "\n",
       "   enc_loans_account_cur_3  \n",
       "0                        0  \n",
       "1                        0  \n",
       "2                        0  \n",
       "3                        0  \n",
       "4                        0  \n",
       "\n",
       "[5 rows x 196 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26162717, 196)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.iloc[:, :2].copy().groupby(['id']).max()\n",
    "df_1.columns = ['max_rn']\n",
    "df = df.groupby(['id']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rn</th>\n",
       "      <th>is_zero_loans5</th>\n",
       "      <th>is_zero_loans530</th>\n",
       "      <th>is_zero_loans3060</th>\n",
       "      <th>is_zero_loans6090</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "      <th>pre_pterm_1</th>\n",
       "      <th>pre_pterm_2</th>\n",
       "      <th>pre_pterm_3</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_loans_credit_type_1</th>\n",
       "      <th>enc_loans_credit_type_2</th>\n",
       "      <th>enc_loans_credit_type_3</th>\n",
       "      <th>enc_loans_credit_type_4</th>\n",
       "      <th>enc_loans_credit_type_5</th>\n",
       "      <th>enc_loans_credit_type_6</th>\n",
       "      <th>enc_loans_credit_type_7</th>\n",
       "      <th>enc_loans_account_cur_1</th>\n",
       "      <th>enc_loans_account_cur_2</th>\n",
       "      <th>enc_loans_account_cur_3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     rn  is_zero_loans5  is_zero_loans530  is_zero_loans3060  \\\n",
       "id                                                             \n",
       "0    55               9                10                 10   \n",
       "1   105              12                10                 12   \n",
       "2     6               3                 2                  2   \n",
       "3   120              15                15                 15   \n",
       "4     1               1                 1                  1   \n",
       "\n",
       "    is_zero_loans6090  pclose_flag  fclose_flag  pre_pterm_1  pre_pterm_2  \\\n",
       "id                                                                          \n",
       "0                  10            1            2            1            3   \n",
       "1                  12            1            2            3            1   \n",
       "2                   2            2            2            0            0   \n",
       "3                  15            5            6            1            1   \n",
       "4                   1            1            1            0            0   \n",
       "\n",
       "    pre_pterm_3  ...  enc_loans_credit_type_1  enc_loans_credit_type_2  \\\n",
       "id               ...                                                     \n",
       "0             0  ...                        1                        0   \n",
       "1             0  ...                        3                        0   \n",
       "2             0  ...                        0                        0   \n",
       "3             0  ...                        1                        0   \n",
       "4             0  ...                        0                        0   \n",
       "\n",
       "    enc_loans_credit_type_3  enc_loans_credit_type_4  enc_loans_credit_type_5  \\\n",
       "id                                                                              \n",
       "0                         2                        7                        0   \n",
       "1                         3                        8                        0   \n",
       "2                         2                        1                        0   \n",
       "3                         4                        9                        1   \n",
       "4                         1                        0                        0   \n",
       "\n",
       "    enc_loans_credit_type_6  enc_loans_credit_type_7  enc_loans_account_cur_1  \\\n",
       "id                                                                              \n",
       "0                         0                        0                       10   \n",
       "1                         0                        0                       14   \n",
       "2                         0                        0                        3   \n",
       "3                         0                        0                       15   \n",
       "4                         0                        0                        1   \n",
       "\n",
       "    enc_loans_account_cur_2  enc_loans_account_cur_3  \n",
       "id                                                    \n",
       "0                         0                        0  \n",
       "1                         0                        0  \n",
       "2                         0                        0  \n",
       "3                         0                        0  \n",
       "4                         0                        0  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_zero_loans5</th>\n",
       "      <th>is_zero_loans530</th>\n",
       "      <th>is_zero_loans3060</th>\n",
       "      <th>is_zero_loans6090</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "      <th>pre_pterm_1</th>\n",
       "      <th>pre_pterm_2</th>\n",
       "      <th>pre_pterm_3</th>\n",
       "      <th>pre_pterm_4</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_loans_credit_type_2</th>\n",
       "      <th>enc_loans_credit_type_3</th>\n",
       "      <th>enc_loans_credit_type_4</th>\n",
       "      <th>enc_loans_credit_type_5</th>\n",
       "      <th>enc_loans_credit_type_6</th>\n",
       "      <th>enc_loans_credit_type_7</th>\n",
       "      <th>enc_loans_account_cur_1</th>\n",
       "      <th>enc_loans_account_cur_2</th>\n",
       "      <th>enc_loans_account_cur_3</th>\n",
       "      <th>max_rn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999995</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999996</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999997</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999998</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999999</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000000 rows × 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         is_zero_loans5  is_zero_loans530  is_zero_loans3060  \\\n",
       "id                                                             \n",
       "0                     9                10                 10   \n",
       "1                    12                10                 12   \n",
       "2                     3                 2                  2   \n",
       "3                    15                15                 15   \n",
       "4                     1                 1                  1   \n",
       "...                 ...               ...                ...   \n",
       "2999995               9                 5                 11   \n",
       "2999996              12                13                 12   \n",
       "2999997               9                 7                 10   \n",
       "2999998               5                 5                  4   \n",
       "2999999              11                 8                 11   \n",
       "\n",
       "         is_zero_loans6090  pclose_flag  fclose_flag  pre_pterm_1  \\\n",
       "id                                                                  \n",
       "0                       10            1            2            1   \n",
       "1                       12            1            2            3   \n",
       "2                        2            2            2            0   \n",
       "3                       15            5            6            1   \n",
       "4                        1            1            1            0   \n",
       "...                    ...          ...          ...          ...   \n",
       "2999995                 11            2            5            0   \n",
       "2999996                 13            5            4            0   \n",
       "2999997                 10            1            1            0   \n",
       "2999998                  4            2            1            0   \n",
       "2999999                 12            2            1            0   \n",
       "\n",
       "         pre_pterm_2  pre_pterm_3  pre_pterm_4  ...  enc_loans_credit_type_2  \\\n",
       "id                                              ...                            \n",
       "0                  3            0            1  ...                        0   \n",
       "1                  1            0            1  ...                        0   \n",
       "2                  0            0            2  ...                        0   \n",
       "3                  1            0            5  ...                        0   \n",
       "4                  0            0            1  ...                        0   \n",
       "...              ...          ...          ...  ...                      ...   \n",
       "2999995            2            0            2  ...                        0   \n",
       "2999996            0            0            5  ...                        0   \n",
       "2999997            1            0            1  ...                        0   \n",
       "2999998            0            0            2  ...                        0   \n",
       "2999999            3            0            2  ...                        0   \n",
       "\n",
       "         enc_loans_credit_type_3  enc_loans_credit_type_4  \\\n",
       "id                                                          \n",
       "0                              2                        7   \n",
       "1                              3                        8   \n",
       "2                              2                        1   \n",
       "3                              4                        9   \n",
       "4                              1                        0   \n",
       "...                          ...                      ...   \n",
       "2999995                        4                        6   \n",
       "2999996                        4                        8   \n",
       "2999997                        3                        7   \n",
       "2999998                        2                        2   \n",
       "2999999                        3                        9   \n",
       "\n",
       "         enc_loans_credit_type_5  enc_loans_credit_type_6  \\\n",
       "id                                                          \n",
       "0                              0                        0   \n",
       "1                              0                        0   \n",
       "2                              0                        0   \n",
       "3                              1                        0   \n",
       "4                              0                        0   \n",
       "...                          ...                      ...   \n",
       "2999995                        0                        0   \n",
       "2999996                        0                        0   \n",
       "2999997                        0                        0   \n",
       "2999998                        0                        0   \n",
       "2999999                        0                        0   \n",
       "\n",
       "         enc_loans_credit_type_7  enc_loans_account_cur_1  \\\n",
       "id                                                          \n",
       "0                              0                       10   \n",
       "1                              0                       14   \n",
       "2                              0                        3   \n",
       "3                              0                       15   \n",
       "4                              0                        1   \n",
       "...                          ...                      ...   \n",
       "2999995                        1                       11   \n",
       "2999996                        1                       13   \n",
       "2999997                        0                       10   \n",
       "2999998                        0                        5   \n",
       "2999999                        0                       12   \n",
       "\n",
       "         enc_loans_account_cur_2  enc_loans_account_cur_3  max_rn  \n",
       "id                                                                 \n",
       "0                              0                        0      10  \n",
       "1                              0                        0      14  \n",
       "2                              0                        0       3  \n",
       "3                              0                        0      15  \n",
       "4                              0                        0       1  \n",
       "...                          ...                      ...     ...  \n",
       "2999995                        0                        0      11  \n",
       "2999996                        0                        0      13  \n",
       "2999997                        0                        0      10  \n",
       "2999998                        0                        0       5  \n",
       "2999999                        0                        0      12  \n",
       "\n",
       "[3000000 rows x 195 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.merge(df_1, how='left', on='id').drop(columns='rn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'target'] = targets.loc[:, 'flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rn</th>\n",
       "      <th>is_zero_loans5</th>\n",
       "      <th>is_zero_loans530</th>\n",
       "      <th>is_zero_loans3060</th>\n",
       "      <th>is_zero_loans6090</th>\n",
       "      <th>pclose_flag</th>\n",
       "      <th>fclose_flag</th>\n",
       "      <th>pre_pterm_1</th>\n",
       "      <th>pre_pterm_2</th>\n",
       "      <th>pre_pterm_3</th>\n",
       "      <th>...</th>\n",
       "      <th>enc_loans_credit_type_2</th>\n",
       "      <th>enc_loans_credit_type_3</th>\n",
       "      <th>enc_loans_credit_type_4</th>\n",
       "      <th>enc_loans_credit_type_5</th>\n",
       "      <th>enc_loans_credit_type_6</th>\n",
       "      <th>enc_loans_credit_type_7</th>\n",
       "      <th>enc_loans_account_cur_1</th>\n",
       "      <th>enc_loans_account_cur_2</th>\n",
       "      <th>enc_loans_account_cur_3</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2165</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2275</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3974</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999891</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999902</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999923</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999974</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140830 rows × 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         rn  is_zero_loans5  is_zero_loans530  is_zero_loans3060  \\\n",
       "id                                                                 \n",
       "488       1               1                 1                  1   \n",
       "1327      1               1                 1                  1   \n",
       "2165      1               1                 1                  1   \n",
       "2275      1               1                 1                  1   \n",
       "3974      1               1                 1                  1   \n",
       "...      ..             ...               ...                ...   \n",
       "2999887   1               1                 1                  1   \n",
       "2999891   1               1                 1                  1   \n",
       "2999902   1               1                 1                  1   \n",
       "2999923   1               1                 1                  1   \n",
       "2999974   1               1                 1                  1   \n",
       "\n",
       "         is_zero_loans6090  pclose_flag  fclose_flag  pre_pterm_1  \\\n",
       "id                                                                  \n",
       "488                      1            0            0            0   \n",
       "1327                     1            0            0            0   \n",
       "2165                     1            1            1            0   \n",
       "2275                     1            0            0            0   \n",
       "3974                     1            1            1            0   \n",
       "...                    ...          ...          ...          ...   \n",
       "2999887                  1            0            0            0   \n",
       "2999891                  1            0            0            0   \n",
       "2999902                  1            1            1            0   \n",
       "2999923                  1            0            0            0   \n",
       "2999974                  1            1            1            0   \n",
       "\n",
       "         pre_pterm_2  pre_pterm_3  ...  enc_loans_credit_type_2  \\\n",
       "id                                 ...                            \n",
       "488                0            0  ...                        0   \n",
       "1327               0            0  ...                        0   \n",
       "2165               0            0  ...                        0   \n",
       "2275               0            0  ...                        0   \n",
       "3974               0            0  ...                        0   \n",
       "...              ...          ...  ...                      ...   \n",
       "2999887            0            0  ...                        0   \n",
       "2999891            0            0  ...                        0   \n",
       "2999902            0            0  ...                        0   \n",
       "2999923            0            0  ...                        0   \n",
       "2999974            0            0  ...                        0   \n",
       "\n",
       "         enc_loans_credit_type_3  enc_loans_credit_type_4  \\\n",
       "id                                                          \n",
       "488                            0                        1   \n",
       "1327                           0                        1   \n",
       "2165                           1                        0   \n",
       "2275                           1                        0   \n",
       "3974                           1                        0   \n",
       "...                          ...                      ...   \n",
       "2999887                        1                        0   \n",
       "2999891                        1                        0   \n",
       "2999902                        0                        1   \n",
       "2999923                        0                        1   \n",
       "2999974                        1                        0   \n",
       "\n",
       "         enc_loans_credit_type_5  enc_loans_credit_type_6  \\\n",
       "id                                                          \n",
       "488                            0                        0   \n",
       "1327                           0                        0   \n",
       "2165                           0                        0   \n",
       "2275                           0                        0   \n",
       "3974                           0                        0   \n",
       "...                          ...                      ...   \n",
       "2999887                        0                        0   \n",
       "2999891                        0                        0   \n",
       "2999902                        0                        0   \n",
       "2999923                        0                        0   \n",
       "2999974                        0                        0   \n",
       "\n",
       "         enc_loans_credit_type_7  enc_loans_account_cur_1  \\\n",
       "id                                                          \n",
       "488                            0                        1   \n",
       "1327                           0                        1   \n",
       "2165                           0                        1   \n",
       "2275                           0                        1   \n",
       "3974                           0                        1   \n",
       "...                          ...                      ...   \n",
       "2999887                        0                        1   \n",
       "2999891                        0                        1   \n",
       "2999902                        0                        1   \n",
       "2999923                        0                        1   \n",
       "2999974                        0                        1   \n",
       "\n",
       "         enc_loans_account_cur_2  enc_loans_account_cur_3  target  \n",
       "id                                                                 \n",
       "488                            0                        0       0  \n",
       "1327                           0                        0       0  \n",
       "2165                           0                        0       0  \n",
       "2275                           0                        0       0  \n",
       "3974                           0                        0       0  \n",
       "...                          ...                      ...     ...  \n",
       "2999887                        0                        0       0  \n",
       "2999891                        0                        0       0  \n",
       "2999902                        0                        0       0  \n",
       "2999923                        0                        0       0  \n",
       "2999974                        0                        0       0  \n",
       "\n",
       "[140830 rows x 196 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2859170 entries, 0 to 2999999\n",
      "Columns: 196 entries, rn to target\n",
      "dtypes: int64(8), int8(188)\n",
      "memory usage: 708.9 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_64_cols  = df.select_dtypes('int64').columns\n",
    "df[int_64_cols] = df[int_64_cols].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03740344539696373"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()[1]/df['target'].value_counts()[0]\n",
    "#strong class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7232753423140577\n",
      "test:  0.721330067265804\n"
     ]
    }
   ],
   "source": [
    "log = LogisticRegression()\n",
    "log.fit(X_train_sc, y_train)\n",
    "print('train: ', roc_auc_score(y_train, log.predict_proba(X_train_sc)[:,1]))\n",
    "print('test: ', roc_auc_score(y_test, log.predict_proba(X_test_sc)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7432296527381334\n",
      "test:  0.7173899047440927\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "os = SMOTE(k_neighbors=2)\n",
    "X_train_2, y_train_2 = os.fit_resample(X_train_sc, y_train)\n",
    "\n",
    "log.fit(X_train_2, y_train_2)\n",
    "print('train: ', roc_auc_score(y_train_2, log.predict_proba(X_train_2)[:,1]))\n",
    "print('test: ', roc_auc_score(y_test, log.predict_proba(X_test_sc)[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score has not improved, model is a little overfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [10, 1, 0.1, 0.01],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;balanced&#x27;, None]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=2, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={&#x27;C&#x27;: [10, 1, 0.1, 0.01],\n",
       "                         &#x27;class_weight&#x27;: [&#x27;balanced&#x27;, None]},\n",
       "             scoring=&#x27;roc_auc&#x27;, verbose=1)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2, estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [10, 1, 0.1, 0.01],\n",
       "                         'class_weight': ['balanced', None]},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_log = {'C': [10, 1, 0.1, 0.01],\n",
    "             'class_weight': ['balanced', None]}\n",
    "grid_log = GridSearchCV(log, param_grid=params_log, cv=2, scoring='roc_auc', verbose=1, n_jobs=-1)\n",
    "grid_log.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7256235829729973"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_log.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bogho\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  0.7267444233617524\n",
      "test:  0.7245628731277359\n"
     ]
    }
   ],
   "source": [
    "grid_log_model = grid_log.best_estimator_\n",
    "grid_log_model.fit(X_train_sc, y_train)\n",
    "print('train: ', roc_auc_score(y_train, grid_log_model.predict_proba(X_train_sc)[:,1]))\n",
    "print('test: ', roc_auc_score(y_test, grid_log_model.predict_proba(X_test_sc)[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ad2f2085b74124bd1f6cf796cb7425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5136239\tbest: 0.5136239 (0)\ttotal: 316ms\tremaining: 1m 2s\n",
      "20:\ttest: 0.7268109\tbest: 0.7268109 (20)\ttotal: 11s\tremaining: 1m 34s\n",
      "40:\ttest: 0.7377880\tbest: 0.7377880 (40)\ttotal: 21.7s\tremaining: 1m 24s\n",
      "60:\ttest: 0.7429433\tbest: 0.7429433 (60)\ttotal: 32.5s\tremaining: 1m 14s\n",
      "80:\ttest: 0.7452983\tbest: 0.7452983 (80)\ttotal: 42.8s\tremaining: 1m 2s\n",
      "100:\ttest: 0.7465679\tbest: 0.7465679 (100)\ttotal: 52.6s\tremaining: 51.6s\n",
      "120:\ttest: 0.7474535\tbest: 0.7474535 (120)\ttotal: 1m 2s\tremaining: 40.7s\n",
      "140:\ttest: 0.7483265\tbest: 0.7483265 (140)\ttotal: 1m 11s\tremaining: 30.1s\n",
      "160:\ttest: 0.7487045\tbest: 0.7487069 (158)\ttotal: 1m 21s\tremaining: 19.7s\n",
      "180:\ttest: 0.7490716\tbest: 0.7490716 (180)\ttotal: 1m 30s\tremaining: 9.5s\n",
      "199:\ttest: 0.7493597\tbest: 0.7493773 (197)\ttotal: 1m 40s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7493772559\n",
      "bestIteration = 197\n",
      "\n",
      "Shrink model to first 198 iterations.\n",
      "train:  0.7578590434922184\n",
      "test:  0.7493772559287459\n"
     ]
    }
   ],
   "source": [
    "boosting_model = catboost.CatBoostClassifier(n_estimators=200, learning_rate=0.3, eval_metric='AUC',\n",
    "                                             early_stopping_rounds=20, verbose=20, random_seed=42)\n",
    "boosting_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=20, plot=True)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, boosting_model.predict_proba(X_train)[:, 1]))\n",
    "print('test: ', roc_auc_score(y_test, boosting_model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063358a704a54a6497f81ea47d5f48e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 177ms\tremaining: 35.2s\n",
      "20:\ttest: 0.6941101\tbest: 0.6941101 (20)\ttotal: 4.86s\tremaining: 41.4s\n",
      "40:\ttest: 0.7078560\tbest: 0.7078560 (40)\ttotal: 10.6s\tremaining: 41.1s\n",
      "60:\ttest: 0.7169596\tbest: 0.7169596 (60)\ttotal: 16.5s\tremaining: 37.5s\n",
      "80:\ttest: 0.7235050\tbest: 0.7235050 (80)\ttotal: 22.7s\tremaining: 33.3s\n",
      "100:\ttest: 0.7275101\tbest: 0.7275101 (100)\ttotal: 29.3s\tremaining: 28.7s\n",
      "120:\ttest: 0.7309723\tbest: 0.7309723 (120)\ttotal: 36.3s\tremaining: 23.7s\n",
      "140:\ttest: 0.7341186\tbest: 0.7341186 (140)\ttotal: 43.2s\tremaining: 18.1s\n",
      "160:\ttest: 0.7363189\tbest: 0.7363189 (160)\ttotal: 49.5s\tremaining: 12s\n",
      "180:\ttest: 0.7382495\tbest: 0.7382495 (180)\ttotal: 56s\tremaining: 5.88s\n",
      "199:\ttest: 0.7393601\tbest: 0.7393601 (199)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7393601424\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "0:\tloss: 0.7393601\tbest: 0.7393601 (0)\ttotal: 1m 4s\tremaining: 33m 17s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7110566\tbest: 0.7110566 (20)\ttotal: 6.41s\tremaining: 54.6s\n",
      "40:\ttest: 0.7257202\tbest: 0.7257202 (40)\ttotal: 12.5s\tremaining: 48.5s\n",
      "60:\ttest: 0.7332879\tbest: 0.7332879 (60)\ttotal: 19.2s\tremaining: 43.7s\n",
      "80:\ttest: 0.7373864\tbest: 0.7373864 (80)\ttotal: 26s\tremaining: 38.2s\n",
      "100:\ttest: 0.7404107\tbest: 0.7404107 (100)\ttotal: 32.8s\tremaining: 32.1s\n",
      "120:\ttest: 0.7423518\tbest: 0.7423518 (120)\ttotal: 39.9s\tremaining: 26s\n",
      "140:\ttest: 0.7433657\tbest: 0.7433657 (140)\ttotal: 45.9s\tremaining: 19.2s\n",
      "160:\ttest: 0.7448501\tbest: 0.7448501 (160)\ttotal: 53s\tremaining: 12.8s\n",
      "180:\ttest: 0.7455125\tbest: 0.7455128 (179)\ttotal: 59.1s\tremaining: 6.21s\n",
      "199:\ttest: 0.7459531\tbest: 0.7459531 (199)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7459531324\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "1:\tloss: 0.7459531\tbest: 0.7459531 (1)\ttotal: 2m 9s\tremaining: 32m 21s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 235ms\tremaining: 46.8s\n",
      "20:\ttest: 0.7185768\tbest: 0.7185768 (20)\ttotal: 6.48s\tremaining: 55.3s\n",
      "40:\ttest: 0.7308254\tbest: 0.7308254 (40)\ttotal: 12.8s\tremaining: 49.5s\n",
      "60:\ttest: 0.7380665\tbest: 0.7380665 (60)\ttotal: 19.2s\tremaining: 43.7s\n",
      "80:\ttest: 0.7411564\tbest: 0.7411564 (80)\ttotal: 25.4s\tremaining: 37.3s\n",
      "100:\ttest: 0.7432658\tbest: 0.7432658 (100)\ttotal: 31.6s\tremaining: 31s\n",
      "120:\ttest: 0.7446442\tbest: 0.7446442 (120)\ttotal: 37.7s\tremaining: 24.6s\n",
      "140:\ttest: 0.7451662\tbest: 0.7451662 (140)\ttotal: 43.9s\tremaining: 18.4s\n",
      "160:\ttest: 0.7462863\tbest: 0.7463030 (158)\ttotal: 50.4s\tremaining: 12.2s\n",
      "180:\ttest: 0.7470859\tbest: 0.7470859 (180)\ttotal: 56.8s\tremaining: 5.96s\n",
      "199:\ttest: 0.7474472\tbest: 0.7474523 (197)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7474523159\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "2:\tloss: 0.7474523\tbest: 0.7474523 (2)\ttotal: 3m 12s\tremaining: 30m 58s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 231ms\tremaining: 46s\n",
      "20:\ttest: 0.7249919\tbest: 0.7249919 (20)\ttotal: 7.35s\tremaining: 1m 2s\n",
      "40:\ttest: 0.7353034\tbest: 0.7353034 (40)\ttotal: 14.1s\tremaining: 54.8s\n",
      "60:\ttest: 0.7392334\tbest: 0.7392334 (60)\ttotal: 20.4s\tremaining: 46.4s\n",
      "80:\ttest: 0.7430480\tbest: 0.7430480 (80)\ttotal: 27.6s\tremaining: 40.6s\n",
      "100:\ttest: 0.7444092\tbest: 0.7444092 (100)\ttotal: 34.8s\tremaining: 34.1s\n",
      "120:\ttest: 0.7458438\tbest: 0.7458438 (120)\ttotal: 40.8s\tremaining: 26.6s\n",
      "140:\ttest: 0.7468424\tbest: 0.7468424 (140)\ttotal: 46.9s\tremaining: 19.6s\n",
      "160:\ttest: 0.7474859\tbest: 0.7474935 (155)\ttotal: 53.1s\tremaining: 12.9s\n",
      "180:\ttest: 0.7480808\tbest: 0.7480857 (178)\ttotal: 59.3s\tremaining: 6.22s\n",
      "199:\ttest: 0.7482791\tbest: 0.7483154 (197)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7483154481\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "3:\tloss: 0.7483154\tbest: 0.7483154 (3)\ttotal: 4m 16s\tremaining: 29m 58s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 256ms\tremaining: 51s\n",
      "20:\ttest: 0.7008331\tbest: 0.7008331 (20)\ttotal: 6.49s\tremaining: 55.4s\n",
      "40:\ttest: 0.7134338\tbest: 0.7134338 (40)\ttotal: 13.2s\tremaining: 51s\n",
      "60:\ttest: 0.7215962\tbest: 0.7215962 (60)\ttotal: 19.5s\tremaining: 44.4s\n",
      "80:\ttest: 0.7282169\tbest: 0.7282169 (80)\ttotal: 26.3s\tremaining: 38.6s\n",
      "100:\ttest: 0.7326643\tbest: 0.7326643 (100)\ttotal: 33.2s\tremaining: 32.5s\n",
      "120:\ttest: 0.7352217\tbest: 0.7352217 (120)\ttotal: 39.6s\tremaining: 25.9s\n",
      "140:\ttest: 0.7377332\tbest: 0.7377332 (140)\ttotal: 46.2s\tremaining: 19.3s\n",
      "160:\ttest: 0.7396521\tbest: 0.7396521 (160)\ttotal: 53.1s\tremaining: 12.9s\n",
      "180:\ttest: 0.7410868\tbest: 0.7410868 (180)\ttotal: 1m\tremaining: 6.3s\n",
      "199:\ttest: 0.7421636\tbest: 0.7421636 (199)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7421636392\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "4:\tloss: 0.7421636\tbest: 0.7483154 (3)\ttotal: 5m 23s\tremaining: 29m 6s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 234ms\tremaining: 46.6s\n",
      "20:\ttest: 0.7153431\tbest: 0.7153431 (20)\ttotal: 6.69s\tremaining: 57.1s\n",
      "40:\ttest: 0.7301921\tbest: 0.7301921 (40)\ttotal: 13.9s\tremaining: 53.8s\n",
      "60:\ttest: 0.7363345\tbest: 0.7363345 (60)\ttotal: 20.9s\tremaining: 47.7s\n",
      "80:\ttest: 0.7401659\tbest: 0.7401856 (79)\ttotal: 28s\tremaining: 41.2s\n",
      "100:\ttest: 0.7423290\tbest: 0.7423290 (100)\ttotal: 34.6s\tremaining: 33.9s\n",
      "120:\ttest: 0.7440772\tbest: 0.7440772 (120)\ttotal: 41.4s\tremaining: 27s\n",
      "140:\ttest: 0.7451441\tbest: 0.7451441 (140)\ttotal: 48s\tremaining: 20.1s\n",
      "160:\ttest: 0.7460738\tbest: 0.7460738 (160)\ttotal: 54.6s\tremaining: 13.2s\n",
      "180:\ttest: 0.7466708\tbest: 0.7466708 (180)\ttotal: 1m 1s\tremaining: 6.42s\n",
      "199:\ttest: 0.7470675\tbest: 0.7470706 (198)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7470706135\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "5:\tloss: 0.7470706\tbest: 0.7483154 (3)\ttotal: 6m 30s\tremaining: 28m 12s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 238ms\tremaining: 47.3s\n",
      "20:\ttest: 0.7225821\tbest: 0.7225821 (20)\ttotal: 6.86s\tremaining: 58.5s\n",
      "40:\ttest: 0.7353872\tbest: 0.7353872 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7398937\tbest: 0.7398937 (60)\ttotal: 20.7s\tremaining: 47.2s\n",
      "80:\ttest: 0.7433685\tbest: 0.7433778 (78)\ttotal: 27.2s\tremaining: 39.9s\n",
      "100:\ttest: 0.7453004\tbest: 0.7453340 (99)\ttotal: 33.5s\tremaining: 32.8s\n",
      "120:\ttest: 0.7468075\tbest: 0.7468075 (120)\ttotal: 39.9s\tremaining: 26s\n",
      "140:\ttest: 0.7474972\tbest: 0.7475054 (139)\ttotal: 46s\tremaining: 19.3s\n",
      "160:\ttest: 0.7479784\tbest: 0.7479784 (160)\ttotal: 52.5s\tremaining: 12.7s\n",
      "180:\ttest: 0.7484241\tbest: 0.7484526 (179)\ttotal: 58.7s\tremaining: 6.16s\n",
      "199:\ttest: 0.7487468\tbest: 0.7487472 (198)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7487472057\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "6:\tloss: 0.7487472\tbest: 0.7487472 (6)\ttotal: 7m 35s\tremaining: 27m 6s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 239ms\tremaining: 47.5s\n",
      "20:\ttest: 0.7272551\tbest: 0.7272551 (20)\ttotal: 7.09s\tremaining: 1m\n",
      "40:\ttest: 0.7386984\tbest: 0.7386984 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7419438\tbest: 0.7419438 (60)\ttotal: 20.3s\tremaining: 46.4s\n",
      "80:\ttest: 0.7448588\tbest: 0.7448588 (80)\ttotal: 26.6s\tremaining: 39.1s\n",
      "100:\ttest: 0.7466908\tbest: 0.7466908 (100)\ttotal: 33.5s\tremaining: 32.9s\n",
      "120:\ttest: 0.7474343\tbest: 0.7474343 (120)\ttotal: 39.5s\tremaining: 25.8s\n",
      "140:\ttest: 0.7480092\tbest: 0.7480092 (140)\ttotal: 45.7s\tremaining: 19.1s\n",
      "160:\ttest: 0.7484148\tbest: 0.7484318 (159)\ttotal: 51.8s\tremaining: 12.5s\n",
      "180:\ttest: 0.7489792\tbest: 0.7489792 (180)\ttotal: 58.3s\tremaining: 6.12s\n",
      "199:\ttest: 0.7494230\tbest: 0.7494230 (199)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7494229948\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "7:\tloss: 0.7494230\tbest: 0.7494230 (7)\ttotal: 8m 40s\tremaining: 26m\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 231ms\tremaining: 45.9s\n",
      "20:\ttest: 0.7034370\tbest: 0.7034370 (20)\ttotal: 6.65s\tremaining: 56.7s\n",
      "40:\ttest: 0.7186187\tbest: 0.7186187 (40)\ttotal: 14.1s\tremaining: 54.7s\n",
      "60:\ttest: 0.7279600\tbest: 0.7279600 (60)\ttotal: 21.6s\tremaining: 49.3s\n",
      "80:\ttest: 0.7331008\tbest: 0.7331008 (80)\ttotal: 29.5s\tremaining: 43.3s\n",
      "100:\ttest: 0.7367479\tbest: 0.7367479 (100)\ttotal: 37.5s\tremaining: 36.7s\n",
      "120:\ttest: 0.7396022\tbest: 0.7396022 (120)\ttotal: 45.2s\tremaining: 29.5s\n",
      "140:\ttest: 0.7414634\tbest: 0.7414634 (140)\ttotal: 52.9s\tremaining: 22.1s\n",
      "160:\ttest: 0.7429885\tbest: 0.7429885 (160)\ttotal: 1m 2s\tremaining: 15.1s\n",
      "180:\ttest: 0.7442519\tbest: 0.7442519 (180)\ttotal: 1m 12s\tremaining: 7.58s\n",
      "199:\ttest: 0.7451130\tbest: 0.7451130 (199)\ttotal: 1m 19s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7451130219\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "8:\tloss: 0.7451130\tbest: 0.7494230 (7)\ttotal: 9m 59s\tremaining: 25m 33s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 243ms\tremaining: 48.3s\n",
      "20:\ttest: 0.7183954\tbest: 0.7183954 (20)\ttotal: 7.61s\tremaining: 1m 4s\n",
      "40:\ttest: 0.7324296\tbest: 0.7324296 (40)\ttotal: 15.9s\tremaining: 1m 1s\n",
      "60:\ttest: 0.7379777\tbest: 0.7379777 (60)\ttotal: 23.9s\tremaining: 54.5s\n",
      "80:\ttest: 0.7417054\tbest: 0.7417054 (80)\ttotal: 32.9s\tremaining: 48.3s\n",
      "100:\ttest: 0.7441019\tbest: 0.7441019 (100)\ttotal: 40.1s\tremaining: 39.3s\n",
      "120:\ttest: 0.7451301\tbest: 0.7451361 (119)\ttotal: 47.1s\tremaining: 30.7s\n",
      "140:\ttest: 0.7464575\tbest: 0.7464575 (140)\ttotal: 54.7s\tremaining: 22.9s\n",
      "160:\ttest: 0.7474590\tbest: 0.7474590 (160)\ttotal: 1m 2s\tremaining: 15s\n",
      "180:\ttest: 0.7476996\tbest: 0.7476996 (180)\ttotal: 1m 8s\tremaining: 7.2s\n",
      "199:\ttest: 0.7485463\tbest: 0.7485519 (198)\ttotal: 1m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7485518556\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "9:\tloss: 0.7485519\tbest: 0.7494230 (7)\ttotal: 11m 15s\tremaining: 24m 45s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 244ms\tremaining: 48.6s\n",
      "20:\ttest: 0.7266890\tbest: 0.7266890 (20)\ttotal: 7.72s\tremaining: 1m 5s\n",
      "40:\ttest: 0.7379538\tbest: 0.7379538 (40)\ttotal: 15.8s\tremaining: 1m 1s\n",
      "60:\ttest: 0.7432693\tbest: 0.7432693 (60)\ttotal: 23.2s\tremaining: 52.8s\n",
      "80:\ttest: 0.7452448\tbest: 0.7452633 (79)\ttotal: 30.2s\tremaining: 44.4s\n",
      "100:\ttest: 0.7466005\tbest: 0.7466005 (100)\ttotal: 36.9s\tremaining: 36.2s\n",
      "120:\ttest: 0.7477099\tbest: 0.7477099 (120)\ttotal: 44.1s\tremaining: 28.8s\n",
      "140:\ttest: 0.7484400\tbest: 0.7484400 (140)\ttotal: 50.6s\tremaining: 21.2s\n",
      "160:\ttest: 0.7486040\tbest: 0.7486128 (151)\ttotal: 57.1s\tremaining: 13.8s\n",
      "180:\ttest: 0.7489293\tbest: 0.7489661 (177)\ttotal: 1m 3s\tremaining: 6.69s\n",
      "199:\ttest: 0.7497762\tbest: 0.7497762 (199)\ttotal: 1m 11s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.749776223\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "10:\tloss: 0.7497762\tbest: 0.7497762 (10)\ttotal: 12m 26s\tremaining: 23m 45s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 317ms\tremaining: 1m 3s\n",
      "20:\ttest: 0.7329100\tbest: 0.7329100 (20)\ttotal: 9.23s\tremaining: 1m 18s\n",
      "40:\ttest: 0.7403687\tbest: 0.7403687 (40)\ttotal: 17.3s\tremaining: 1m 7s\n",
      "60:\ttest: 0.7442135\tbest: 0.7442135 (60)\ttotal: 25s\tremaining: 57.1s\n",
      "80:\ttest: 0.7458341\tbest: 0.7458341 (80)\ttotal: 31.9s\tremaining: 46.8s\n",
      "100:\ttest: 0.7469134\tbest: 0.7469134 (100)\ttotal: 38.9s\tremaining: 38.1s\n",
      "120:\ttest: 0.7477579\tbest: 0.7477579 (120)\ttotal: 45.9s\tremaining: 30s\n",
      "140:\ttest: 0.7482154\tbest: 0.7482154 (140)\ttotal: 52.4s\tremaining: 21.9s\n",
      "160:\ttest: 0.7487303\tbest: 0.7487768 (159)\ttotal: 58.8s\tremaining: 14.2s\n",
      "180:\ttest: 0.7493876\tbest: 0.7493876 (180)\ttotal: 1m 6s\tremaining: 6.97s\n",
      "199:\ttest: 0.7495236\tbest: 0.7495931 (195)\ttotal: 1m 13s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7495930754\n",
      "bestIteration = 195\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "11:\tloss: 0.7495931\tbest: 0.7497762 (10)\ttotal: 13m 40s\tremaining: 22m 47s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 298ms\tremaining: 59.4s\n",
      "20:\ttest: 0.7059916\tbest: 0.7059916 (20)\ttotal: 7.86s\tremaining: 1m 6s\n",
      "40:\ttest: 0.7231107\tbest: 0.7231107 (40)\ttotal: 17.1s\tremaining: 1m 6s\n",
      "60:\ttest: 0.7307839\tbest: 0.7307839 (60)\ttotal: 25s\tremaining: 57s\n",
      "80:\ttest: 0.7353967\tbest: 0.7353967 (80)\ttotal: 33.6s\tremaining: 49.4s\n",
      "100:\ttest: 0.7386240\tbest: 0.7386240 (100)\ttotal: 43.1s\tremaining: 42.3s\n",
      "120:\ttest: 0.7410789\tbest: 0.7410789 (120)\ttotal: 51.6s\tremaining: 33.7s\n",
      "140:\ttest: 0.7426301\tbest: 0.7426301 (140)\ttotal: 60s\tremaining: 25.1s\n",
      "160:\ttest: 0.7441156\tbest: 0.7441156 (160)\ttotal: 1m 8s\tremaining: 16.6s\n",
      "180:\ttest: 0.7452739\tbest: 0.7452739 (180)\ttotal: 1m 16s\tremaining: 8.07s\n",
      "199:\ttest: 0.7460072\tbest: 0.7460072 (199)\ttotal: 1m 24s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7460072366\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "12:\tloss: 0.7460072\tbest: 0.7497762 (10)\ttotal: 15m 4s\tremaining: 22m 2s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 291ms\tremaining: 58s\n",
      "20:\ttest: 0.7213168\tbest: 0.7213168 (20)\ttotal: 8.53s\tremaining: 1m 12s\n",
      "40:\ttest: 0.7355076\tbest: 0.7355076 (40)\ttotal: 17.2s\tremaining: 1m 6s\n",
      "60:\ttest: 0.7408197\tbest: 0.7408197 (60)\ttotal: 25.5s\tremaining: 58.1s\n",
      "80:\ttest: 0.7438093\tbest: 0.7438093 (80)\ttotal: 34.2s\tremaining: 50.2s\n",
      "100:\ttest: 0.7455383\tbest: 0.7455383 (100)\ttotal: 44.3s\tremaining: 43.4s\n",
      "120:\ttest: 0.7471059\tbest: 0.7471059 (120)\ttotal: 52.6s\tremaining: 34.4s\n",
      "140:\ttest: 0.7481037\tbest: 0.7481037 (140)\ttotal: 1m\tremaining: 25.3s\n",
      "160:\ttest: 0.7488365\tbest: 0.7488365 (160)\ttotal: 1m 8s\tremaining: 16.5s\n",
      "180:\ttest: 0.7495665\tbest: 0.7495665 (180)\ttotal: 1m 16s\tremaining: 7.99s\n",
      "199:\ttest: 0.7500661\tbest: 0.7500672 (198)\ttotal: 1m 23s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7500671645\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "13:\tloss: 0.7500672\tbest: 0.7500672 (13)\ttotal: 16m 28s\tremaining: 21m 10s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 297ms\tremaining: 59s\n",
      "20:\ttest: 0.7312267\tbest: 0.7312267 (20)\ttotal: 9.32s\tremaining: 1m 19s\n",
      "40:\ttest: 0.7398249\tbest: 0.7398249 (40)\ttotal: 17.3s\tremaining: 1m 7s\n",
      "60:\ttest: 0.7438871\tbest: 0.7438964 (59)\ttotal: 25.1s\tremaining: 57.3s\n",
      "80:\ttest: 0.7454393\tbest: 0.7454461 (78)\ttotal: 32.2s\tremaining: 47.2s\n",
      "100:\ttest: 0.7471704\tbest: 0.7472111 (99)\ttotal: 39.3s\tremaining: 38.5s\n",
      "120:\ttest: 0.7477966\tbest: 0.7477966 (120)\ttotal: 46s\tremaining: 30s\n",
      "140:\ttest: 0.7489173\tbest: 0.7489173 (140)\ttotal: 53.4s\tremaining: 22.3s\n",
      "160:\ttest: 0.7496694\tbest: 0.7496694 (160)\ttotal: 1m 2s\tremaining: 15.1s\n",
      "180:\ttest: 0.7498144\tbest: 0.7500010 (174)\ttotal: 1m 10s\tremaining: 7.39s\n",
      "199:\ttest: 0.7501471\tbest: 0.7501471 (199)\ttotal: 1m 17s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7501471068\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "14:\tloss: 0.7501471\tbest: 0.7501471 (14)\ttotal: 17m 45s\tremaining: 20m 8s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 301ms\tremaining: 60s\n",
      "20:\ttest: 0.7336760\tbest: 0.7336760 (20)\ttotal: 8.55s\tremaining: 1m 12s\n",
      "40:\ttest: 0.7421537\tbest: 0.7421537 (40)\ttotal: 16.5s\tremaining: 1m 3s\n",
      "60:\ttest: 0.7457007\tbest: 0.7457007 (60)\ttotal: 24s\tremaining: 54.7s\n",
      "80:\ttest: 0.7472166\tbest: 0.7472166 (80)\ttotal: 31s\tremaining: 45.5s\n",
      "100:\ttest: 0.7482485\tbest: 0.7482485 (100)\ttotal: 37.8s\tremaining: 37.1s\n",
      "120:\ttest: 0.7489985\tbest: 0.7490256 (119)\ttotal: 45.2s\tremaining: 29.5s\n",
      "140:\ttest: 0.7490803\tbest: 0.7491625 (129)\ttotal: 52.1s\tremaining: 21.8s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7491625186\n",
      "bestIteration = 129\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "15:\tloss: 0.7491625\tbest: 0.7501471 (14)\ttotal: 18m 41s\tremaining: 18m 41s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 223ms\tremaining: 44.3s\n",
      "20:\ttest: 0.6941101\tbest: 0.6941101 (20)\ttotal: 5.75s\tremaining: 49s\n",
      "40:\ttest: 0.7078560\tbest: 0.7078560 (40)\ttotal: 11.6s\tremaining: 45.1s\n",
      "60:\ttest: 0.7169596\tbest: 0.7169596 (60)\ttotal: 17.3s\tremaining: 39.5s\n",
      "80:\ttest: 0.7235050\tbest: 0.7235050 (80)\ttotal: 23.2s\tremaining: 34s\n",
      "100:\ttest: 0.7275101\tbest: 0.7275101 (100)\ttotal: 29s\tremaining: 28.4s\n",
      "120:\ttest: 0.7309723\tbest: 0.7309723 (120)\ttotal: 34.8s\tremaining: 22.7s\n",
      "140:\ttest: 0.7341186\tbest: 0.7341186 (140)\ttotal: 41.1s\tremaining: 17.2s\n",
      "160:\ttest: 0.7363189\tbest: 0.7363189 (160)\ttotal: 46.9s\tremaining: 11.4s\n",
      "180:\ttest: 0.7382495\tbest: 0.7382495 (180)\ttotal: 53s\tremaining: 5.56s\n",
      "199:\ttest: 0.7393601\tbest: 0.7393601 (199)\ttotal: 59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7393601424\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "16:\tloss: 0.7393601\tbest: 0.7501471 (14)\ttotal: 19m 40s\tremaining: 17m 21s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 223ms\tremaining: 44.4s\n",
      "20:\ttest: 0.7110566\tbest: 0.7110566 (20)\ttotal: 6.85s\tremaining: 58.4s\n",
      "40:\ttest: 0.7257202\tbest: 0.7257202 (40)\ttotal: 13.1s\tremaining: 50.6s\n",
      "60:\ttest: 0.7332879\tbest: 0.7332879 (60)\ttotal: 19.6s\tremaining: 44.7s\n",
      "80:\ttest: 0.7373864\tbest: 0.7373864 (80)\ttotal: 26.2s\tremaining: 38.5s\n",
      "100:\ttest: 0.7404107\tbest: 0.7404107 (100)\ttotal: 32.3s\tremaining: 31.6s\n",
      "120:\ttest: 0.7423518\tbest: 0.7423518 (120)\ttotal: 38.2s\tremaining: 25s\n",
      "140:\ttest: 0.7433657\tbest: 0.7433657 (140)\ttotal: 44.1s\tremaining: 18.5s\n",
      "160:\ttest: 0.7448501\tbest: 0.7448501 (160)\ttotal: 50.4s\tremaining: 12.2s\n",
      "180:\ttest: 0.7455125\tbest: 0.7455128 (179)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7459531\tbest: 0.7459531 (199)\ttotal: 1m 1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7459531324\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "17:\tloss: 0.7459531\tbest: 0.7501471 (14)\ttotal: 20m 41s\tremaining: 16m 5s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7185768\tbest: 0.7185768 (20)\ttotal: 6.28s\tremaining: 53.5s\n",
      "40:\ttest: 0.7308254\tbest: 0.7308254 (40)\ttotal: 12.4s\tremaining: 48.1s\n",
      "60:\ttest: 0.7380665\tbest: 0.7380665 (60)\ttotal: 18.7s\tremaining: 42.6s\n",
      "80:\ttest: 0.7411564\tbest: 0.7411564 (80)\ttotal: 25.1s\tremaining: 36.9s\n",
      "100:\ttest: 0.7432658\tbest: 0.7432658 (100)\ttotal: 31.1s\tremaining: 30.5s\n",
      "120:\ttest: 0.7446442\tbest: 0.7446442 (120)\ttotal: 36.9s\tremaining: 24.1s\n",
      "140:\ttest: 0.7451662\tbest: 0.7451662 (140)\ttotal: 42.8s\tremaining: 17.9s\n",
      "160:\ttest: 0.7462863\tbest: 0.7463030 (158)\ttotal: 49.5s\tremaining: 12s\n",
      "180:\ttest: 0.7470859\tbest: 0.7470859 (180)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7474472\tbest: 0.7474523 (197)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7474523159\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "18:\tloss: 0.7474523\tbest: 0.7501471 (14)\ttotal: 21m 44s\tremaining: 14m 52s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 252ms\tremaining: 50.2s\n",
      "20:\ttest: 0.7249919\tbest: 0.7249919 (20)\ttotal: 6.6s\tremaining: 56.3s\n",
      "40:\ttest: 0.7353034\tbest: 0.7353034 (40)\ttotal: 12.8s\tremaining: 49.7s\n",
      "60:\ttest: 0.7392334\tbest: 0.7392334 (60)\ttotal: 18.3s\tremaining: 41.7s\n",
      "80:\ttest: 0.7430480\tbest: 0.7430480 (80)\ttotal: 24.3s\tremaining: 35.8s\n",
      "100:\ttest: 0.7444092\tbest: 0.7444092 (100)\ttotal: 29.9s\tremaining: 29.3s\n",
      "120:\ttest: 0.7458438\tbest: 0.7458438 (120)\ttotal: 35.9s\tremaining: 23.4s\n",
      "140:\ttest: 0.7468424\tbest: 0.7468424 (140)\ttotal: 42s\tremaining: 17.6s\n",
      "160:\ttest: 0.7474859\tbest: 0.7474935 (155)\ttotal: 47.7s\tremaining: 11.6s\n",
      "180:\ttest: 0.7480808\tbest: 0.7480857 (178)\ttotal: 53.5s\tremaining: 5.61s\n",
      "199:\ttest: 0.7482791\tbest: 0.7483154 (197)\ttotal: 58.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7483154481\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "19:\tloss: 0.7483154\tbest: 0.7501471 (14)\ttotal: 22m 43s\tremaining: 13m 38s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7008331\tbest: 0.7008331 (20)\ttotal: 6.51s\tremaining: 55.5s\n",
      "40:\ttest: 0.7134338\tbest: 0.7134338 (40)\ttotal: 13.6s\tremaining: 52.6s\n",
      "60:\ttest: 0.7215962\tbest: 0.7215962 (60)\ttotal: 20s\tremaining: 45.5s\n",
      "80:\ttest: 0.7282169\tbest: 0.7282169 (80)\ttotal: 26.7s\tremaining: 39.2s\n",
      "100:\ttest: 0.7326643\tbest: 0.7326643 (100)\ttotal: 34s\tremaining: 33.3s\n",
      "120:\ttest: 0.7352217\tbest: 0.7352217 (120)\ttotal: 40.8s\tremaining: 26.6s\n",
      "140:\ttest: 0.7377332\tbest: 0.7377332 (140)\ttotal: 47.7s\tremaining: 20s\n",
      "160:\ttest: 0.7396521\tbest: 0.7396521 (160)\ttotal: 54.4s\tremaining: 13.2s\n",
      "180:\ttest: 0.7410868\tbest: 0.7410868 (180)\ttotal: 1m 1s\tremaining: 6.46s\n",
      "199:\ttest: 0.7421636\tbest: 0.7421636 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7421636392\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "20:\tloss: 0.7421636\tbest: 0.7501471 (14)\ttotal: 23m 51s\tremaining: 12m 29s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 233ms\tremaining: 46.3s\n",
      "20:\ttest: 0.7153431\tbest: 0.7153431 (20)\ttotal: 7.19s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7301921\tbest: 0.7301921 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7363345\tbest: 0.7363345 (60)\ttotal: 21.5s\tremaining: 49s\n",
      "80:\ttest: 0.7401659\tbest: 0.7401856 (79)\ttotal: 28.3s\tremaining: 41.6s\n",
      "100:\ttest: 0.7423290\tbest: 0.7423290 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7440772\tbest: 0.7440772 (120)\ttotal: 41.4s\tremaining: 27s\n",
      "140:\ttest: 0.7451441\tbest: 0.7451441 (140)\ttotal: 48.1s\tremaining: 20.1s\n",
      "160:\ttest: 0.7460738\tbest: 0.7460738 (160)\ttotal: 54.6s\tremaining: 13.2s\n",
      "180:\ttest: 0.7466708\tbest: 0.7466708 (180)\ttotal: 1m 1s\tremaining: 6.42s\n",
      "199:\ttest: 0.7470675\tbest: 0.7470706 (198)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7470706135\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "21:\tloss: 0.7470706\tbest: 0.7501471 (14)\ttotal: 24m 58s\tremaining: 11m 21s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 261ms\tremaining: 52s\n",
      "20:\ttest: 0.7225821\tbest: 0.7225821 (20)\ttotal: 6.78s\tremaining: 57.8s\n",
      "40:\ttest: 0.7353872\tbest: 0.7353872 (40)\ttotal: 13.5s\tremaining: 52.5s\n",
      "60:\ttest: 0.7398937\tbest: 0.7398937 (60)\ttotal: 20.3s\tremaining: 46.3s\n",
      "80:\ttest: 0.7433685\tbest: 0.7433778 (78)\ttotal: 27.1s\tremaining: 39.9s\n",
      "100:\ttest: 0.7453004\tbest: 0.7453340 (99)\ttotal: 33.6s\tremaining: 32.9s\n",
      "120:\ttest: 0.7468075\tbest: 0.7468075 (120)\ttotal: 40.1s\tremaining: 26.2s\n",
      "140:\ttest: 0.7474972\tbest: 0.7475054 (139)\ttotal: 46.5s\tremaining: 19.4s\n",
      "160:\ttest: 0.7479784\tbest: 0.7479784 (160)\ttotal: 52.9s\tremaining: 12.8s\n",
      "180:\ttest: 0.7484241\tbest: 0.7484526 (179)\ttotal: 59.3s\tremaining: 6.22s\n",
      "199:\ttest: 0.7487468\tbest: 0.7487472 (198)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7487472057\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "22:\tloss: 0.7487472\tbest: 0.7501471 (14)\ttotal: 26m 3s\tremaining: 10m 11s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 240ms\tremaining: 47.8s\n",
      "20:\ttest: 0.7272551\tbest: 0.7272551 (20)\ttotal: 7.2s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7386984\tbest: 0.7386984 (40)\ttotal: 14.6s\tremaining: 56.8s\n",
      "60:\ttest: 0.7419438\tbest: 0.7419438 (60)\ttotal: 22.5s\tremaining: 51.3s\n",
      "80:\ttest: 0.7448588\tbest: 0.7448588 (80)\ttotal: 29.3s\tremaining: 43s\n",
      "100:\ttest: 0.7466908\tbest: 0.7466908 (100)\ttotal: 36.4s\tremaining: 35.7s\n",
      "120:\ttest: 0.7474343\tbest: 0.7474343 (120)\ttotal: 42.5s\tremaining: 27.8s\n",
      "140:\ttest: 0.7480092\tbest: 0.7480092 (140)\ttotal: 49.1s\tremaining: 20.6s\n",
      "160:\ttest: 0.7484148\tbest: 0.7484318 (159)\ttotal: 55.3s\tremaining: 13.4s\n",
      "180:\ttest: 0.7489792\tbest: 0.7489792 (180)\ttotal: 1m 1s\tremaining: 6.48s\n",
      "199:\ttest: 0.7494230\tbest: 0.7494230 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7494229948\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "23:\tloss: 0.7494230\tbest: 0.7501471 (14)\ttotal: 27m 12s\tremaining: 9m 4s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7034370\tbest: 0.7034370 (20)\ttotal: 6.84s\tremaining: 58.3s\n",
      "40:\ttest: 0.7186187\tbest: 0.7186187 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7279600\tbest: 0.7279600 (60)\ttotal: 22.1s\tremaining: 50.5s\n",
      "80:\ttest: 0.7331008\tbest: 0.7331008 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7367479\tbest: 0.7367479 (100)\ttotal: 40.7s\tremaining: 39.9s\n",
      "120:\ttest: 0.7396022\tbest: 0.7396022 (120)\ttotal: 48.3s\tremaining: 31.6s\n",
      "140:\ttest: 0.7414634\tbest: 0.7414634 (140)\ttotal: 56.3s\tremaining: 23.6s\n",
      "160:\ttest: 0.7429885\tbest: 0.7429885 (160)\ttotal: 1m 3s\tremaining: 15.5s\n",
      "180:\ttest: 0.7442519\tbest: 0.7442519 (180)\ttotal: 1m 11s\tremaining: 7.5s\n",
      "199:\ttest: 0.7451130\tbest: 0.7451130 (199)\ttotal: 1m 18s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7451130219\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "24:\tloss: 0.7451130\tbest: 0.7501471 (14)\ttotal: 28m 30s\tremaining: 7m 59s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 243ms\tremaining: 48.3s\n",
      "20:\ttest: 0.7183954\tbest: 0.7183954 (20)\ttotal: 8.34s\tremaining: 1m 11s\n",
      "40:\ttest: 0.7324296\tbest: 0.7324296 (40)\ttotal: 16s\tremaining: 1m 2s\n",
      "60:\ttest: 0.7379777\tbest: 0.7379777 (60)\ttotal: 24.1s\tremaining: 55s\n",
      "80:\ttest: 0.7417054\tbest: 0.7417054 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7441019\tbest: 0.7441019 (100)\ttotal: 39.6s\tremaining: 38.8s\n",
      "120:\ttest: 0.7451301\tbest: 0.7451361 (119)\ttotal: 46.6s\tremaining: 30.4s\n",
      "140:\ttest: 0.7464575\tbest: 0.7464575 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7474590\tbest: 0.7474590 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7476996\tbest: 0.7476996 (180)\ttotal: 1m 8s\tremaining: 7.14s\n",
      "199:\ttest: 0.7485463\tbest: 0.7485519 (198)\ttotal: 1m 14s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7485518556\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "25:\tloss: 0.7485519\tbest: 0.7501471 (14)\ttotal: 29m 45s\tremaining: 6m 52s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 228ms\tremaining: 45.3s\n",
      "20:\ttest: 0.7266890\tbest: 0.7266890 (20)\ttotal: 7.23s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7379538\tbest: 0.7379538 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7432693\tbest: 0.7432693 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7452448\tbest: 0.7452633 (79)\ttotal: 28.1s\tremaining: 41.3s\n",
      "100:\ttest: 0.7466005\tbest: 0.7466005 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7477099\tbest: 0.7477099 (120)\ttotal: 41.2s\tremaining: 26.9s\n",
      "140:\ttest: 0.7484400\tbest: 0.7484400 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7486040\tbest: 0.7486128 (151)\ttotal: 53.6s\tremaining: 13s\n",
      "180:\ttest: 0.7489293\tbest: 0.7489661 (177)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7497762\tbest: 0.7497762 (199)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.749776223\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "26:\tloss: 0.7497762\tbest: 0.7501471 (14)\ttotal: 30m 51s\tremaining: 5m 42s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 221ms\tremaining: 44s\n",
      "20:\ttest: 0.7329100\tbest: 0.7329100 (20)\ttotal: 7.42s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7403687\tbest: 0.7403687 (40)\ttotal: 14.5s\tremaining: 56.3s\n",
      "60:\ttest: 0.7442135\tbest: 0.7442135 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7458341\tbest: 0.7458341 (80)\ttotal: 27.7s\tremaining: 40.7s\n",
      "100:\ttest: 0.7469134\tbest: 0.7469134 (100)\ttotal: 34.4s\tremaining: 33.7s\n",
      "120:\ttest: 0.7477579\tbest: 0.7477579 (120)\ttotal: 40.9s\tremaining: 26.7s\n",
      "140:\ttest: 0.7482154\tbest: 0.7482154 (140)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7487303\tbest: 0.7487768 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "180:\ttest: 0.7493876\tbest: 0.7493876 (180)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7495236\tbest: 0.7495931 (195)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7495930754\n",
      "bestIteration = 195\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "27:\tloss: 0.7495931\tbest: 0.7501471 (14)\ttotal: 31m 57s\tremaining: 4m 33s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 283ms\tremaining: 56.2s\n",
      "20:\ttest: 0.7059916\tbest: 0.7059916 (20)\ttotal: 6.93s\tremaining: 59s\n",
      "40:\ttest: 0.7231107\tbest: 0.7231107 (40)\ttotal: 14.9s\tremaining: 57.7s\n",
      "60:\ttest: 0.7307839\tbest: 0.7307839 (60)\ttotal: 22.6s\tremaining: 51.4s\n",
      "80:\ttest: 0.7353967\tbest: 0.7353967 (80)\ttotal: 30.3s\tremaining: 44.5s\n",
      "100:\ttest: 0.7386240\tbest: 0.7386240 (100)\ttotal: 38.2s\tremaining: 37.5s\n",
      "120:\ttest: 0.7410789\tbest: 0.7410789 (120)\ttotal: 46.2s\tremaining: 30.2s\n",
      "140:\ttest: 0.7426301\tbest: 0.7426301 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7441156\tbest: 0.7441156 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7452739\tbest: 0.7452739 (180)\ttotal: 1m 9s\tremaining: 7.26s\n",
      "199:\ttest: 0.7460072\tbest: 0.7460072 (199)\ttotal: 1m 16s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7460072366\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "28:\tloss: 0.7460072\tbest: 0.7501471 (14)\ttotal: 33m 13s\tremaining: 3m 26s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 286ms\tremaining: 56.8s\n",
      "20:\ttest: 0.7213168\tbest: 0.7213168 (20)\ttotal: 7.97s\tremaining: 1m 7s\n",
      "40:\ttest: 0.7355076\tbest: 0.7355076 (40)\ttotal: 15.7s\tremaining: 1m\n",
      "60:\ttest: 0.7408197\tbest: 0.7408197 (60)\ttotal: 23.7s\tremaining: 54s\n",
      "80:\ttest: 0.7438093\tbest: 0.7438093 (80)\ttotal: 33s\tremaining: 48.4s\n",
      "100:\ttest: 0.7455383\tbest: 0.7455383 (100)\ttotal: 40.4s\tremaining: 39.6s\n",
      "120:\ttest: 0.7471059\tbest: 0.7471059 (120)\ttotal: 47.8s\tremaining: 31.2s\n",
      "140:\ttest: 0.7481037\tbest: 0.7481037 (140)\ttotal: 54.9s\tremaining: 23s\n",
      "160:\ttest: 0.7488365\tbest: 0.7488365 (160)\ttotal: 1m 1s\tremaining: 15s\n",
      "180:\ttest: 0.7495665\tbest: 0.7495665 (180)\ttotal: 1m 8s\tremaining: 7.22s\n",
      "199:\ttest: 0.7500661\tbest: 0.7500672 (198)\ttotal: 1m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7500671645\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "29:\tloss: 0.7500672\tbest: 0.7501471 (14)\ttotal: 34m 29s\tremaining: 2m 17s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 280ms\tremaining: 55.7s\n",
      "20:\ttest: 0.7312267\tbest: 0.7312267 (20)\ttotal: 8.1s\tremaining: 1m 9s\n",
      "40:\ttest: 0.7398249\tbest: 0.7398249 (40)\ttotal: 16.3s\tremaining: 1m 3s\n",
      "60:\ttest: 0.7438871\tbest: 0.7438964 (59)\ttotal: 25.2s\tremaining: 57.4s\n",
      "80:\ttest: 0.7454393\tbest: 0.7454461 (78)\ttotal: 33.2s\tremaining: 48.7s\n",
      "100:\ttest: 0.7471704\tbest: 0.7472111 (99)\ttotal: 41.4s\tremaining: 40.5s\n",
      "120:\ttest: 0.7477966\tbest: 0.7477966 (120)\ttotal: 48.9s\tremaining: 31.9s\n",
      "140:\ttest: 0.7489173\tbest: 0.7489173 (140)\ttotal: 57.2s\tremaining: 23.9s\n",
      "160:\ttest: 0.7496694\tbest: 0.7496694 (160)\ttotal: 1m 5s\tremaining: 15.8s\n",
      "180:\ttest: 0.7498144\tbest: 0.7500010 (174)\ttotal: 1m 12s\tremaining: 7.66s\n",
      "199:\ttest: 0.7501471\tbest: 0.7501471 (199)\ttotal: 1m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7501471068\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "30:\tloss: 0.7501471\tbest: 0.7501471 (14)\ttotal: 35m 49s\tremaining: 1m 9s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 330ms\tremaining: 1m 5s\n",
      "20:\ttest: 0.7336760\tbest: 0.7336760 (20)\ttotal: 8.78s\tremaining: 1m 14s\n",
      "40:\ttest: 0.7421537\tbest: 0.7421537 (40)\ttotal: 16.9s\tremaining: 1m 5s\n",
      "60:\ttest: 0.7457007\tbest: 0.7457007 (60)\ttotal: 25s\tremaining: 57s\n",
      "80:\ttest: 0.7472166\tbest: 0.7472166 (80)\ttotal: 33.4s\tremaining: 49.1s\n",
      "100:\ttest: 0.7482485\tbest: 0.7482485 (100)\ttotal: 41.5s\tremaining: 40.6s\n",
      "120:\ttest: 0.7489985\tbest: 0.7490256 (119)\ttotal: 49.4s\tremaining: 32.2s\n",
      "140:\ttest: 0.7490803\tbest: 0.7491625 (129)\ttotal: 56.8s\tremaining: 23.7s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7491625186\n",
      "bestIteration = 129\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "31:\tloss: 0.7491625\tbest: 0.7501471 (14)\ttotal: 36m 49s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/3]\n",
      "0:\ttest: 0.5466240\tbest: 0.5466240 (0)\ttotal: 308ms\tremaining: 1m 1s\n",
      "20:\ttest: 0.7280062\tbest: 0.7280062 (20)\ttotal: 7.44s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7372597\tbest: 0.7372597 (40)\ttotal: 14.3s\tremaining: 55.6s\n",
      "60:\ttest: 0.7412573\tbest: 0.7412573 (60)\ttotal: 20.9s\tremaining: 47.5s\n",
      "80:\ttest: 0.7433852\tbest: 0.7433852 (80)\ttotal: 27.2s\tremaining: 40s\n",
      "100:\ttest: 0.7441864\tbest: 0.7441900 (99)\ttotal: 33.5s\tremaining: 32.8s\n",
      "120:\ttest: 0.7447557\tbest: 0.7448117 (117)\ttotal: 39.3s\tremaining: 25.7s\n",
      "140:\ttest: 0.7449382\tbest: 0.7450386 (133)\ttotal: 45.3s\tremaining: 18.9s\n",
      "160:\ttest: 0.7453059\tbest: 0.7453059 (160)\ttotal: 51.4s\tremaining: 12.5s\n",
      "180:\ttest: 0.7456439\tbest: 0.7456526 (176)\ttotal: 57.5s\tremaining: 6.03s\n",
      "199:\ttest: 0.7458717\tbest: 0.7458717 (199)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.745871653\n",
      "bestIteration = 199\n",
      "\n",
      "Training on fold [1/3]\n",
      "0:\ttest: 0.5030965\tbest: 0.5030965 (0)\ttotal: 208ms\tremaining: 41.4s\n",
      "20:\ttest: 0.7269786\tbest: 0.7269786 (20)\ttotal: 6.96s\tremaining: 59.3s\n",
      "40:\ttest: 0.7359736\tbest: 0.7359736 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7395899\tbest: 0.7395899 (60)\ttotal: 20.7s\tremaining: 47.1s\n",
      "80:\ttest: 0.7424179\tbest: 0.7424200 (79)\ttotal: 27.4s\tremaining: 40.2s\n",
      "100:\ttest: 0.7438677\tbest: 0.7438677 (100)\ttotal: 33.8s\tremaining: 33.1s\n",
      "120:\ttest: 0.7449566\tbest: 0.7449770 (117)\ttotal: 40.6s\tremaining: 26.5s\n",
      "140:\ttest: 0.7450859\tbest: 0.7451233 (135)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7453141\tbest: 0.7454346 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "\n",
      "bestTest = 0.7454346082\n",
      "bestIteration = 159\n",
      "\n",
      "Training on fold [2/3]\n",
      "0:\ttest: 0.5472850\tbest: 0.5472850 (0)\ttotal: 283ms\tremaining: 56.3s\n",
      "20:\ttest: 0.7259946\tbest: 0.7259946 (20)\ttotal: 7.25s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7372018\tbest: 0.7372018 (40)\ttotal: 14s\tremaining: 54.1s\n",
      "60:\ttest: 0.7409509\tbest: 0.7409509 (60)\ttotal: 20.2s\tremaining: 46.1s\n",
      "80:\ttest: 0.7430899\tbest: 0.7430904 (79)\ttotal: 26.3s\tremaining: 38.7s\n",
      "100:\ttest: 0.7441000\tbest: 0.7441192 (99)\ttotal: 32.6s\tremaining: 32s\n",
      "120:\ttest: 0.7449599\tbest: 0.7449599 (120)\ttotal: 40.4s\tremaining: 26.4s\n",
      "140:\ttest: 0.7455849\tbest: 0.7455849 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7460361\tbest: 0.7460840 (157)\ttotal: 53.2s\tremaining: 12.9s\n",
      "180:\ttest: 0.7466474\tbest: 0.7466474 (180)\ttotal: 59.4s\tremaining: 6.23s\n",
      "199:\ttest: 0.7467573\tbest: 0.7468616 (186)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7468616149\n",
      "bestIteration = 186\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'min_data_in_leaf': 3,\n",
       "  'depth': 7,\n",
       "  'learning_rate': 0.30000000000000004},\n",
       " 'cv_results': defaultdict(list,\n",
       "             {'iterations': [0,\n",
       "               1,\n",
       "               2,\n",
       "               3,\n",
       "               4,\n",
       "               5,\n",
       "               6,\n",
       "               7,\n",
       "               8,\n",
       "               9,\n",
       "               10,\n",
       "               11,\n",
       "               12,\n",
       "               13,\n",
       "               14,\n",
       "               15,\n",
       "               16,\n",
       "               17,\n",
       "               18,\n",
       "               19,\n",
       "               20,\n",
       "               21,\n",
       "               22,\n",
       "               23,\n",
       "               24,\n",
       "               25,\n",
       "               26,\n",
       "               27,\n",
       "               28,\n",
       "               29,\n",
       "               30,\n",
       "               31,\n",
       "               32,\n",
       "               33,\n",
       "               34,\n",
       "               35,\n",
       "               36,\n",
       "               37,\n",
       "               38,\n",
       "               39,\n",
       "               40,\n",
       "               41,\n",
       "               42,\n",
       "               43,\n",
       "               44,\n",
       "               45,\n",
       "               46,\n",
       "               47,\n",
       "               48,\n",
       "               49,\n",
       "               50,\n",
       "               51,\n",
       "               52,\n",
       "               53,\n",
       "               54,\n",
       "               55,\n",
       "               56,\n",
       "               57,\n",
       "               58,\n",
       "               59,\n",
       "               60,\n",
       "               61,\n",
       "               62,\n",
       "               63,\n",
       "               64,\n",
       "               65,\n",
       "               66,\n",
       "               67,\n",
       "               68,\n",
       "               69,\n",
       "               70,\n",
       "               71,\n",
       "               72,\n",
       "               73,\n",
       "               74,\n",
       "               75,\n",
       "               76,\n",
       "               77,\n",
       "               78,\n",
       "               79,\n",
       "               80,\n",
       "               81,\n",
       "               82,\n",
       "               83,\n",
       "               84,\n",
       "               85,\n",
       "               86,\n",
       "               87,\n",
       "               88,\n",
       "               89,\n",
       "               90,\n",
       "               91,\n",
       "               92,\n",
       "               93,\n",
       "               94,\n",
       "               95,\n",
       "               96,\n",
       "               97,\n",
       "               98,\n",
       "               99,\n",
       "               100,\n",
       "               101,\n",
       "               102,\n",
       "               103,\n",
       "               104,\n",
       "               105,\n",
       "               106,\n",
       "               107,\n",
       "               108,\n",
       "               109,\n",
       "               110,\n",
       "               111,\n",
       "               112,\n",
       "               113,\n",
       "               114,\n",
       "               115,\n",
       "               116,\n",
       "               117,\n",
       "               118,\n",
       "               119,\n",
       "               120,\n",
       "               121,\n",
       "               122,\n",
       "               123,\n",
       "               124,\n",
       "               125,\n",
       "               126,\n",
       "               127,\n",
       "               128,\n",
       "               129,\n",
       "               130,\n",
       "               131,\n",
       "               132,\n",
       "               133,\n",
       "               134,\n",
       "               135,\n",
       "               136,\n",
       "               137,\n",
       "               138,\n",
       "               139,\n",
       "               140,\n",
       "               141,\n",
       "               142,\n",
       "               143,\n",
       "               144,\n",
       "               145,\n",
       "               146,\n",
       "               147,\n",
       "               148,\n",
       "               149,\n",
       "               150,\n",
       "               151,\n",
       "               152,\n",
       "               153,\n",
       "               154,\n",
       "               155,\n",
       "               156,\n",
       "               157,\n",
       "               158,\n",
       "               159,\n",
       "               160,\n",
       "               161,\n",
       "               162,\n",
       "               163,\n",
       "               164,\n",
       "               165,\n",
       "               166,\n",
       "               167,\n",
       "               168,\n",
       "               169,\n",
       "               170,\n",
       "               171,\n",
       "               172,\n",
       "               173,\n",
       "               174,\n",
       "               175,\n",
       "               176,\n",
       "               177,\n",
       "               178,\n",
       "               179,\n",
       "               180,\n",
       "               181,\n",
       "               182,\n",
       "               183,\n",
       "               184,\n",
       "               185,\n",
       "               186,\n",
       "               187,\n",
       "               188,\n",
       "               189,\n",
       "               190,\n",
       "               191,\n",
       "               192,\n",
       "               193,\n",
       "               194,\n",
       "               195,\n",
       "               196,\n",
       "               197,\n",
       "               198,\n",
       "               199],\n",
       "              'test-AUC-mean': [0.5323351871134413,\n",
       "               0.5672485377872802,\n",
       "               0.6073729499291639,\n",
       "               0.6682448148498139,\n",
       "               0.6815865549484652,\n",
       "               0.6905379693067895,\n",
       "               0.697254815238835,\n",
       "               0.6989171525505835,\n",
       "               0.7034855810236008,\n",
       "               0.7081957164191247,\n",
       "               0.712240618753471,\n",
       "               0.7139975265978536,\n",
       "               0.7159752865593644,\n",
       "               0.7175685424543378,\n",
       "               0.7192737969143583,\n",
       "               0.7205048017282761,\n",
       "               0.7221750988457073,\n",
       "               0.7236253837036376,\n",
       "               0.724682731134001,\n",
       "               0.7257887194849176,\n",
       "               0.7269931493303302,\n",
       "               0.7279065089832814,\n",
       "               0.7288224791887279,\n",
       "               0.7293681792534473,\n",
       "               0.7300194439278632,\n",
       "               0.7309084709246666,\n",
       "               0.7314423107118962,\n",
       "               0.7318070239971508,\n",
       "               0.7326287091636717,\n",
       "               0.7330962965236035,\n",
       "               0.7338660846256045,\n",
       "               0.7341638368570527,\n",
       "               0.7347563041881463,\n",
       "               0.7350512845234406,\n",
       "               0.7353708112052444,\n",
       "               0.7355543177924654,\n",
       "               0.7358423387504356,\n",
       "               0.7361120806751931,\n",
       "               0.7363384454684726,\n",
       "               0.7365760315204622,\n",
       "               0.7368117247389444,\n",
       "               0.7370277696510957,\n",
       "               0.7375601115497495,\n",
       "               0.7377754549380248,\n",
       "               0.7378912040874498,\n",
       "               0.7380370290763554,\n",
       "               0.7382350999242915,\n",
       "               0.7384344775060164,\n",
       "               0.7385329708839873,\n",
       "               0.7385593695851984,\n",
       "               0.7387566201175919,\n",
       "               0.7389830207813505,\n",
       "               0.7391369383935688,\n",
       "               0.7393880118340554,\n",
       "               0.7395543786468469,\n",
       "               0.7396444015967685,\n",
       "               0.7398647277582878,\n",
       "               0.7400291683973553,\n",
       "               0.7402085796786584,\n",
       "               0.7403351133767547,\n",
       "               0.7405993644608135,\n",
       "               0.740807386585787,\n",
       "               0.7409894061838483,\n",
       "               0.7411666514507255,\n",
       "               0.741305777489257,\n",
       "               0.7413729131333818,\n",
       "               0.7414738583117991,\n",
       "               0.741625442448362,\n",
       "               0.7417041600133513,\n",
       "               0.7418539191210747,\n",
       "               0.7419784933896265,\n",
       "               0.742097296431333,\n",
       "               0.7422724865920779,\n",
       "               0.7423835930003587,\n",
       "               0.7424318242905521,\n",
       "               0.7425292580388412,\n",
       "               0.742646728928146,\n",
       "               0.7427824920624081,\n",
       "               0.7428386192241708,\n",
       "               0.7429419396670838,\n",
       "               0.7429643327710437,\n",
       "               0.7430798827049051,\n",
       "               0.7431569563374699,\n",
       "               0.7432715937467483,\n",
       "               0.7433677880138255,\n",
       "               0.7434412558096488,\n",
       "               0.7435074239438036,\n",
       "               0.7435534593720735,\n",
       "               0.7435571054726401,\n",
       "               0.7435775020787556,\n",
       "               0.7436365666175054,\n",
       "               0.7437064085778027,\n",
       "               0.7437422225170586,\n",
       "               0.7437764896028108,\n",
       "               0.7438178820318823,\n",
       "               0.74387061624648,\n",
       "               0.7438905727649447,\n",
       "               0.7439927661673247,\n",
       "               0.7440024146013199,\n",
       "               0.7440377978482045,\n",
       "               0.7440513821192641,\n",
       "               0.7441273484926604,\n",
       "               0.7441669408564753,\n",
       "               0.7441858556113946,\n",
       "               0.7442124979466755,\n",
       "               0.7442771576979249,\n",
       "               0.7443005728006197,\n",
       "               0.7443311953439778,\n",
       "               0.7443650456556239,\n",
       "               0.744397027291618,\n",
       "               0.74440267424395,\n",
       "               0.7444382190263493,\n",
       "               0.7445137351926846,\n",
       "               0.7445525301781267,\n",
       "               0.7447270665016884,\n",
       "               0.7447642871083064,\n",
       "               0.7448551349518983,\n",
       "               0.744901713303259,\n",
       "               0.7448661628733549,\n",
       "               0.7448505716325475,\n",
       "               0.7448907389692989,\n",
       "               0.7449061961955286,\n",
       "               0.7449644381774257,\n",
       "               0.744962178409649,\n",
       "               0.7449676824869712,\n",
       "               0.7449661186879172,\n",
       "               0.7450267738808226,\n",
       "               0.7450470288716411,\n",
       "               0.745069257522755,\n",
       "               0.7450743944343543,\n",
       "               0.7450800318421195,\n",
       "               0.7450750130676216,\n",
       "               0.7450995540641578,\n",
       "               0.7451100815027649,\n",
       "               0.7450914218059653,\n",
       "               0.7450837653500435,\n",
       "               0.7450892893651595,\n",
       "               0.7450785936404719,\n",
       "               0.7451303086800137,\n",
       "               0.7452062206328023,\n",
       "               0.7452029918855784,\n",
       "               0.7452044702790896,\n",
       "               0.7452078183243406,\n",
       "               0.7452132176265415,\n",
       "               0.7452586561354283,\n",
       "               0.745277115252616,\n",
       "               0.7453254146410546,\n",
       "               0.7453198801360831,\n",
       "               0.7453331444036664,\n",
       "               0.7453590264265971,\n",
       "               0.7454014770083107,\n",
       "               0.7453813748399947,\n",
       "               0.7454102949284719,\n",
       "               0.7453979245570119,\n",
       "               0.7454442849065378,\n",
       "               0.7454711749955631,\n",
       "               0.7455215502306715,\n",
       "               0.7455523830208541,\n",
       "               0.7455553145576462,\n",
       "               0.7455870789437258,\n",
       "               0.7455520387048767,\n",
       "               0.7455574928939246,\n",
       "               0.7455483202326004,\n",
       "               0.7455471571826099,\n",
       "               0.745548586218574,\n",
       "               0.7455594295195236,\n",
       "               0.7456040841891513,\n",
       "               0.745622476326688,\n",
       "               0.7456657491240811,\n",
       "               0.7456576837885875,\n",
       "               0.7456868405923801,\n",
       "               0.7457052704803594,\n",
       "               0.7456997394776056,\n",
       "               0.7457246870874767,\n",
       "               0.7458033622719472,\n",
       "               0.7458844799535762,\n",
       "               0.7458810888184993,\n",
       "               0.7458754325125145,\n",
       "               0.7458818436495037,\n",
       "               0.7458766029947742,\n",
       "               0.7458869207620135,\n",
       "               0.7458900103891483,\n",
       "               0.7459060247495494,\n",
       "               0.7459091832116601,\n",
       "               0.745936544993763,\n",
       "               0.7459397310864073,\n",
       "               0.7459678665192087,\n",
       "               0.7459554688132831,\n",
       "               0.7459666337707551,\n",
       "               0.7459636588161501,\n",
       "               0.7459680057902269,\n",
       "               0.7459433389010277,\n",
       "               0.7459593686895082,\n",
       "               0.7459496736175785,\n",
       "               0.7459615684481582,\n",
       "               0.7459976774971092,\n",
       "               0.7459951829498932,\n",
       "               0.7459921037133087,\n",
       "               0.7459959742143821,\n",
       "               0.7459994991456217],\n",
       "              'test-AUC-std': [0.02532359239281949,\n",
       "               0.008466858922239156,\n",
       "               0.004585950280540858,\n",
       "               0.004815449956083742,\n",
       "               0.0019913225430410898,\n",
       "               0.0036590378327971533,\n",
       "               0.0010377349078268805,\n",
       "               0.0018098267662474293,\n",
       "               0.0021389364814496147,\n",
       "               0.0010816210541981707,\n",
       "               0.0009109152685680217,\n",
       "               0.002434088331193539,\n",
       "               0.0016368986811836261,\n",
       "               0.0023295211697072406,\n",
       "               0.002577920523229613,\n",
       "               0.0023204793795475248,\n",
       "               0.0015796647918663812,\n",
       "               0.0012675131083679748,\n",
       "               0.0005959262009161987,\n",
       "               0.0006274494740008236,\n",
       "               0.0010058442669958383,\n",
       "               0.0012940433041228297,\n",
       "               0.0013456221322952367,\n",
       "               0.001144895839357038,\n",
       "               0.0012123648314638605,\n",
       "               0.0013366982727912094,\n",
       "               0.001339040058419934,\n",
       "               0.0012871037044102289,\n",
       "               0.0013943295534002281,\n",
       "               0.0012317236773407934,\n",
       "               0.0009226238463269436,\n",
       "               0.0009207596209577093,\n",
       "               0.0008880951451911988,\n",
       "               0.0008277279030819567,\n",
       "               0.0009236325653394494,\n",
       "               0.0008266193498340217,\n",
       "               0.000908486289915637,\n",
       "               0.0008861306885369425,\n",
       "               0.0007098059362100951,\n",
       "               0.0006512860068509332,\n",
       "               0.0007263825197793467,\n",
       "               0.0007253717277105685,\n",
       "               0.0005512314198523541,\n",
       "               0.0007581512258018975,\n",
       "               0.0006857224396737535,\n",
       "               0.0007797304147385075,\n",
       "               0.0008170636530197181,\n",
       "               0.0007581922769637968,\n",
       "               0.0009294312946726416,\n",
       "               0.0009398192095068884,\n",
       "               0.0009126326959371571,\n",
       "               0.0009698487471650067,\n",
       "               0.0009380196943498978,\n",
       "               0.0007515349890663632,\n",
       "               0.0006187828199820805,\n",
       "               0.0005057435706896258,\n",
       "               0.0005871467477125114,\n",
       "               0.000615019317967743,\n",
       "               0.0007190485263371711,\n",
       "               0.0008866482515136522,\n",
       "               0.0008875667375294412,\n",
       "               0.0007285008398668062,\n",
       "               0.0008960573508490511,\n",
       "               0.0008079691274163551,\n",
       "               0.0007062063761600933,\n",
       "               0.00069487865013244,\n",
       "               0.0007233722926929515,\n",
       "               0.0005597041835083738,\n",
       "               0.0005264861194173963,\n",
       "               0.0004724843053420606,\n",
       "               0.0005093057362766326,\n",
       "               0.0005173542554170649,\n",
       "               0.0004808240967711858,\n",
       "               0.0004109066501969464,\n",
       "               0.00044570281115598973,\n",
       "               0.0004272792413529454,\n",
       "               0.000439054745604249,\n",
       "               0.00041803362579330325,\n",
       "               0.0004773632261178682,\n",
       "               0.00046583858069193973,\n",
       "               0.0004957508939481096,\n",
       "               0.0005727322649158489,\n",
       "               0.000561120993353562,\n",
       "               0.00042532409476542867,\n",
       "               0.0003669174434163326,\n",
       "               0.00037835918011397277,\n",
       "               0.0003135152491979665,\n",
       "               0.0003497234273534862,\n",
       "               0.00027304021948943374,\n",
       "               0.0002696544315423864,\n",
       "               0.0003023229851023641,\n",
       "               0.00025026774289106294,\n",
       "               0.0002469934991097206,\n",
       "               0.00022811245710394585,\n",
       "               0.00022213018620281731,\n",
       "               0.00020629942162416544,\n",
       "               0.00017864512502352914,\n",
       "               0.00017977635554250856,\n",
       "               0.0001847724069356753,\n",
       "               0.0002053234473095896,\n",
       "               0.0001648434464886782,\n",
       "               0.00020761385073964593,\n",
       "               0.00017284493789659886,\n",
       "               0.00017687275828017155,\n",
       "               0.00017746676757125648,\n",
       "               0.0002437725109907605,\n",
       "               0.0002940307387516383,\n",
       "               0.0003006814677872576,\n",
       "               0.0003304338159713527,\n",
       "               0.00032571608564376794,\n",
       "               0.00031200116755029656,\n",
       "               0.0003233719214620155,\n",
       "               0.00021271755789855547,\n",
       "               0.00019601258827327953,\n",
       "               5.5581490153552954e-05,\n",
       "               6.25634462612514e-05,\n",
       "               4.728296292219027e-05,\n",
       "               8.362111438530978e-05,\n",
       "               7.324550546186718e-05,\n",
       "               8.548065158074559e-05,\n",
       "               0.00011692009061906363,\n",
       "               0.000138881805328732,\n",
       "               0.000159372486809559,\n",
       "               0.00016631320414211336,\n",
       "               0.0001540334681279224,\n",
       "               0.00015879743220322478,\n",
       "               0.00013113605028878332,\n",
       "               0.0001007652880926456,\n",
       "               0.00011658553193894731,\n",
       "               0.00011585040775047589,\n",
       "               6.494702932034623e-05,\n",
       "               7.290157957630871e-05,\n",
       "               9.225930628721633e-05,\n",
       "               6.856129893881642e-05,\n",
       "               8.783686234356395e-05,\n",
       "               9.637590039608928e-05,\n",
       "               9.994698282443942e-05,\n",
       "               6.393479474443715e-05,\n",
       "               0.00013651364132303686,\n",
       "               0.0003143939839207871,\n",
       "               0.000338909681777747,\n",
       "               0.0003266314328202843,\n",
       "               0.0003432787309968498,\n",
       "               0.00037432162328763184,\n",
       "               0.00034363456493125894,\n",
       "               0.00035450351200104084,\n",
       "               0.00035323490055288895,\n",
       "               0.00036865662560145657,\n",
       "               0.0003782092031153797,\n",
       "               0.0003926083943459441,\n",
       "               0.0003963172431402646,\n",
       "               0.00039696883222072743,\n",
       "               0.00038464737381002504,\n",
       "               0.0003971747478911386,\n",
       "               0.00040905672666516406,\n",
       "               0.0004183232031727523,\n",
       "               0.0004381367373615701,\n",
       "               0.0004672035306302684,\n",
       "               0.0004367392241883043,\n",
       "               0.00040946734858020973,\n",
       "               0.0004192579104515046,\n",
       "               0.0004193533568755244,\n",
       "               0.00041762369170116943,\n",
       "               0.0004192615712828445,\n",
       "               0.0004362101258684112,\n",
       "               0.00042009784981146027,\n",
       "               0.00041748720069425047,\n",
       "               0.00040888224065514,\n",
       "               0.00038802859342702844,\n",
       "               0.00037026441296878425,\n",
       "               0.00037861962453010386,\n",
       "               0.00037692481417933614,\n",
       "               0.00043027335014486477,\n",
       "               0.00046121400920888083,\n",
       "               0.0005446723221166341,\n",
       "               0.0006442071833715171,\n",
       "               0.0006351865003248502,\n",
       "               0.0006310578465817754,\n",
       "               0.0006551561172386533,\n",
       "               0.0006502535147695251,\n",
       "               0.0006727224133376267,\n",
       "               0.0006781888070109002,\n",
       "               0.0007001067016156205,\n",
       "               0.0007298574057930118,\n",
       "               0.0007459932046994607,\n",
       "               0.0007589191745506932,\n",
       "               0.0007886918261306194,\n",
       "               0.0007713032934364513,\n",
       "               0.0007866913976575951,\n",
       "               0.0007817532214235002,\n",
       "               0.0007707687804994566,\n",
       "               0.0007440474526188147,\n",
       "               0.00072979931181204,\n",
       "               0.0007106698589331393,\n",
       "               0.0007112330868961144,\n",
       "               0.0007289071317548428,\n",
       "               0.0007106591616577817,\n",
       "               0.0007046394371559195,\n",
       "               0.0007053257487551718,\n",
       "               0.0007026838108733379],\n",
       "              'test-Logloss-mean': [0.35180698124470894,\n",
       "               0.23026996637385042,\n",
       "               0.18397858416083512,\n",
       "               0.16388931064488058,\n",
       "               0.15536707191661384,\n",
       "               0.1510823448581409,\n",
       "               0.14897560631064996,\n",
       "               0.1478944726648073,\n",
       "               0.14696395529728792,\n",
       "               0.14631252901941508,\n",
       "               0.145841463446266,\n",
       "               0.14553385428737553,\n",
       "               0.14517442685501203,\n",
       "               0.1449521560323742,\n",
       "               0.1447005467928781,\n",
       "               0.1445106367788774,\n",
       "               0.14432068138816234,\n",
       "               0.14414496820162906,\n",
       "               0.1439847337617169,\n",
       "               0.14387398888328623,\n",
       "               0.14372076145221693,\n",
       "               0.14362783120004263,\n",
       "               0.1435155113638539,\n",
       "               0.14343977554118434,\n",
       "               0.14335047017673552,\n",
       "               0.14324950377697632,\n",
       "               0.14318476583632203,\n",
       "               0.14314203109113346,\n",
       "               0.14304399774300544,\n",
       "               0.1429828010495902,\n",
       "               0.1428858103828851,\n",
       "               0.1428420702822026,\n",
       "               0.14277711119100542,\n",
       "               0.1427274278415724,\n",
       "               0.14268195075713838,\n",
       "               0.14264702864002174,\n",
       "               0.14261450467344372,\n",
       "               0.1425712541522943,\n",
       "               0.14254550909796357,\n",
       "               0.1425100209513985,\n",
       "               0.14248380388225937,\n",
       "               0.14245558471070216,\n",
       "               0.1423873084659136,\n",
       "               0.1423520416533882,\n",
       "               0.1423287957389005,\n",
       "               0.14231113232960577,\n",
       "               0.14228566315011004,\n",
       "               0.14225297869098777,\n",
       "               0.14224203548224248,\n",
       "               0.14223670194442095,\n",
       "               0.14221032597081132,\n",
       "               0.14217898327163894,\n",
       "               0.14216172290761173,\n",
       "               0.14213419209220823,\n",
       "               0.14211192770888062,\n",
       "               0.14210231526719985,\n",
       "               0.1420770087522377,\n",
       "               0.14204410282256663,\n",
       "               0.14202606162162001,\n",
       "               0.14201045814390814,\n",
       "               0.14197721330507398,\n",
       "               0.1419531007127323,\n",
       "               0.14192575821498002,\n",
       "               0.14190428045063205,\n",
       "               0.14189037205556174,\n",
       "               0.14188098536520286,\n",
       "               0.14186819654531244,\n",
       "               0.14184966432416232,\n",
       "               0.14184200720921683,\n",
       "               0.14182600533948445,\n",
       "               0.14181088796536323,\n",
       "               0.14180080846969337,\n",
       "               0.14178081205392776,\n",
       "               0.14176719688894732,\n",
       "               0.14175871266596737,\n",
       "               0.1417443225176791,\n",
       "               0.14173009605088135,\n",
       "               0.1417141639351123,\n",
       "               0.1417039128790523,\n",
       "               0.14168981376374737,\n",
       "               0.14168606369182637,\n",
       "               0.14167251170840303,\n",
       "               0.14166279303869245,\n",
       "               0.1416498733847369,\n",
       "               0.14163891132959164,\n",
       "               0.1416314475128556,\n",
       "               0.14162375164882954,\n",
       "               0.141617063539044,\n",
       "               0.14161587546664803,\n",
       "               0.14161372009561105,\n",
       "               0.14160998769666766,\n",
       "               0.14159501406094213,\n",
       "               0.14159161971921572,\n",
       "               0.14158785351263456,\n",
       "               0.14158697940660855,\n",
       "               0.1415806387129128,\n",
       "               0.1415760637177954,\n",
       "               0.14155996488760939,\n",
       "               0.14155938399343068,\n",
       "               0.14155387493436364,\n",
       "               0.14155017932230232,\n",
       "               0.1415419595162399,\n",
       "               0.14153822758747733,\n",
       "               0.1415350291456736,\n",
       "               0.1415312814443288,\n",
       "               0.14152165555152485,\n",
       "               0.14151630013793481,\n",
       "               0.14151476059711635,\n",
       "               0.14150941452489504,\n",
       "               0.14150600003735156,\n",
       "               0.1415052428427487,\n",
       "               0.14149906230146095,\n",
       "               0.1414926589488338,\n",
       "               0.14148659525263066,\n",
       "               0.1414660278632282,\n",
       "               0.14146043397263794,\n",
       "               0.14144873823631485,\n",
       "               0.14144002355354315,\n",
       "               0.14144415240665806,\n",
       "               0.14144604054162202,\n",
       "               0.14144125860299242,\n",
       "               0.14143835166125726,\n",
       "               0.14142670970835183,\n",
       "               0.1414250409164651,\n",
       "               0.1414200430209583,\n",
       "               0.14142080227367038,\n",
       "               0.1414180805128696,\n",
       "               0.14141618674546222,\n",
       "               0.14141078615690797,\n",
       "               0.14141248458233396,\n",
       "               0.1414112349496702,\n",
       "               0.14141413588947427,\n",
       "               0.14141265949537735,\n",
       "               0.14141145199939856,\n",
       "               0.14141178290805975,\n",
       "               0.14141114333444463,\n",
       "               0.14141080390410607,\n",
       "               0.14141310715010907,\n",
       "               0.1414108689820429,\n",
       "               0.14139944188870576,\n",
       "               0.1413977009161523,\n",
       "               0.1413963499663271,\n",
       "               0.1413958386481126,\n",
       "               0.14139585639743146,\n",
       "               0.14138849366175707,\n",
       "               0.14138650872046887,\n",
       "               0.1413791420381603,\n",
       "               0.1413770472558733,\n",
       "               0.14137833331128102,\n",
       "               0.14137638136198225,\n",
       "               0.14137064295410784,\n",
       "               0.14137072728782887,\n",
       "               0.14136202210448945,\n",
       "               0.14136268221242856,\n",
       "               0.14135602703029135,\n",
       "               0.141349377286439,\n",
       "               0.14134609326019285,\n",
       "               0.14134338895287124,\n",
       "               0.14134162195493039,\n",
       "               0.14133863938359015,\n",
       "               0.14133996281159514,\n",
       "               0.14133855952276983,\n",
       "               0.14133807029450488,\n",
       "               0.1413378928908927,\n",
       "               0.1413373025229666,\n",
       "               0.14133475574633034,\n",
       "               0.14133037028239848,\n",
       "               0.1413275942109404,\n",
       "               0.1413251591967822,\n",
       "               0.14132296669682873,\n",
       "               0.14131770421067583,\n",
       "               0.14131642574302705,\n",
       "               0.14131841985522717,\n",
       "               0.1413192589439878,\n",
       "               0.14130964437704643,\n",
       "               0.1413012829024133,\n",
       "               0.1412990966681225,\n",
       "               0.1413001065928621,\n",
       "               0.1412967970987309,\n",
       "               0.1412967750989941,\n",
       "               0.14129504703454443,\n",
       "               0.1412946356937097,\n",
       "               0.14129258051240134,\n",
       "               0.14129059960092724,\n",
       "               0.1412848249743339,\n",
       "               0.14128466619544164,\n",
       "               0.14128254634476312,\n",
       "               0.1412836482114419,\n",
       "               0.14128390733264062,\n",
       "               0.14128330202817227,\n",
       "               0.14128161916479465,\n",
       "               0.14128313583350016,\n",
       "               0.1412802605935267,\n",
       "               0.14128111301307322,\n",
       "               0.14128100561449025,\n",
       "               0.14127810296945267,\n",
       "               0.14127796915402568,\n",
       "               0.1412775856065105,\n",
       "               0.14127725974326685,\n",
       "               0.14127669077226337],\n",
       "              'test-Logloss-std': [0.0005837574209663514,\n",
       "               0.0002172022677762946,\n",
       "               0.0002969095093022844,\n",
       "               0.0002555372726815721,\n",
       "               0.00021283460587267903,\n",
       "               0.0004202418831078934,\n",
       "               0.00015571945343389128,\n",
       "               0.00029774229598745,\n",
       "               0.000255313187081981,\n",
       "               7.651501357620769e-05,\n",
       "               6.601226108440212e-05,\n",
       "               0.00015527991153946248,\n",
       "               9.569326022227246e-05,\n",
       "               0.00013837819247912122,\n",
       "               0.00017732068608738,\n",
       "               0.00014250898480069082,\n",
       "               8.814574624388235e-05,\n",
       "               7.139889431199906e-05,\n",
       "               1.607344990252169e-05,\n",
       "               2.866477445658262e-05,\n",
       "               5.356389392858374e-05,\n",
       "               8.259948087794028e-05,\n",
       "               7.444936079662396e-05,\n",
       "               7.77515981070996e-05,\n",
       "               5.640494310542395e-05,\n",
       "               7.643880984889307e-05,\n",
       "               7.105094830057104e-05,\n",
       "               7.496255602883405e-05,\n",
       "               0.00011145816238551958,\n",
       "               9.38104898945748e-05,\n",
       "               8.372438310563817e-05,\n",
       "               8.986669510752975e-05,\n",
       "               0.00012254943227647165,\n",
       "               0.00012187171824515203,\n",
       "               0.00013883099459294484,\n",
       "               0.00013211310021562464,\n",
       "               0.00014585429006628737,\n",
       "               0.00015549452387443895,\n",
       "               0.00013651174479778572,\n",
       "               0.00012415571057365965,\n",
       "               0.00014041173981286227,\n",
       "               0.00013620109597409572,\n",
       "               0.00011235786649752728,\n",
       "               0.00011653202054386406,\n",
       "               0.00010210442476872522,\n",
       "               0.00011273146632724944,\n",
       "               0.00011687462910787654,\n",
       "               0.00010945650691776421,\n",
       "               0.00012728800618267363,\n",
       "               0.00012925052853179211,\n",
       "               0.00013983451997903303,\n",
       "               0.0001523510697805438,\n",
       "               0.0001594896129564806,\n",
       "               0.00013648011382222085,\n",
       "               0.00012407535496673214,\n",
       "               0.00011141986347715864,\n",
       "               0.00011830915317039878,\n",
       "               0.0001259481216724747,\n",
       "               0.0001439758503337618,\n",
       "               0.00015771086775209515,\n",
       "               0.00014812792851109184,\n",
       "               0.00012962203553671252,\n",
       "               0.00015078396066287738,\n",
       "               0.0001423358562330602,\n",
       "               0.0001306194204554875,\n",
       "               0.00012721313461129983,\n",
       "               0.00013043100454917224,\n",
       "               0.00010932449382016015,\n",
       "               0.0001090088811735865,\n",
       "               0.00010094318949279182,\n",
       "               0.00010260780813623869,\n",
       "               9.950202183128705e-05,\n",
       "               0.00010097808467949662,\n",
       "               9.654888975527773e-05,\n",
       "               0.00010431894850519347,\n",
       "               0.0001017464208828689,\n",
       "               9.387029485048853e-05,\n",
       "               9.550527731257697e-05,\n",
       "               0.00010193501529597636,\n",
       "               0.00010983615484675611,\n",
       "               0.00011384966341538812,\n",
       "               0.0001230273342997177,\n",
       "               0.00012328518005062745,\n",
       "               0.00010278953997408501,\n",
       "               9.378491064992746e-05,\n",
       "               9.297002195327092e-05,\n",
       "               8.983828348207946e-05,\n",
       "               9.493597249738022e-05,\n",
       "               8.726891692369035e-05,\n",
       "               8.7130003331294e-05,\n",
       "               9.427663044962366e-05,\n",
       "               7.995372419299508e-05,\n",
       "               8.324923770092814e-05,\n",
       "               7.775548428555403e-05,\n",
       "               7.583759418347922e-05,\n",
       "               7.289448918584842e-05,\n",
       "               7.089323722283244e-05,\n",
       "               6.264991983508518e-05,\n",
       "               6.407420304045691e-05,\n",
       "               6.463002371439278e-05,\n",
       "               5.7335849856998854e-05,\n",
       "               6.246125505097976e-05,\n",
       "               5.979969665840396e-05,\n",
       "               6.046755540919396e-05,\n",
       "               5.980628880092118e-05,\n",
       "               6.610711467160652e-05,\n",
       "               7.404103454735078e-05,\n",
       "               7.557026894425236e-05,\n",
       "               8.150631755721151e-05,\n",
       "               7.22909314737312e-05,\n",
       "               7.319811159014766e-05,\n",
       "               7.563359492100841e-05,\n",
       "               6.068805251699489e-05,\n",
       "               5.890877355035975e-05,\n",
       "               4.38688628517993e-05,\n",
       "               4.427185538906972e-05,\n",
       "               4.487312522128361e-05,\n",
       "               3.4624741940115545e-05,\n",
       "               3.259873988833714e-05,\n",
       "               3.7310097541288e-05,\n",
       "               3.811389415132559e-05,\n",
       "               4.411542118701477e-05,\n",
       "               4.9759621052660846e-05,\n",
       "               4.981362418877432e-05,\n",
       "               4.71560938041917e-05,\n",
       "               4.879135435237901e-05,\n",
       "               4.798959782481898e-05,\n",
       "               4.6648647541788485e-05,\n",
       "               5.0882543414887834e-05,\n",
       "               5.205288624310396e-05,\n",
       "               4.891886052195218e-05,\n",
       "               4.8611591989005545e-05,\n",
       "               4.9077509982445886e-05,\n",
       "               4.737288865673105e-05,\n",
       "               4.779436275417745e-05,\n",
       "               4.5352558244441845e-05,\n",
       "               4.648221319766166e-05,\n",
       "               4.377061058012832e-05,\n",
       "               5.118881145351792e-05,\n",
       "               7.594619011424548e-05,\n",
       "               7.849900553625912e-05,\n",
       "               7.949688173696476e-05,\n",
       "               7.972078999701114e-05,\n",
       "               8.311798386173363e-05,\n",
       "               7.79851652020845e-05,\n",
       "               7.98279527995231e-05,\n",
       "               7.818287299813165e-05,\n",
       "               8.494478098622199e-05,\n",
       "               8.578208589125797e-05,\n",
       "               8.590563320240281e-05,\n",
       "               8.454305358862779e-05,\n",
       "               8.616326299728145e-05,\n",
       "               8.33815643417997e-05,\n",
       "               8.381480126212499e-05,\n",
       "               8.817837957241318e-05,\n",
       "               8.455625796519155e-05,\n",
       "               8.696261266478662e-05,\n",
       "               8.961305868389429e-05,\n",
       "               8.57325047236974e-05,\n",
       "               8.362210604100771e-05,\n",
       "               8.892350729529429e-05,\n",
       "               8.844021293350916e-05,\n",
       "               8.862459583290053e-05,\n",
       "               8.899522253085028e-05,\n",
       "               8.992056784820182e-05,\n",
       "               8.915130025969569e-05,\n",
       "               8.925661120271388e-05,\n",
       "               8.804937829189448e-05,\n",
       "               8.870027412321713e-05,\n",
       "               9.095366683031001e-05,\n",
       "               9.165466389866536e-05,\n",
       "               9.289472580313485e-05,\n",
       "               9.52925465615794e-05,\n",
       "               0.00010282733124660046,\n",
       "               0.00011196968165069219,\n",
       "               0.0001191153698187999,\n",
       "               0.00012134300142199919,\n",
       "               0.00012053436089811714,\n",
       "               0.00012762531319839942,\n",
       "               0.0001259248579265282,\n",
       "               0.0001302077458216833,\n",
       "               0.00013105892259699574,\n",
       "               0.00013410723834397532,\n",
       "               0.00013876626870233406,\n",
       "               0.00014404384456570665,\n",
       "               0.00014480416500771662,\n",
       "               0.0001472706684137815,\n",
       "               0.0001452581528960754,\n",
       "               0.0001443353205326422,\n",
       "               0.00014390817413346057,\n",
       "               0.00014502682958959478,\n",
       "               0.00014262283077907367,\n",
       "               0.0001414876946954318,\n",
       "               0.00013962905829049602,\n",
       "               0.00013851266644080633,\n",
       "               0.00013938581278184973,\n",
       "               0.0001385702005323355,\n",
       "               0.00013838900160632086,\n",
       "               0.00013823319820230331,\n",
       "               0.00013797027325572947],\n",
       "              'train-Logloss-mean': [0.35176721840712855,\n",
       "               0.23018842093806988,\n",
       "               0.1838362830940481,\n",
       "               0.1637560661806244,\n",
       "               0.15521621418876286,\n",
       "               0.15093659878292745,\n",
       "               0.1488188462995693,\n",
       "               0.1476920105594722,\n",
       "               0.14671708789146032,\n",
       "               0.14605019227412705,\n",
       "               0.14556231998984911,\n",
       "               0.1452189806667257,\n",
       "               0.14483764258838752,\n",
       "               0.14458950136337978,\n",
       "               0.1443172819396642,\n",
       "               0.14410078484716118,\n",
       "               0.14390084996005137,\n",
       "               0.14370604154415495,\n",
       "               0.14353251937575573,\n",
       "               0.14340012470374497,\n",
       "               0.14322025600933078,\n",
       "               0.14310482505853225,\n",
       "               0.14297094016168024,\n",
       "               0.14287329100760862,\n",
       "               0.14275130106845982,\n",
       "               0.14262723412591133,\n",
       "               0.1425432461616484,\n",
       "               0.14248055831343437,\n",
       "               0.1423648128953731,\n",
       "               0.1422773597415666,\n",
       "               0.14216179724717595,\n",
       "               0.14209819480826724,\n",
       "               0.1420227139142685,\n",
       "               0.14193682403856936,\n",
       "               0.14186934286402508,\n",
       "               0.1418026651434272,\n",
       "               0.14174519012985098,\n",
       "               0.14167588021524805,\n",
       "               0.14163163263175252,\n",
       "               0.1415760647990761,\n",
       "               0.14152758080569702,\n",
       "               0.14148548714575404,\n",
       "               0.14139658523078957,\n",
       "               0.14134049592515532,\n",
       "               0.1412918696079497,\n",
       "               0.1412541544053633,\n",
       "               0.1412048862507497,\n",
       "               0.1411464863801517,\n",
       "               0.1411136668722227,\n",
       "               0.14109406663470267,\n",
       "               0.14104309364786013,\n",
       "               0.1409899621362947,\n",
       "               0.14094532052216216,\n",
       "               0.14089379790577983,\n",
       "               0.14084314509024884,\n",
       "               0.14081988885124908,\n",
       "               0.14076198231224032,\n",
       "               0.14071287725196743,\n",
       "               0.14067329311589327,\n",
       "               0.1406303483791496,\n",
       "               0.14056224316823054,\n",
       "               0.14051547619762605,\n",
       "               0.1404605995826689,\n",
       "               0.14042185860137638,\n",
       "               0.14038286198638272,\n",
       "               0.14034686979815425,\n",
       "               0.14031191238563992,\n",
       "               0.14027631371489327,\n",
       "               0.14025295150255293,\n",
       "               0.14021773805975285,\n",
       "               0.14018570402470198,\n",
       "               0.140149593603721,\n",
       "               0.14009985433640054,\n",
       "               0.14006119981030782,\n",
       "               0.14003182976801853,\n",
       "               0.140000544512758,\n",
       "               0.139964987252702,\n",
       "               0.13992762671854178,\n",
       "               0.13989929046056523,\n",
       "               0.13986530571484754,\n",
       "               0.13984549887885372,\n",
       "               0.13980110006972193,\n",
       "               0.13976558407486048,\n",
       "               0.13971955001567735,\n",
       "               0.139691095773846,\n",
       "               0.13965934458884802,\n",
       "               0.13962680430399296,\n",
       "               0.13959602711289984,\n",
       "               0.13956875061736038,\n",
       "               0.13955078360140794,\n",
       "               0.1395212324693135,\n",
       "               0.1394914874166521,\n",
       "               0.1394671521363924,\n",
       "               0.13944711552922892,\n",
       "               0.13942401330689383,\n",
       "               0.13938733043924798,\n",
       "               0.13936795548748163,\n",
       "               0.13932981536095262,\n",
       "               0.13930586524010427,\n",
       "               0.13928601266442842,\n",
       "               0.1392610478298087,\n",
       "               0.13923770436125366,\n",
       "               0.13921257494781888,\n",
       "               0.13919281800977,\n",
       "               0.13916664335970874,\n",
       "               0.13914009683618428,\n",
       "               0.13911115037410074,\n",
       "               0.13908892379008056,\n",
       "               0.1390572420808804,\n",
       "               0.13902560863172353,\n",
       "               0.13901676431024837,\n",
       "               0.13899053672650707,\n",
       "               0.13896310717817936,\n",
       "               0.13893024044937322,\n",
       "               0.1388802017174836,\n",
       "               0.1388501502047339,\n",
       "               0.13881533933858872,\n",
       "               0.13878572017969776,\n",
       "               0.13877051537350063,\n",
       "               0.13874405253161692,\n",
       "               0.13873303992089822,\n",
       "               0.13871209658056027,\n",
       "               0.13868508877185384,\n",
       "               0.13866600846156332,\n",
       "               0.1386463450327977,\n",
       "               0.13862644474617528,\n",
       "               0.13860084722938792,\n",
       "               0.138577370653178,\n",
       "               0.1385567784350801,\n",
       "               0.1385466444498232,\n",
       "               0.13852269145485283,\n",
       "               0.1385029150149172,\n",
       "               0.13848510504995346,\n",
       "               0.13846949528800026,\n",
       "               0.1384584285571925,\n",
       "               0.13844391243212137,\n",
       "               0.13842507419017588,\n",
       "               0.13840730331161372,\n",
       "               0.13838153326715827,\n",
       "               0.1383463701971577,\n",
       "               0.1383259651487725,\n",
       "               0.13830732996701875,\n",
       "               0.13828988481385085,\n",
       "               0.13827020096039208,\n",
       "               0.1382551670503943,\n",
       "               0.1382335031033536,\n",
       "               0.13821220569502998,\n",
       "               0.1381878223719961,\n",
       "               0.13817164396648865,\n",
       "               0.1381550512735098,\n",
       "               0.13813050478242536,\n",
       "               0.13810830184237036,\n",
       "               0.1380880724420219,\n",
       "               0.1380751897656275,\n",
       "               0.13805856440065714,\n",
       "               0.1380311861421408,\n",
       "               0.13800259229750325,\n",
       "               0.13798666800783746,\n",
       "               0.13796589764485132,\n",
       "               0.13794644316558324,\n",
       "               0.13792268394644283,\n",
       "               0.13790889415935423,\n",
       "               0.1378919428130947,\n",
       "               0.1378868830103335,\n",
       "               0.13787497480088065,\n",
       "               0.13786192435724057,\n",
       "               0.13784064026862777,\n",
       "               0.1378169141196284,\n",
       "               0.13779313403588592,\n",
       "               0.13776636564750858,\n",
       "               0.13774503746505687,\n",
       "               0.1377277387857395,\n",
       "               0.13770511218496997,\n",
       "               0.13768097015880446,\n",
       "               0.13765684393063704,\n",
       "               0.1376279556886996,\n",
       "               0.13760999495604478,\n",
       "               0.13759731793082552,\n",
       "               0.1375712740771714,\n",
       "               0.13755815944580205,\n",
       "               0.1375484091235318,\n",
       "               0.1375426207381006,\n",
       "               0.13753019131851948,\n",
       "               0.13750704657677051,\n",
       "               0.13749691127694194,\n",
       "               0.13749264866577232,\n",
       "               0.1374833582061873,\n",
       "               0.13747181164986075,\n",
       "               0.13746590979443882,\n",
       "               0.13745502502730517,\n",
       "               0.1374432703113095,\n",
       "               0.13742796269885318,\n",
       "               0.13741876916890677,\n",
       "               0.1374118154860651,\n",
       "               0.137396334060938,\n",
       "               0.13738451172119714,\n",
       "               0.1373745758659598,\n",
       "               0.13736175204492085,\n",
       "               0.13735508492162815,\n",
       "               0.13734589424327126],\n",
       "              'train-Logloss-std': [0.0005285859497856227,\n",
       "               0.0001724951039144888,\n",
       "               0.00025956986131828104,\n",
       "               0.00022496022328593243,\n",
       "               0.00017580056977787765,\n",
       "               0.000404254940537632,\n",
       "               0.00010263781081431873,\n",
       "               0.0002396043237037392,\n",
       "               0.00018537336205053955,\n",
       "               3.078236545027917e-05,\n",
       "               6.054536488274788e-05,\n",
       "               0.00018359831556467136,\n",
       "               0.00012219361960540188,\n",
       "               0.00017086153499480818,\n",
       "               0.00020493159309817086,\n",
       "               0.00021026966137538161,\n",
       "               0.00018015468961708593,\n",
       "               0.00012592144578431837,\n",
       "               0.00010886510033230686,\n",
       "               0.00013486777613450692,\n",
       "               0.0001517120456242865,\n",
       "               0.00017043539010006407,\n",
       "               0.00014277172835272417,\n",
       "               0.00010586724348178574,\n",
       "               0.00012855019253002226,\n",
       "               0.00013250272593265047,\n",
       "               0.00014365708490960213,\n",
       "               0.00014764553679245032,\n",
       "               0.00015706758987282682,\n",
       "               0.00016343054351362194,\n",
       "               0.0001250883894274553,\n",
       "               0.00012314908357763914,\n",
       "               0.00011373022629257952,\n",
       "               8.720021132652935e-05,\n",
       "               9.512227837272311e-05,\n",
       "               0.00010298573060979864,\n",
       "               0.00010210478354556449,\n",
       "               0.00011165049279775778,\n",
       "               9.919094220391805e-05,\n",
       "               0.00011476459250603898,\n",
       "               0.00010429643747693835,\n",
       "               0.00010747334385549215,\n",
       "               0.00010872582070796802,\n",
       "               0.00014759805544408986,\n",
       "               0.00013835707583457728,\n",
       "               0.00014369213401962274,\n",
       "               0.0001531440216473359,\n",
       "               0.00013794673515662837,\n",
       "               0.00015988839949567516,\n",
       "               0.00017167276068744556,\n",
       "               0.00016570646891429455,\n",
       "               0.0001687709561524094,\n",
       "               0.0001703388236619904,\n",
       "               0.00014801970688812496,\n",
       "               0.00014255119325574781,\n",
       "               0.00013368997163518083,\n",
       "               0.00014699790508239093,\n",
       "               0.0001583273981988634,\n",
       "               0.00014421152377796647,\n",
       "               0.0001414881304748178,\n",
       "               0.0001475405174773958,\n",
       "               0.00014011290550204307,\n",
       "               0.00016302141963962888,\n",
       "               0.00016506028837294404,\n",
       "               0.00015960354969256945,\n",
       "               0.00015837030470253264,\n",
       "               0.00016129532735844248,\n",
       "               0.00017576909584325756,\n",
       "               0.00015749762776877315,\n",
       "               0.00017057821534079948,\n",
       "               0.00017862188807734952,\n",
       "               0.00018632728693156094,\n",
       "               0.00018295208480445294,\n",
       "               0.00017851389874293152,\n",
       "               0.00018423079033022812,\n",
       "               0.00017470720181221545,\n",
       "               0.0001901626697785432,\n",
       "               0.00017640376277095471,\n",
       "               0.00017515635672423017,\n",
       "               0.0001675944834156824,\n",
       "               0.00018357587697156706,\n",
       "               0.00017683939379386005,\n",
       "               0.0001836948618113667,\n",
       "               0.00019138473447048303,\n",
       "               0.00017907168233227555,\n",
       "               0.00019789970337782947,\n",
       "               0.00018435570859666516,\n",
       "               0.00018607527514636314,\n",
       "               0.00018633498226311394,\n",
       "               0.0001758855477547297,\n",
       "               0.0001794105903620079,\n",
       "               0.00019684341721436135,\n",
       "               0.00019852254585758577,\n",
       "               0.0002030731157898976,\n",
       "               0.0001954318704669913,\n",
       "               0.0001984955201909476,\n",
       "               0.00021185374080156226,\n",
       "               0.00020923688937042048,\n",
       "               0.00020728690377439876,\n",
       "               0.0002181194354089805,\n",
       "               0.00023324779019839813,\n",
       "               0.00023041861747999338,\n",
       "               0.00022630355483092837,\n",
       "               0.00021378628281811104,\n",
       "               0.00020316378328769137,\n",
       "               0.0002087339823524504,\n",
       "               0.0002054782555150741,\n",
       "               0.00020174810729570747,\n",
       "               0.00018284297622297448,\n",
       "               0.00018156079277095215,\n",
       "               0.0001852505014592404,\n",
       "               0.00018831531482320075,\n",
       "               0.0001895226257879383,\n",
       "               0.00018574296058357015,\n",
       "               0.00020940967566002462,\n",
       "               0.0002060273670065533,\n",
       "               0.00020890962798296775,\n",
       "               0.00021763963831431806,\n",
       "               0.00021621835775528322,\n",
       "               0.0002152636010268415,\n",
       "               0.00020973500285833568,\n",
       "               0.0002069539105045894,\n",
       "               0.000204181371300185,\n",
       "               0.000212509182965831,\n",
       "               0.0002220682723092787,\n",
       "               0.0002218799302961182,\n",
       "               0.0002344005912472272,\n",
       "               0.00023016101737716052,\n",
       "               0.0002211406087114977,\n",
       "               0.0002254788764410636,\n",
       "               0.0002240109546059361,\n",
       "               0.00023209509987769312,\n",
       "               0.0002344647253573436,\n",
       "               0.0002319471504334184,\n",
       "               0.000238856316368723,\n",
       "               0.0002454106374958728,\n",
       "               0.00024529926536625993,\n",
       "               0.00022927148652566196,\n",
       "               0.00023205889954256708,\n",
       "               0.00019884828728649337,\n",
       "               0.00019092767080716974,\n",
       "               0.0001886237265973517,\n",
       "               0.00019387594709236407,\n",
       "               0.00019519344917595232,\n",
       "               0.00020405856922089215,\n",
       "               0.00019682226767867082,\n",
       "               0.00020467508591132303,\n",
       "               0.00019663791117165762,\n",
       "               0.00019474684810997917,\n",
       "               0.00018947796872728422,\n",
       "               0.00020531358145854428,\n",
       "               0.00019619821151415232,\n",
       "               0.00020301899479277,\n",
       "               0.00021297488576562632,\n",
       "               0.00020380194652188142,\n",
       "               0.0002174354205637758,\n",
       "               0.00020784304602385712,\n",
       "               0.00019932892111356495,\n",
       "               0.00020144450209196764,\n",
       "               0.00020832523764318948,\n",
       "               0.0002176971013620358,\n",
       "               0.00021110341340097887,\n",
       "               0.00021148644755659637,\n",
       "               0.00020928166214866553,\n",
       "               0.0001974080143002658,\n",
       "               0.0002025729334612479,\n",
       "               0.000209322297519319,\n",
       "               0.00021196267951857837,\n",
       "               0.0002159513316942829,\n",
       "               0.00019919760231606662,\n",
       "               0.0001965326132326443,\n",
       "               0.00020401979417468709,\n",
       "               0.0001977303475661469,\n",
       "               0.00019648439620748983,\n",
       "               0.0001786854586575178,\n",
       "               0.00017762549001625177,\n",
       "               0.00017113203497562922,\n",
       "               0.00017897356096562895,\n",
       "               0.00017204671894432164,\n",
       "               0.0001728103842133515,\n",
       "               0.00016827360301073754,\n",
       "               0.0001599470137579489,\n",
       "               0.00014074053987737543,\n",
       "               0.00012075480184389592,\n",
       "               0.0001179179718607274,\n",
       "               0.00011241224121733347,\n",
       "               0.0001098845986741196,\n",
       "               0.0001038341059744456,\n",
       "               9.874429818183713e-05,\n",
       "               9.884504529195414e-05,\n",
       "               9.552246400539639e-05,\n",
       "               9.185434477057187e-05,\n",
       "               0.00010214981000028083,\n",
       "               9.471013252230616e-05,\n",
       "               0.00010220054873369587,\n",
       "               0.00010819572680575705,\n",
       "               0.00011406853932196002,\n",
       "               0.00012411586485373656,\n",
       "               0.00012553951198232088,\n",
       "               0.00013564530033628817]})}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20:\ttest: 0.6941101\tbest: 0.6941101 (20)\ttotal: 5.75s\tremaining: 49s\n",
      "40:\ttest: 0.7078560\tbest: 0.7078560 (40)\ttotal: 11.6s\tremaining: 45.1s\n",
      "60:\ttest: 0.7169596\tbest: 0.7169596 (60)\ttotal: 17.3s\tremaining: 39.5s\n",
      "80:\ttest: 0.7235050\tbest: 0.7235050 (80)\ttotal: 23.2s\tremaining: 34s\n",
      "100:\ttest: 0.7275101\tbest: 0.7275101 (100)\ttotal: 29s\tremaining: 28.4s\n",
      "120:\ttest: 0.7309723\tbest: 0.7309723 (120)\ttotal: 34.8s\tremaining: 22.7s\n",
      "140:\ttest: 0.7341186\tbest: 0.7341186 (140)\ttotal: 41.1s\tremaining: 17.2s\n",
      "160:\ttest: 0.7363189\tbest: 0.7363189 (160)\ttotal: 46.9s\tremaining: 11.4s\n",
      "180:\ttest: 0.7382495\tbest: 0.7382495 (180)\ttotal: 53s\tremaining: 5.56s\n",
      "199:\ttest: 0.7393601\tbest: 0.7393601 (199)\ttotal: 59s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7393601424\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "16:\tloss: 0.7393601\tbest: 0.7501471 (14)\ttotal: 19m 40s\tremaining: 17m 21s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 223ms\tremaining: 44.4s\n",
      "20:\ttest: 0.7110566\tbest: 0.7110566 (20)\ttotal: 6.85s\tremaining: 58.4s\n",
      "40:\ttest: 0.7257202\tbest: 0.7257202 (40)\ttotal: 13.1s\tremaining: 50.6s\n",
      "60:\ttest: 0.7332879\tbest: 0.7332879 (60)\ttotal: 19.6s\tremaining: 44.7s\n",
      "80:\ttest: 0.7373864\tbest: 0.7373864 (80)\ttotal: 26.2s\tremaining: 38.5s\n",
      "100:\ttest: 0.7404107\tbest: 0.7404107 (100)\ttotal: 32.3s\tremaining: 31.6s\n",
      "120:\ttest: 0.7423518\tbest: 0.7423518 (120)\ttotal: 38.2s\tremaining: 25s\n",
      "140:\ttest: 0.7433657\tbest: 0.7433657 (140)\ttotal: 44.1s\tremaining: 18.5s\n",
      "160:\ttest: 0.7448501\tbest: 0.7448501 (160)\ttotal: 50.4s\tremaining: 12.2s\n",
      "180:\ttest: 0.7455125\tbest: 0.7455128 (179)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7459531\tbest: 0.7459531 (199)\ttotal: 1m 1s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7459531324\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "17:\tloss: 0.7459531\tbest: 0.7501471 (14)\ttotal: 20m 41s\tremaining: 16m 5s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7185768\tbest: 0.7185768 (20)\ttotal: 6.28s\tremaining: 53.5s\n",
      "40:\ttest: 0.7308254\tbest: 0.7308254 (40)\ttotal: 12.4s\tremaining: 48.1s\n",
      "60:\ttest: 0.7380665\tbest: 0.7380665 (60)\ttotal: 18.7s\tremaining: 42.6s\n",
      "80:\ttest: 0.7411564\tbest: 0.7411564 (80)\ttotal: 25.1s\tremaining: 36.9s\n",
      "100:\ttest: 0.7432658\tbest: 0.7432658 (100)\ttotal: 31.1s\tremaining: 30.5s\n",
      "120:\ttest: 0.7446442\tbest: 0.7446442 (120)\ttotal: 36.9s\tremaining: 24.1s\n",
      "140:\ttest: 0.7451662\tbest: 0.7451662 (140)\ttotal: 42.8s\tremaining: 17.9s\n",
      "160:\ttest: 0.7462863\tbest: 0.7463030 (158)\ttotal: 49.5s\tremaining: 12s\n",
      "180:\ttest: 0.7470859\tbest: 0.7470859 (180)\ttotal: 56.2s\tremaining: 5.9s\n",
      "199:\ttest: 0.7474472\tbest: 0.7474523 (197)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7474523159\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "18:\tloss: 0.7474523\tbest: 0.7501471 (14)\ttotal: 21m 44s\tremaining: 14m 52s\n",
      "0:\ttest: 0.5053388\tbest: 0.5053388 (0)\ttotal: 252ms\tremaining: 50.2s\n",
      "20:\ttest: 0.7249919\tbest: 0.7249919 (20)\ttotal: 6.6s\tremaining: 56.3s\n",
      "40:\ttest: 0.7353034\tbest: 0.7353034 (40)\ttotal: 12.8s\tremaining: 49.7s\n",
      "60:\ttest: 0.7392334\tbest: 0.7392334 (60)\ttotal: 18.3s\tremaining: 41.7s\n",
      "80:\ttest: 0.7430480\tbest: 0.7430480 (80)\ttotal: 24.3s\tremaining: 35.8s\n",
      "100:\ttest: 0.7444092\tbest: 0.7444092 (100)\ttotal: 29.9s\tremaining: 29.3s\n",
      "120:\ttest: 0.7458438\tbest: 0.7458438 (120)\ttotal: 35.9s\tremaining: 23.4s\n",
      "140:\ttest: 0.7468424\tbest: 0.7468424 (140)\ttotal: 42s\tremaining: 17.6s\n",
      "160:\ttest: 0.7474859\tbest: 0.7474935 (155)\ttotal: 47.7s\tremaining: 11.6s\n",
      "180:\ttest: 0.7480808\tbest: 0.7480857 (178)\ttotal: 53.5s\tremaining: 5.61s\n",
      "199:\ttest: 0.7482791\tbest: 0.7483154 (197)\ttotal: 58.8s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7483154481\n",
      "bestIteration = 197\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "19:\tloss: 0.7483154\tbest: 0.7501471 (14)\ttotal: 22m 43s\tremaining: 13m 38s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7008331\tbest: 0.7008331 (20)\ttotal: 6.51s\tremaining: 55.5s\n",
      "40:\ttest: 0.7134338\tbest: 0.7134338 (40)\ttotal: 13.6s\tremaining: 52.6s\n",
      "60:\ttest: 0.7215962\tbest: 0.7215962 (60)\ttotal: 20s\tremaining: 45.5s\n",
      "80:\ttest: 0.7282169\tbest: 0.7282169 (80)\ttotal: 26.7s\tremaining: 39.2s\n",
      "100:\ttest: 0.7326643\tbest: 0.7326643 (100)\ttotal: 34s\tremaining: 33.3s\n",
      "120:\ttest: 0.7352217\tbest: 0.7352217 (120)\ttotal: 40.8s\tremaining: 26.6s\n",
      "140:\ttest: 0.7377332\tbest: 0.7377332 (140)\ttotal: 47.7s\tremaining: 20s\n",
      "160:\ttest: 0.7396521\tbest: 0.7396521 (160)\ttotal: 54.4s\tremaining: 13.2s\n",
      "180:\ttest: 0.7410868\tbest: 0.7410868 (180)\ttotal: 1m 1s\tremaining: 6.46s\n",
      "199:\ttest: 0.7421636\tbest: 0.7421636 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7421636392\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "20:\tloss: 0.7421636\tbest: 0.7501471 (14)\ttotal: 23m 51s\tremaining: 12m 29s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 233ms\tremaining: 46.3s\n",
      "20:\ttest: 0.7153431\tbest: 0.7153431 (20)\ttotal: 7.19s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7301921\tbest: 0.7301921 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7363345\tbest: 0.7363345 (60)\ttotal: 21.5s\tremaining: 49s\n",
      "80:\ttest: 0.7401659\tbest: 0.7401856 (79)\ttotal: 28.3s\tremaining: 41.6s\n",
      "100:\ttest: 0.7423290\tbest: 0.7423290 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7440772\tbest: 0.7440772 (120)\ttotal: 41.4s\tremaining: 27s\n",
      "140:\ttest: 0.7451441\tbest: 0.7451441 (140)\ttotal: 48.1s\tremaining: 20.1s\n",
      "160:\ttest: 0.7460738\tbest: 0.7460738 (160)\ttotal: 54.6s\tremaining: 13.2s\n",
      "180:\ttest: 0.7466708\tbest: 0.7466708 (180)\ttotal: 1m 1s\tremaining: 6.42s\n",
      "199:\ttest: 0.7470675\tbest: 0.7470706 (198)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7470706135\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "21:\tloss: 0.7470706\tbest: 0.7501471 (14)\ttotal: 24m 58s\tremaining: 11m 21s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 261ms\tremaining: 52s\n",
      "20:\ttest: 0.7225821\tbest: 0.7225821 (20)\ttotal: 6.78s\tremaining: 57.8s\n",
      "40:\ttest: 0.7353872\tbest: 0.7353872 (40)\ttotal: 13.5s\tremaining: 52.5s\n",
      "60:\ttest: 0.7398937\tbest: 0.7398937 (60)\ttotal: 20.3s\tremaining: 46.3s\n",
      "80:\ttest: 0.7433685\tbest: 0.7433778 (78)\ttotal: 27.1s\tremaining: 39.9s\n",
      "100:\ttest: 0.7453004\tbest: 0.7453340 (99)\ttotal: 33.6s\tremaining: 32.9s\n",
      "120:\ttest: 0.7468075\tbest: 0.7468075 (120)\ttotal: 40.1s\tremaining: 26.2s\n",
      "140:\ttest: 0.7474972\tbest: 0.7475054 (139)\ttotal: 46.5s\tremaining: 19.4s\n",
      "160:\ttest: 0.7479784\tbest: 0.7479784 (160)\ttotal: 52.9s\tremaining: 12.8s\n",
      "180:\ttest: 0.7484241\tbest: 0.7484526 (179)\ttotal: 59.3s\tremaining: 6.22s\n",
      "199:\ttest: 0.7487468\tbest: 0.7487472 (198)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7487472057\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "22:\tloss: 0.7487472\tbest: 0.7501471 (14)\ttotal: 26m 3s\tremaining: 10m 11s\n",
      "0:\ttest: 0.5053672\tbest: 0.5053672 (0)\ttotal: 240ms\tremaining: 47.8s\n",
      "20:\ttest: 0.7272551\tbest: 0.7272551 (20)\ttotal: 7.2s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7386984\tbest: 0.7386984 (40)\ttotal: 14.6s\tremaining: 56.8s\n",
      "60:\ttest: 0.7419438\tbest: 0.7419438 (60)\ttotal: 22.5s\tremaining: 51.3s\n",
      "80:\ttest: 0.7448588\tbest: 0.7448588 (80)\ttotal: 29.3s\tremaining: 43s\n",
      "100:\ttest: 0.7466908\tbest: 0.7466908 (100)\ttotal: 36.4s\tremaining: 35.7s\n",
      "120:\ttest: 0.7474343\tbest: 0.7474343 (120)\ttotal: 42.5s\tremaining: 27.8s\n",
      "140:\ttest: 0.7480092\tbest: 0.7480092 (140)\ttotal: 49.1s\tremaining: 20.6s\n",
      "160:\ttest: 0.7484148\tbest: 0.7484318 (159)\ttotal: 55.3s\tremaining: 13.4s\n",
      "180:\ttest: 0.7489792\tbest: 0.7489792 (180)\ttotal: 1m 1s\tremaining: 6.48s\n",
      "199:\ttest: 0.7494230\tbest: 0.7494230 (199)\ttotal: 1m 7s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7494229948\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "23:\tloss: 0.7494230\tbest: 0.7501471 (14)\ttotal: 27m 12s\tremaining: 9m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 237ms\tremaining: 47.1s\n",
      "20:\ttest: 0.7034370\tbest: 0.7034370 (20)\ttotal: 6.84s\tremaining: 58.3s\n",
      "40:\ttest: 0.7186187\tbest: 0.7186187 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7279600\tbest: 0.7279600 (60)\ttotal: 22.1s\tremaining: 50.5s\n",
      "80:\ttest: 0.7331008\tbest: 0.7331008 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7367479\tbest: 0.7367479 (100)\ttotal: 40.7s\tremaining: 39.9s\n",
      "120:\ttest: 0.7396022\tbest: 0.7396022 (120)\ttotal: 48.3s\tremaining: 31.6s\n",
      "140:\ttest: 0.7414634\tbest: 0.7414634 (140)\ttotal: 56.3s\tremaining: 23.6s\n",
      "160:\ttest: 0.7429885\tbest: 0.7429885 (160)\ttotal: 1m 3s\tremaining: 15.5s\n",
      "180:\ttest: 0.7442519\tbest: 0.7442519 (180)\ttotal: 1m 11s\tremaining: 7.5s\n",
      "199:\ttest: 0.7451130\tbest: 0.7451130 (199)\ttotal: 1m 18s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7451130219\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "24:\tloss: 0.7451130\tbest: 0.7501471 (14)\ttotal: 28m 30s\tremaining: 7m 59s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 243ms\tremaining: 48.3s\n",
      "20:\ttest: 0.7183954\tbest: 0.7183954 (20)\ttotal: 8.34s\tremaining: 1m 11s\n",
      "40:\ttest: 0.7324296\tbest: 0.7324296 (40)\ttotal: 16s\tremaining: 1m 2s\n",
      "60:\ttest: 0.7379777\tbest: 0.7379777 (60)\ttotal: 24.1s\tremaining: 55s\n",
      "80:\ttest: 0.7417054\tbest: 0.7417054 (80)\ttotal: 32.2s\tremaining: 47.3s\n",
      "100:\ttest: 0.7441019\tbest: 0.7441019 (100)\ttotal: 39.6s\tremaining: 38.8s\n",
      "120:\ttest: 0.7451301\tbest: 0.7451361 (119)\ttotal: 46.6s\tremaining: 30.4s\n",
      "140:\ttest: 0.7464575\tbest: 0.7464575 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7474590\tbest: 0.7474590 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7476996\tbest: 0.7476996 (180)\ttotal: 1m 8s\tremaining: 7.14s\n",
      "199:\ttest: 0.7485463\tbest: 0.7485519 (198)\ttotal: 1m 14s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7485518556\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "25:\tloss: 0.7485519\tbest: 0.7501471 (14)\ttotal: 29m 45s\tremaining: 6m 52s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 228ms\tremaining: 45.3s\n",
      "20:\ttest: 0.7266890\tbest: 0.7266890 (20)\ttotal: 7.23s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7379538\tbest: 0.7379538 (40)\ttotal: 14.4s\tremaining: 56s\n",
      "60:\ttest: 0.7432693\tbest: 0.7432693 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7452448\tbest: 0.7452633 (79)\ttotal: 28.1s\tremaining: 41.3s\n",
      "100:\ttest: 0.7466005\tbest: 0.7466005 (100)\ttotal: 34.5s\tremaining: 33.8s\n",
      "120:\ttest: 0.7477099\tbest: 0.7477099 (120)\ttotal: 41.2s\tremaining: 26.9s\n",
      "140:\ttest: 0.7484400\tbest: 0.7484400 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7486040\tbest: 0.7486128 (151)\ttotal: 53.6s\tremaining: 13s\n",
      "180:\ttest: 0.7489293\tbest: 0.7489661 (177)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7497762\tbest: 0.7497762 (199)\ttotal: 1m 6s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.749776223\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "26:\tloss: 0.7497762\tbest: 0.7501471 (14)\ttotal: 30m 51s\tremaining: 5m 42s\n",
      "0:\ttest: 0.5579289\tbest: 0.5579289 (0)\ttotal: 221ms\tremaining: 44s\n",
      "20:\ttest: 0.7329100\tbest: 0.7329100 (20)\ttotal: 7.42s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7403687\tbest: 0.7403687 (40)\ttotal: 14.5s\tremaining: 56.3s\n",
      "60:\ttest: 0.7442135\tbest: 0.7442135 (60)\ttotal: 21.4s\tremaining: 48.7s\n",
      "80:\ttest: 0.7458341\tbest: 0.7458341 (80)\ttotal: 27.7s\tremaining: 40.7s\n",
      "100:\ttest: 0.7469134\tbest: 0.7469134 (100)\ttotal: 34.4s\tremaining: 33.7s\n",
      "120:\ttest: 0.7477579\tbest: 0.7477579 (120)\ttotal: 40.9s\tremaining: 26.7s\n",
      "140:\ttest: 0.7482154\tbest: 0.7482154 (140)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7487303\tbest: 0.7487768 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "180:\ttest: 0.7493876\tbest: 0.7493876 (180)\ttotal: 59.7s\tremaining: 6.27s\n",
      "199:\ttest: 0.7495236\tbest: 0.7495931 (195)\ttotal: 1m 5s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7495930754\n",
      "bestIteration = 195\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "27:\tloss: 0.7495931\tbest: 0.7501471 (14)\ttotal: 31m 57s\tremaining: 4m 33s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 283ms\tremaining: 56.2s\n",
      "20:\ttest: 0.7059916\tbest: 0.7059916 (20)\ttotal: 6.93s\tremaining: 59s\n",
      "40:\ttest: 0.7231107\tbest: 0.7231107 (40)\ttotal: 14.9s\tremaining: 57.7s\n",
      "60:\ttest: 0.7307839\tbest: 0.7307839 (60)\ttotal: 22.6s\tremaining: 51.4s\n",
      "80:\ttest: 0.7353967\tbest: 0.7353967 (80)\ttotal: 30.3s\tremaining: 44.5s\n",
      "100:\ttest: 0.7386240\tbest: 0.7386240 (100)\ttotal: 38.2s\tremaining: 37.5s\n",
      "120:\ttest: 0.7410789\tbest: 0.7410789 (120)\ttotal: 46.2s\tremaining: 30.2s\n",
      "140:\ttest: 0.7426301\tbest: 0.7426301 (140)\ttotal: 54.1s\tremaining: 22.6s\n",
      "160:\ttest: 0.7441156\tbest: 0.7441156 (160)\ttotal: 1m 1s\tremaining: 14.9s\n",
      "180:\ttest: 0.7452739\tbest: 0.7452739 (180)\ttotal: 1m 9s\tremaining: 7.26s\n",
      "199:\ttest: 0.7460072\tbest: 0.7460072 (199)\ttotal: 1m 16s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7460072366\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "28:\tloss: 0.7460072\tbest: 0.7501471 (14)\ttotal: 33m 13s\tremaining: 3m 26s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 286ms\tremaining: 56.8s\n",
      "20:\ttest: 0.7213168\tbest: 0.7213168 (20)\ttotal: 7.97s\tremaining: 1m 7s\n",
      "40:\ttest: 0.7355076\tbest: 0.7355076 (40)\ttotal: 15.7s\tremaining: 1m\n",
      "60:\ttest: 0.7408197\tbest: 0.7408197 (60)\ttotal: 23.7s\tremaining: 54s\n",
      "80:\ttest: 0.7438093\tbest: 0.7438093 (80)\ttotal: 33s\tremaining: 48.4s\n",
      "100:\ttest: 0.7455383\tbest: 0.7455383 (100)\ttotal: 40.4s\tremaining: 39.6s\n",
      "120:\ttest: 0.7471059\tbest: 0.7471059 (120)\ttotal: 47.8s\tremaining: 31.2s\n",
      "140:\ttest: 0.7481037\tbest: 0.7481037 (140)\ttotal: 54.9s\tremaining: 23s\n",
      "160:\ttest: 0.7488365\tbest: 0.7488365 (160)\ttotal: 1m 1s\tremaining: 15s\n",
      "180:\ttest: 0.7495665\tbest: 0.7495665 (180)\ttotal: 1m 8s\tremaining: 7.22s\n",
      "199:\ttest: 0.7500661\tbest: 0.7500672 (198)\ttotal: 1m 15s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7500671645\n",
      "bestIteration = 198\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "29:\tloss: 0.7500672\tbest: 0.7501471 (14)\ttotal: 34m 29s\tremaining: 2m 17s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 280ms\tremaining: 55.7s\n",
      "20:\ttest: 0.7312267\tbest: 0.7312267 (20)\ttotal: 8.1s\tremaining: 1m 9s\n",
      "40:\ttest: 0.7398249\tbest: 0.7398249 (40)\ttotal: 16.3s\tremaining: 1m 3s\n",
      "60:\ttest: 0.7438871\tbest: 0.7438964 (59)\ttotal: 25.2s\tremaining: 57.4s\n",
      "80:\ttest: 0.7454393\tbest: 0.7454461 (78)\ttotal: 33.2s\tremaining: 48.7s\n",
      "100:\ttest: 0.7471704\tbest: 0.7472111 (99)\ttotal: 41.4s\tremaining: 40.5s\n",
      "120:\ttest: 0.7477966\tbest: 0.7477966 (120)\ttotal: 48.9s\tremaining: 31.9s\n",
      "140:\ttest: 0.7489173\tbest: 0.7489173 (140)\ttotal: 57.2s\tremaining: 23.9s\n",
      "160:\ttest: 0.7496694\tbest: 0.7496694 (160)\ttotal: 1m 5s\tremaining: 15.8s\n",
      "180:\ttest: 0.7498144\tbest: 0.7500010 (174)\ttotal: 1m 12s\tremaining: 7.66s\n",
      "199:\ttest: 0.7501471\tbest: 0.7501471 (199)\ttotal: 1m 20s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7501471068\n",
      "bestIteration = 199\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "30:\tloss: 0.7501471\tbest: 0.7501471 (14)\ttotal: 35m 49s\tremaining: 1m 9s\n",
      "0:\ttest: 0.5608588\tbest: 0.5608588 (0)\ttotal: 330ms\tremaining: 1m 5s\n",
      "20:\ttest: 0.7336760\tbest: 0.7336760 (20)\ttotal: 8.78s\tremaining: 1m 14s\n",
      "40:\ttest: 0.7421537\tbest: 0.7421537 (40)\ttotal: 16.9s\tremaining: 1m 5s\n",
      "60:\ttest: 0.7457007\tbest: 0.7457007 (60)\ttotal: 25s\tremaining: 57s\n",
      "80:\ttest: 0.7472166\tbest: 0.7472166 (80)\ttotal: 33.4s\tremaining: 49.1s\n",
      "100:\ttest: 0.7482485\tbest: 0.7482485 (100)\ttotal: 41.5s\tremaining: 40.6s\n",
      "120:\ttest: 0.7489985\tbest: 0.7490256 (119)\ttotal: 49.4s\tremaining: 32.2s\n",
      "140:\ttest: 0.7490803\tbest: 0.7491625 (129)\ttotal: 56.8s\tremaining: 23.7s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7491625186\n",
      "bestIteration = 129\n",
      "\n",
      "Metric AUC is not calculated on train by default. To calculate this metric on train, add hints=skip_train~false to metric parameters.\n",
      "31:\tloss: 0.7491625\tbest: 0.7501471 (14)\ttotal: 36m 49s\tremaining: 0us\n",
      "Estimating final quality...\n",
      "Training on fold [0/3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5466240\tbest: 0.5466240 (0)\ttotal: 308ms\tremaining: 1m 1s\n",
      "20:\ttest: 0.7280062\tbest: 0.7280062 (20)\ttotal: 7.44s\tremaining: 1m 3s\n",
      "40:\ttest: 0.7372597\tbest: 0.7372597 (40)\ttotal: 14.3s\tremaining: 55.6s\n",
      "60:\ttest: 0.7412573\tbest: 0.7412573 (60)\ttotal: 20.9s\tremaining: 47.5s\n",
      "80:\ttest: 0.7433852\tbest: 0.7433852 (80)\ttotal: 27.2s\tremaining: 40s\n",
      "100:\ttest: 0.7441864\tbest: 0.7441900 (99)\ttotal: 33.5s\tremaining: 32.8s\n",
      "120:\ttest: 0.7447557\tbest: 0.7448117 (117)\ttotal: 39.3s\tremaining: 25.7s\n",
      "140:\ttest: 0.7449382\tbest: 0.7450386 (133)\ttotal: 45.3s\tremaining: 18.9s\n",
      "160:\ttest: 0.7453059\tbest: 0.7453059 (160)\ttotal: 51.4s\tremaining: 12.5s\n",
      "180:\ttest: 0.7456439\tbest: 0.7456526 (176)\ttotal: 57.5s\tremaining: 6.03s\n",
      "199:\ttest: 0.7458717\tbest: 0.7458717 (199)\ttotal: 1m 2s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.745871653\n",
      "bestIteration = 199\n",
      "\n",
      "Training on fold [1/3]\n",
      "0:\ttest: 0.5030965\tbest: 0.5030965 (0)\ttotal: 208ms\tremaining: 41.4s\n",
      "20:\ttest: 0.7269786\tbest: 0.7269786 (20)\ttotal: 6.96s\tremaining: 59.3s\n",
      "40:\ttest: 0.7359736\tbest: 0.7359736 (40)\ttotal: 14s\tremaining: 54.4s\n",
      "60:\ttest: 0.7395899\tbest: 0.7395899 (60)\ttotal: 20.7s\tremaining: 47.1s\n",
      "80:\ttest: 0.7424179\tbest: 0.7424200 (79)\ttotal: 27.4s\tremaining: 40.2s\n",
      "100:\ttest: 0.7438677\tbest: 0.7438677 (100)\ttotal: 33.8s\tremaining: 33.1s\n",
      "120:\ttest: 0.7449566\tbest: 0.7449770 (117)\ttotal: 40.6s\tremaining: 26.5s\n",
      "140:\ttest: 0.7450859\tbest: 0.7451233 (135)\ttotal: 46.8s\tremaining: 19.6s\n",
      "160:\ttest: 0.7453141\tbest: 0.7454346 (159)\ttotal: 53s\tremaining: 12.8s\n",
      "\n",
      "bestTest = 0.7454346082\n",
      "bestIteration = 159\n",
      "\n",
      "Training on fold [2/3]\n",
      "0:\ttest: 0.5472850\tbest: 0.5472850 (0)\ttotal: 283ms\tremaining: 56.3s\n",
      "20:\ttest: 0.7259946\tbest: 0.7259946 (20)\ttotal: 7.25s\tremaining: 1m 1s\n",
      "40:\ttest: 0.7372018\tbest: 0.7372018 (40)\ttotal: 14s\tremaining: 54.1s\n",
      "60:\ttest: 0.7409509\tbest: 0.7409509 (60)\ttotal: 20.2s\tremaining: 46.1s\n",
      "80:\ttest: 0.7430899\tbest: 0.7430904 (79)\ttotal: 26.3s\tremaining: 38.7s\n",
      "100:\ttest: 0.7441000\tbest: 0.7441192 (99)\ttotal: 32.6s\tremaining: 32s\n",
      "120:\ttest: 0.7449599\tbest: 0.7449599 (120)\ttotal: 40.4s\tremaining: 26.4s\n",
      "140:\ttest: 0.7455849\tbest: 0.7455849 (140)\ttotal: 47.2s\tremaining: 19.7s\n",
      "160:\ttest: 0.7460361\tbest: 0.7460840 (157)\ttotal: 53.2s\tremaining: 12.9s\n",
      "180:\ttest: 0.7466474\tbest: 0.7466474 (180)\ttotal: 59.4s\tremaining: 6.23s\n",
      "199:\ttest: 0.7467573\tbest: 0.7468616 (186)\ttotal: 1m 4s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7468616149\n",
      "bestIteration = 186\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'params': {'min_data_in_leaf': 3,\n",
       "  'depth': 7,\n",
       "  'learning_rate': 0.30000000000000004},\n",
       " 'cv_results': defaultdict(list,\n",
       "             {'iterations': [0,\n",
       "               1,\n",
       "               2,\n",
       "               3,\n",
       "               4,\n",
       "               5,\n",
       "               6,\n",
       "               7,\n",
       "               8,\n",
       "               9,\n",
       "               10,\n",
       "               11,\n",
       "               12,\n",
       "               13,\n",
       "               14,\n",
       "               15,\n",
       "               16,\n",
       "               17,\n",
       "               18,\n",
       "               19,\n",
       "               20,\n",
       "               21,\n",
       "               22,\n",
       "               23,\n",
       "               24,\n",
       "               25,\n",
       "               26,\n",
       "               27,\n",
       "               28,\n",
       "               29,\n",
       "               30,\n",
       "               31,\n",
       "               32,\n",
       "               33,\n",
       "               34,\n",
       "               35,\n",
       "               36,\n",
       "               37,\n",
       "               38,\n",
       "               39,\n",
       "               40,\n",
       "               41,\n",
       "               42,\n",
       "               43,\n",
       "               44,\n",
       "               45,\n",
       "               46,\n",
       "               47,\n",
       "               48,\n",
       "               49,\n",
       "               50,\n",
       "               51,\n",
       "               52,\n",
       "               53,\n",
       "               54,\n",
       "               55,\n",
       "               56,\n",
       "               57,\n",
       "               58,\n",
       "               59,\n",
       "               60,\n",
       "               61,\n",
       "               62,\n",
       "               63,\n",
       "               64,\n",
       "               65,\n",
       "               66,\n",
       "               67,\n",
       "               68,\n",
       "               69,\n",
       "               70,\n",
       "               71,\n",
       "               72,\n",
       "               73,\n",
       "               74,\n",
       "               75,\n",
       "               76,\n",
       "               77,\n",
       "               78,\n",
       "               79,\n",
       "               80,\n",
       "               81,\n",
       "               82,\n",
       "               83,\n",
       "               84,\n",
       "               85,\n",
       "               86,\n",
       "               87,\n",
       "               88,\n",
       "               89,\n",
       "               90,\n",
       "               91,\n",
       "               92,\n",
       "               93,\n",
       "               94,\n",
       "               95,\n",
       "               96,\n",
       "               97,\n",
       "               98,\n",
       "               99,\n",
       "               100,\n",
       "               101,\n",
       "               102,\n",
       "               103,\n",
       "               104,\n",
       "               105,\n",
       "               106,\n",
       "               107,\n",
       "               108,\n",
       "               109,\n",
       "               110,\n",
       "               111,\n",
       "               112,\n",
       "               113,\n",
       "               114,\n",
       "               115,\n",
       "               116,\n",
       "               117,\n",
       "               118,\n",
       "               119,\n",
       "               120,\n",
       "               121,\n",
       "               122,\n",
       "               123,\n",
       "               124,\n",
       "               125,\n",
       "               126,\n",
       "               127,\n",
       "               128,\n",
       "               129,\n",
       "               130,\n",
       "               131,\n",
       "               132,\n",
       "               133,\n",
       "               134,\n",
       "               135,\n",
       "               136,\n",
       "               137,\n",
       "               138,\n",
       "               139,\n",
       "               140,\n",
       "               141,\n",
       "               142,\n",
       "               143,\n",
       "               144,\n",
       "               145,\n",
       "               146,\n",
       "               147,\n",
       "               148,\n",
       "               149,\n",
       "               150,\n",
       "               151,\n",
       "               152,\n",
       "               153,\n",
       "               154,\n",
       "               155,\n",
       "               156,\n",
       "               157,\n",
       "               158,\n",
       "               159,\n",
       "               160,\n",
       "               161,\n",
       "               162,\n",
       "               163,\n",
       "               164,\n",
       "               165,\n",
       "               166,\n",
       "               167,\n",
       "               168,\n",
       "               169,\n",
       "               170,\n",
       "               171,\n",
       "               172,\n",
       "               173,\n",
       "               174,\n",
       "               175,\n",
       "               176,\n",
       "               177,\n",
       "               178,\n",
       "               179,\n",
       "               180,\n",
       "               181,\n",
       "               182,\n",
       "               183,\n",
       "               184,\n",
       "               185,\n",
       "               186,\n",
       "               187,\n",
       "               188,\n",
       "               189,\n",
       "               190,\n",
       "               191,\n",
       "               192,\n",
       "               193,\n",
       "               194,\n",
       "               195,\n",
       "               196,\n",
       "               197,\n",
       "               198,\n",
       "               199],\n",
       "              'test-AUC-mean': [0.5323351871134413,\n",
       "               0.5672485377872802,\n",
       "               0.6073729499291639,\n",
       "               0.6682448148498139,\n",
       "               0.6815865549484652,\n",
       "               0.6905379693067895,\n",
       "               0.697254815238835,\n",
       "               0.6989171525505835,\n",
       "               0.7034855810236008,\n",
       "               0.7081957164191247,\n",
       "               0.712240618753471,\n",
       "               0.7139975265978536,\n",
       "               0.7159752865593644,\n",
       "               0.7175685424543378,\n",
       "               0.7192737969143583,\n",
       "               0.7205048017282761,\n",
       "               0.7221750988457073,\n",
       "               0.7236253837036376,\n",
       "               0.724682731134001,\n",
       "               0.7257887194849176,\n",
       "               0.7269931493303302,\n",
       "               0.7279065089832814,\n",
       "               0.7288224791887279,\n",
       "               0.7293681792534473,\n",
       "               0.7300194439278632,\n",
       "               0.7309084709246666,\n",
       "               0.7314423107118962,\n",
       "               0.7318070239971508,\n",
       "               0.7326287091636717,\n",
       "               0.7330962965236035,\n",
       "               0.7338660846256045,\n",
       "               0.7341638368570527,\n",
       "               0.7347563041881463,\n",
       "               0.7350512845234406,\n",
       "               0.7353708112052444,\n",
       "               0.7355543177924654,\n",
       "               0.7358423387504356,\n",
       "               0.7361120806751931,\n",
       "               0.7363384454684726,\n",
       "               0.7365760315204622,\n",
       "               0.7368117247389444,\n",
       "               0.7370277696510957,\n",
       "               0.7375601115497495,\n",
       "               0.7377754549380248,\n",
       "               0.7378912040874498,\n",
       "               0.7380370290763554,\n",
       "               0.7382350999242915,\n",
       "               0.7384344775060164,\n",
       "               0.7385329708839873,\n",
       "               0.7385593695851984,\n",
       "               0.7387566201175919,\n",
       "               0.7389830207813505,\n",
       "               0.7391369383935688,\n",
       "               0.7393880118340554,\n",
       "               0.7395543786468469,\n",
       "               0.7396444015967685,\n",
       "               0.7398647277582878,\n",
       "               0.7400291683973553,\n",
       "               0.7402085796786584,\n",
       "               0.7403351133767547,\n",
       "               0.7405993644608135,\n",
       "               0.740807386585787,\n",
       "               0.7409894061838483,\n",
       "               0.7411666514507255,\n",
       "               0.741305777489257,\n",
       "               0.7413729131333818,\n",
       "               0.7414738583117991,\n",
       "               0.741625442448362,\n",
       "               0.7417041600133513,\n",
       "               0.7418539191210747,\n",
       "               0.7419784933896265,\n",
       "               0.742097296431333,\n",
       "               0.7422724865920779,\n",
       "               0.7423835930003587,\n",
       "               0.7424318242905521,\n",
       "               0.7425292580388412,\n",
       "               0.742646728928146,\n",
       "               0.7427824920624081,\n",
       "               0.7428386192241708,\n",
       "               0.7429419396670838,\n",
       "               0.7429643327710437,\n",
       "               0.7430798827049051,\n",
       "               0.7431569563374699,\n",
       "               0.7432715937467483,\n",
       "               0.7433677880138255,\n",
       "               0.7434412558096488,\n",
       "               0.7435074239438036,\n",
       "               0.7435534593720735,\n",
       "               0.7435571054726401,\n",
       "               0.7435775020787556,\n",
       "               0.7436365666175054,\n",
       "               0.7437064085778027,\n",
       "               0.7437422225170586,\n",
       "               0.7437764896028108,\n",
       "               0.7438178820318823,\n",
       "               0.74387061624648,\n",
       "               0.7438905727649447,\n",
       "               0.7439927661673247,\n",
       "               0.7440024146013199,\n",
       "               0.7440377978482045,\n",
       "               0.7440513821192641,\n",
       "               0.7441273484926604,\n",
       "               0.7441669408564753,\n",
       "               0.7441858556113946,\n",
       "               0.7442124979466755,\n",
       "               0.7442771576979249,\n",
       "               0.7443005728006197,\n",
       "               0.7443311953439778,\n",
       "               0.7443650456556239,\n",
       "               0.744397027291618,\n",
       "               0.74440267424395,\n",
       "               0.7444382190263493,\n",
       "               0.7445137351926846,\n",
       "               0.7445525301781267,\n",
       "               0.7447270665016884,\n",
       "               0.7447642871083064,\n",
       "               0.7448551349518983,\n",
       "               0.744901713303259,\n",
       "               0.7448661628733549,\n",
       "               0.7448505716325475,\n",
       "               0.7448907389692989,\n",
       "               0.7449061961955286,\n",
       "               0.7449644381774257,\n",
       "               0.744962178409649,\n",
       "               0.7449676824869712,\n",
       "               0.7449661186879172,\n",
       "               0.7450267738808226,\n",
       "               0.7450470288716411,\n",
       "               0.745069257522755,\n",
       "               0.7450743944343543,\n",
       "               0.7450800318421195,\n",
       "               0.7450750130676216,\n",
       "               0.7450995540641578,\n",
       "               0.7451100815027649,\n",
       "               0.7450914218059653,\n",
       "               0.7450837653500435,\n",
       "               0.7450892893651595,\n",
       "               0.7450785936404719,\n",
       "               0.7451303086800137,\n",
       "               0.7452062206328023,\n",
       "               0.7452029918855784,\n",
       "               0.7452044702790896,\n",
       "               0.7452078183243406,\n",
       "               0.7452132176265415,\n",
       "               0.7452586561354283,\n",
       "               0.745277115252616,\n",
       "               0.7453254146410546,\n",
       "               0.7453198801360831,\n",
       "               0.7453331444036664,\n",
       "               0.7453590264265971,\n",
       "               0.7454014770083107,\n",
       "               0.7453813748399947,\n",
       "               0.7454102949284719,\n",
       "               0.7453979245570119,\n",
       "               0.7454442849065378,\n",
       "               0.7454711749955631,\n",
       "               0.7455215502306715,\n",
       "               0.7455523830208541,\n",
       "               0.7455553145576462,\n",
       "               0.7455870789437258,\n",
       "               0.7455520387048767,\n",
       "               0.7455574928939246,\n",
       "               0.7455483202326004,\n",
       "               0.7455471571826099,\n",
       "               0.745548586218574,\n",
       "               0.7455594295195236,\n",
       "               0.7456040841891513,\n",
       "               0.745622476326688,\n",
       "               0.7456657491240811,\n",
       "               0.7456576837885875,\n",
       "               0.7456868405923801,\n",
       "               0.7457052704803594,\n",
       "               0.7456997394776056,\n",
       "               0.7457246870874767,\n",
       "               0.7458033622719472,\n",
       "               0.7458844799535762,\n",
       "               0.7458810888184993,\n",
       "               0.7458754325125145,\n",
       "               0.7458818436495037,\n",
       "               0.7458766029947742,\n",
       "               0.7458869207620135,\n",
       "               0.7458900103891483,\n",
       "               0.7459060247495494,\n",
       "               0.7459091832116601,\n",
       "               0.745936544993763,\n",
       "               0.7459397310864073,\n",
       "               0.7459678665192087,\n",
       "               0.7459554688132831,\n",
       "               0.7459666337707551,\n",
       "               0.7459636588161501,\n",
       "               0.7459680057902269,\n",
       "               0.7459433389010277,\n",
       "               0.7459593686895082,\n",
       "               0.7459496736175785,\n",
       "               0.7459615684481582,\n",
       "               0.7459976774971092,\n",
       "               0.7459951829498932,\n",
       "               0.7459921037133087,\n",
       "               0.7459959742143821,\n",
       "               0.7459994991456217],\n",
       "              'test-AUC-std': [0.02532359239281949,\n",
       "               0.008466858922239156,\n",
       "               0.004585950280540858,\n",
       "               0.004815449956083742,\n",
       "               0.0019913225430410898,\n",
       "               0.0036590378327971533,\n",
       "               0.0010377349078268805,\n",
       "               0.0018098267662474293,\n",
       "               0.0021389364814496147,\n",
       "               0.0010816210541981707,\n",
       "               0.0009109152685680217,\n",
       "               0.002434088331193539,\n",
       "               0.0016368986811836261,\n",
       "               0.0023295211697072406,\n",
       "               0.002577920523229613,\n",
       "               0.0023204793795475248,\n",
       "               0.0015796647918663812,\n",
       "               0.0012675131083679748,\n",
       "               0.0005959262009161987,\n",
       "               0.0006274494740008236,\n",
       "               0.0010058442669958383,\n",
       "               0.0012940433041228297,\n",
       "               0.0013456221322952367,\n",
       "               0.001144895839357038,\n",
       "               0.0012123648314638605,\n",
       "               0.0013366982727912094,\n",
       "               0.001339040058419934,\n",
       "               0.0012871037044102289,\n",
       "               0.0013943295534002281,\n",
       "               0.0012317236773407934,\n",
       "               0.0009226238463269436,\n",
       "               0.0009207596209577093,\n",
       "               0.0008880951451911988,\n",
       "               0.0008277279030819567,\n",
       "               0.0009236325653394494,\n",
       "               0.0008266193498340217,\n",
       "               0.000908486289915637,\n",
       "               0.0008861306885369425,\n",
       "               0.0007098059362100951,\n",
       "               0.0006512860068509332,\n",
       "               0.0007263825197793467,\n",
       "               0.0007253717277105685,\n",
       "               0.0005512314198523541,\n",
       "               0.0007581512258018975,\n",
       "               0.0006857224396737535,\n",
       "               0.0007797304147385075,\n",
       "               0.0008170636530197181,\n",
       "               0.0007581922769637968,\n",
       "               0.0009294312946726416,\n",
       "               0.0009398192095068884,\n",
       "               0.0009126326959371571,\n",
       "               0.0009698487471650067,\n",
       "               0.0009380196943498978,\n",
       "               0.0007515349890663632,\n",
       "               0.0006187828199820805,\n",
       "               0.0005057435706896258,\n",
       "               0.0005871467477125114,\n",
       "               0.000615019317967743,\n",
       "               0.0007190485263371711,\n",
       "               0.0008866482515136522,\n",
       "               0.0008875667375294412,\n",
       "               0.0007285008398668062,\n",
       "               0.0008960573508490511,\n",
       "               0.0008079691274163551,\n",
       "               0.0007062063761600933,\n",
       "               0.00069487865013244,\n",
       "               0.0007233722926929515,\n",
       "               0.0005597041835083738,\n",
       "               0.0005264861194173963,\n",
       "               0.0004724843053420606,\n",
       "               0.0005093057362766326,\n",
       "               0.0005173542554170649,\n",
       "               0.0004808240967711858,\n",
       "               0.0004109066501969464,\n",
       "               0.00044570281115598973,\n",
       "               0.0004272792413529454,\n",
       "               0.000439054745604249,\n",
       "               0.00041803362579330325,\n",
       "               0.0004773632261178682,\n",
       "               0.00046583858069193973,\n",
       "               0.0004957508939481096,\n",
       "               0.0005727322649158489,\n",
       "               0.000561120993353562,\n",
       "               0.00042532409476542867,\n",
       "               0.0003669174434163326,\n",
       "               0.00037835918011397277,\n",
       "               0.0003135152491979665,\n",
       "               0.0003497234273534862,\n",
       "               0.00027304021948943374,\n",
       "               0.0002696544315423864,\n",
       "               0.0003023229851023641,\n",
       "               0.00025026774289106294,\n",
       "               0.0002469934991097206,\n",
       "               0.00022811245710394585,\n",
       "               0.00022213018620281731,\n",
       "               0.00020629942162416544,\n",
       "               0.00017864512502352914,\n",
       "               0.00017977635554250856,\n",
       "               0.0001847724069356753,\n",
       "               0.0002053234473095896,\n",
       "               0.0001648434464886782,\n",
       "               0.00020761385073964593,\n",
       "               0.00017284493789659886,\n",
       "               0.00017687275828017155,\n",
       "               0.00017746676757125648,\n",
       "               0.0002437725109907605,\n",
       "               0.0002940307387516383,\n",
       "               0.0003006814677872576,\n",
       "               0.0003304338159713527,\n",
       "               0.00032571608564376794,\n",
       "               0.00031200116755029656,\n",
       "               0.0003233719214620155,\n",
       "               0.00021271755789855547,\n",
       "               0.00019601258827327953,\n",
       "               5.5581490153552954e-05,\n",
       "               6.25634462612514e-05,\n",
       "               4.728296292219027e-05,\n",
       "               8.362111438530978e-05,\n",
       "               7.324550546186718e-05,\n",
       "               8.548065158074559e-05,\n",
       "               0.00011692009061906363,\n",
       "               0.000138881805328732,\n",
       "               0.000159372486809559,\n",
       "               0.00016631320414211336,\n",
       "               0.0001540334681279224,\n",
       "               0.00015879743220322478,\n",
       "               0.00013113605028878332,\n",
       "               0.0001007652880926456,\n",
       "               0.00011658553193894731,\n",
       "               0.00011585040775047589,\n",
       "               6.494702932034623e-05,\n",
       "               7.290157957630871e-05,\n",
       "               9.225930628721633e-05,\n",
       "               6.856129893881642e-05,\n",
       "               8.783686234356395e-05,\n",
       "               9.637590039608928e-05,\n",
       "               9.994698282443942e-05,\n",
       "               6.393479474443715e-05,\n",
       "               0.00013651364132303686,\n",
       "               0.0003143939839207871,\n",
       "               0.000338909681777747,\n",
       "               0.0003266314328202843,\n",
       "               0.0003432787309968498,\n",
       "               0.00037432162328763184,\n",
       "               0.00034363456493125894,\n",
       "               0.00035450351200104084,\n",
       "               0.00035323490055288895,\n",
       "               0.00036865662560145657,\n",
       "               0.0003782092031153797,\n",
       "               0.0003926083943459441,\n",
       "               0.0003963172431402646,\n",
       "               0.00039696883222072743,\n",
       "               0.00038464737381002504,\n",
       "               0.0003971747478911386,\n",
       "               0.00040905672666516406,\n",
       "               0.0004183232031727523,\n",
       "               0.0004381367373615701,\n",
       "               0.0004672035306302684,\n",
       "               0.0004367392241883043,\n",
       "               0.00040946734858020973,\n",
       "               0.0004192579104515046,\n",
       "               0.0004193533568755244,\n",
       "               0.00041762369170116943,\n",
       "               0.0004192615712828445,\n",
       "               0.0004362101258684112,\n",
       "               0.00042009784981146027,\n",
       "               0.00041748720069425047,\n",
       "               0.00040888224065514,\n",
       "               0.00038802859342702844,\n",
       "               0.00037026441296878425,\n",
       "               0.00037861962453010386,\n",
       "               0.00037692481417933614,\n",
       "               0.00043027335014486477,\n",
       "               0.00046121400920888083,\n",
       "               0.0005446723221166341,\n",
       "               0.0006442071833715171,\n",
       "               0.0006351865003248502,\n",
       "               0.0006310578465817754,\n",
       "               0.0006551561172386533,\n",
       "               0.0006502535147695251,\n",
       "               0.0006727224133376267,\n",
       "               0.0006781888070109002,\n",
       "               0.0007001067016156205,\n",
       "               0.0007298574057930118,\n",
       "               0.0007459932046994607,\n",
       "               0.0007589191745506932,\n",
       "               0.0007886918261306194,\n",
       "               0.0007713032934364513,\n",
       "               0.0007866913976575951,\n",
       "               0.0007817532214235002,\n",
       "               0.0007707687804994566,\n",
       "               0.0007440474526188147,\n",
       "               0.00072979931181204,\n",
       "               0.0007106698589331393,\n",
       "               0.0007112330868961144,\n",
       "               0.0007289071317548428,\n",
       "               0.0007106591616577817,\n",
       "               0.0007046394371559195,\n",
       "               0.0007053257487551718,\n",
       "               0.0007026838108733379],\n",
       "              'test-Logloss-mean': [0.35180698124470894,\n",
       "               0.23026996637385042,\n",
       "               0.18397858416083512,\n",
       "               0.16388931064488058,\n",
       "               0.15536707191661384,\n",
       "               0.1510823448581409,\n",
       "               0.14897560631064996,\n",
       "               0.1478944726648073,\n",
       "               0.14696395529728792,\n",
       "               0.14631252901941508,\n",
       "               0.145841463446266,\n",
       "               0.14553385428737553,\n",
       "               0.14517442685501203,\n",
       "               0.1449521560323742,\n",
       "               0.1447005467928781,\n",
       "               0.1445106367788774,\n",
       "               0.14432068138816234,\n",
       "               0.14414496820162906,\n",
       "               0.1439847337617169,\n",
       "               0.14387398888328623,\n",
       "               0.14372076145221693,\n",
       "               0.14362783120004263,\n",
       "               0.1435155113638539,\n",
       "               0.14343977554118434,\n",
       "               0.14335047017673552,\n",
       "               0.14324950377697632,\n",
       "               0.14318476583632203,\n",
       "               0.14314203109113346,\n",
       "               0.14304399774300544,\n",
       "               0.1429828010495902,\n",
       "               0.1428858103828851,\n",
       "               0.1428420702822026,\n",
       "               0.14277711119100542,\n",
       "               0.1427274278415724,\n",
       "               0.14268195075713838,\n",
       "               0.14264702864002174,\n",
       "               0.14261450467344372,\n",
       "               0.1425712541522943,\n",
       "               0.14254550909796357,\n",
       "               0.1425100209513985,\n",
       "               0.14248380388225937,\n",
       "               0.14245558471070216,\n",
       "               0.1423873084659136,\n",
       "               0.1423520416533882,\n",
       "               0.1423287957389005,\n",
       "               0.14231113232960577,\n",
       "               0.14228566315011004,\n",
       "               0.14225297869098777,\n",
       "               0.14224203548224248,\n",
       "               0.14223670194442095,\n",
       "               0.14221032597081132,\n",
       "               0.14217898327163894,\n",
       "               0.14216172290761173,\n",
       "               0.14213419209220823,\n",
       "               0.14211192770888062,\n",
       "               0.14210231526719985,\n",
       "               0.1420770087522377,\n",
       "               0.14204410282256663,\n",
       "               0.14202606162162001,\n",
       "               0.14201045814390814,\n",
       "               0.14197721330507398,\n",
       "               0.1419531007127323,\n",
       "               0.14192575821498002,\n",
       "               0.14190428045063205,\n",
       "               0.14189037205556174,\n",
       "               0.14188098536520286,\n",
       "               0.14186819654531244,\n",
       "               0.14184966432416232,\n",
       "               0.14184200720921683,\n",
       "               0.14182600533948445,\n",
       "               0.14181088796536323,\n",
       "               0.14180080846969337,\n",
       "               0.14178081205392776,\n",
       "               0.14176719688894732,\n",
       "               0.14175871266596737,\n",
       "               0.1417443225176791,\n",
       "               0.14173009605088135,\n",
       "               0.1417141639351123,\n",
       "               0.1417039128790523,\n",
       "               0.14168981376374737,\n",
       "               0.14168606369182637,\n",
       "               0.14167251170840303,\n",
       "               0.14166279303869245,\n",
       "               0.1416498733847369,\n",
       "               0.14163891132959164,\n",
       "               0.1416314475128556,\n",
       "               0.14162375164882954,\n",
       "               0.141617063539044,\n",
       "               0.14161587546664803,\n",
       "               0.14161372009561105,\n",
       "               0.14160998769666766,\n",
       "               0.14159501406094213,\n",
       "               0.14159161971921572,\n",
       "               0.14158785351263456,\n",
       "               0.14158697940660855,\n",
       "               0.1415806387129128,\n",
       "               0.1415760637177954,\n",
       "               0.14155996488760939,\n",
       "               0.14155938399343068,\n",
       "               0.14155387493436364,\n",
       "               0.14155017932230232,\n",
       "               0.1415419595162399,\n",
       "               0.14153822758747733,\n",
       "               0.1415350291456736,\n",
       "               0.1415312814443288,\n",
       "               0.14152165555152485,\n",
       "               0.14151630013793481,\n",
       "               0.14151476059711635,\n",
       "               0.14150941452489504,\n",
       "               0.14150600003735156,\n",
       "               0.1415052428427487,\n",
       "               0.14149906230146095,\n",
       "               0.1414926589488338,\n",
       "               0.14148659525263066,\n",
       "               0.1414660278632282,\n",
       "               0.14146043397263794,\n",
       "               0.14144873823631485,\n",
       "               0.14144002355354315,\n",
       "               0.14144415240665806,\n",
       "               0.14144604054162202,\n",
       "               0.14144125860299242,\n",
       "               0.14143835166125726,\n",
       "               0.14142670970835183,\n",
       "               0.1414250409164651,\n",
       "               0.1414200430209583,\n",
       "               0.14142080227367038,\n",
       "               0.1414180805128696,\n",
       "               0.14141618674546222,\n",
       "               0.14141078615690797,\n",
       "               0.14141248458233396,\n",
       "               0.1414112349496702,\n",
       "               0.14141413588947427,\n",
       "               0.14141265949537735,\n",
       "               0.14141145199939856,\n",
       "               0.14141178290805975,\n",
       "               0.14141114333444463,\n",
       "               0.14141080390410607,\n",
       "               0.14141310715010907,\n",
       "               0.1414108689820429,\n",
       "               0.14139944188870576,\n",
       "               0.1413977009161523,\n",
       "               0.1413963499663271,\n",
       "               0.1413958386481126,\n",
       "               0.14139585639743146,\n",
       "               0.14138849366175707,\n",
       "               0.14138650872046887,\n",
       "               0.1413791420381603,\n",
       "               0.1413770472558733,\n",
       "               0.14137833331128102,\n",
       "               0.14137638136198225,\n",
       "               0.14137064295410784,\n",
       "               0.14137072728782887,\n",
       "               0.14136202210448945,\n",
       "               0.14136268221242856,\n",
       "               0.14135602703029135,\n",
       "               0.141349377286439,\n",
       "               0.14134609326019285,\n",
       "               0.14134338895287124,\n",
       "               0.14134162195493039,\n",
       "               0.14133863938359015,\n",
       "               0.14133996281159514,\n",
       "               0.14133855952276983,\n",
       "               0.14133807029450488,\n",
       "               0.1413378928908927,\n",
       "               0.1413373025229666,\n",
       "               0.14133475574633034,\n",
       "               0.14133037028239848,\n",
       "               0.1413275942109404,\n",
       "               0.1413251591967822,\n",
       "               0.14132296669682873,\n",
       "               0.14131770421067583,\n",
       "               0.14131642574302705,\n",
       "               0.14131841985522717,\n",
       "               0.1413192589439878,\n",
       "               0.14130964437704643,\n",
       "               0.1413012829024133,\n",
       "               0.1412990966681225,\n",
       "               0.1413001065928621,\n",
       "               0.1412967970987309,\n",
       "               0.1412967750989941,\n",
       "               0.14129504703454443,\n",
       "               0.1412946356937097,\n",
       "               0.14129258051240134,\n",
       "               0.14129059960092724,\n",
       "               0.1412848249743339,\n",
       "               0.14128466619544164,\n",
       "               0.14128254634476312,\n",
       "               0.1412836482114419,\n",
       "               0.14128390733264062,\n",
       "               0.14128330202817227,\n",
       "               0.14128161916479465,\n",
       "               0.14128313583350016,\n",
       "               0.1412802605935267,\n",
       "               0.14128111301307322,\n",
       "               0.14128100561449025,\n",
       "               0.14127810296945267,\n",
       "               0.14127796915402568,\n",
       "               0.1412775856065105,\n",
       "               0.14127725974326685,\n",
       "               0.14127669077226337],\n",
       "              'test-Logloss-std': [0.0005837574209663514,\n",
       "               0.0002172022677762946,\n",
       "               0.0002969095093022844,\n",
       "               0.0002555372726815721,\n",
       "               0.00021283460587267903,\n",
       "               0.0004202418831078934,\n",
       "               0.00015571945343389128,\n",
       "               0.00029774229598745,\n",
       "               0.000255313187081981,\n",
       "               7.651501357620769e-05,\n",
       "               6.601226108440212e-05,\n",
       "               0.00015527991153946248,\n",
       "               9.569326022227246e-05,\n",
       "               0.00013837819247912122,\n",
       "               0.00017732068608738,\n",
       "               0.00014250898480069082,\n",
       "               8.814574624388235e-05,\n",
       "               7.139889431199906e-05,\n",
       "               1.607344990252169e-05,\n",
       "               2.866477445658262e-05,\n",
       "               5.356389392858374e-05,\n",
       "               8.259948087794028e-05,\n",
       "               7.444936079662396e-05,\n",
       "               7.77515981070996e-05,\n",
       "               5.640494310542395e-05,\n",
       "               7.643880984889307e-05,\n",
       "               7.105094830057104e-05,\n",
       "               7.496255602883405e-05,\n",
       "               0.00011145816238551958,\n",
       "               9.38104898945748e-05,\n",
       "               8.372438310563817e-05,\n",
       "               8.986669510752975e-05,\n",
       "               0.00012254943227647165,\n",
       "               0.00012187171824515203,\n",
       "               0.00013883099459294484,\n",
       "               0.00013211310021562464,\n",
       "               0.00014585429006628737,\n",
       "               0.00015549452387443895,\n",
       "               0.00013651174479778572,\n",
       "               0.00012415571057365965,\n",
       "               0.00014041173981286227,\n",
       "               0.00013620109597409572,\n",
       "               0.00011235786649752728,\n",
       "               0.00011653202054386406,\n",
       "               0.00010210442476872522,\n",
       "               0.00011273146632724944,\n",
       "               0.00011687462910787654,\n",
       "               0.00010945650691776421,\n",
       "               0.00012728800618267363,\n",
       "               0.00012925052853179211,\n",
       "               0.00013983451997903303,\n",
       "               0.0001523510697805438,\n",
       "               0.0001594896129564806,\n",
       "               0.00013648011382222085,\n",
       "               0.00012407535496673214,\n",
       "               0.00011141986347715864,\n",
       "               0.00011830915317039878,\n",
       "               0.0001259481216724747,\n",
       "               0.0001439758503337618,\n",
       "               0.00015771086775209515,\n",
       "               0.00014812792851109184,\n",
       "               0.00012962203553671252,\n",
       "               0.00015078396066287738,\n",
       "               0.0001423358562330602,\n",
       "               0.0001306194204554875,\n",
       "               0.00012721313461129983,\n",
       "               0.00013043100454917224,\n",
       "               0.00010932449382016015,\n",
       "               0.0001090088811735865,\n",
       "               0.00010094318949279182,\n",
       "               0.00010260780813623869,\n",
       "               9.950202183128705e-05,\n",
       "               0.00010097808467949662,\n",
       "               9.654888975527773e-05,\n",
       "               0.00010431894850519347,\n",
       "               0.0001017464208828689,\n",
       "               9.387029485048853e-05,\n",
       "               9.550527731257697e-05,\n",
       "               0.00010193501529597636,\n",
       "               0.00010983615484675611,\n",
       "               0.00011384966341538812,\n",
       "               0.0001230273342997177,\n",
       "               0.00012328518005062745,\n",
       "               0.00010278953997408501,\n",
       "               9.378491064992746e-05,\n",
       "               9.297002195327092e-05,\n",
       "               8.983828348207946e-05,\n",
       "               9.493597249738022e-05,\n",
       "               8.726891692369035e-05,\n",
       "               8.7130003331294e-05,\n",
       "               9.427663044962366e-05,\n",
       "               7.995372419299508e-05,\n",
       "               8.324923770092814e-05,\n",
       "               7.775548428555403e-05,\n",
       "               7.583759418347922e-05,\n",
       "               7.289448918584842e-05,\n",
       "               7.089323722283244e-05,\n",
       "               6.264991983508518e-05,\n",
       "               6.407420304045691e-05,\n",
       "               6.463002371439278e-05,\n",
       "               5.7335849856998854e-05,\n",
       "               6.246125505097976e-05,\n",
       "               5.979969665840396e-05,\n",
       "               6.046755540919396e-05,\n",
       "               5.980628880092118e-05,\n",
       "               6.610711467160652e-05,\n",
       "               7.404103454735078e-05,\n",
       "               7.557026894425236e-05,\n",
       "               8.150631755721151e-05,\n",
       "               7.22909314737312e-05,\n",
       "               7.319811159014766e-05,\n",
       "               7.563359492100841e-05,\n",
       "               6.068805251699489e-05,\n",
       "               5.890877355035975e-05,\n",
       "               4.38688628517993e-05,\n",
       "               4.427185538906972e-05,\n",
       "               4.487312522128361e-05,\n",
       "               3.4624741940115545e-05,\n",
       "               3.259873988833714e-05,\n",
       "               3.7310097541288e-05,\n",
       "               3.811389415132559e-05,\n",
       "               4.411542118701477e-05,\n",
       "               4.9759621052660846e-05,\n",
       "               4.981362418877432e-05,\n",
       "               4.71560938041917e-05,\n",
       "               4.879135435237901e-05,\n",
       "               4.798959782481898e-05,\n",
       "               4.6648647541788485e-05,\n",
       "               5.0882543414887834e-05,\n",
       "               5.205288624310396e-05,\n",
       "               4.891886052195218e-05,\n",
       "               4.8611591989005545e-05,\n",
       "               4.9077509982445886e-05,\n",
       "               4.737288865673105e-05,\n",
       "               4.779436275417745e-05,\n",
       "               4.5352558244441845e-05,\n",
       "               4.648221319766166e-05,\n",
       "               4.377061058012832e-05,\n",
       "               5.118881145351792e-05,\n",
       "               7.594619011424548e-05,\n",
       "               7.849900553625912e-05,\n",
       "               7.949688173696476e-05,\n",
       "               7.972078999701114e-05,\n",
       "               8.311798386173363e-05,\n",
       "               7.79851652020845e-05,\n",
       "               7.98279527995231e-05,\n",
       "               7.818287299813165e-05,\n",
       "               8.494478098622199e-05,\n",
       "               8.578208589125797e-05,\n",
       "               8.590563320240281e-05,\n",
       "               8.454305358862779e-05,\n",
       "               8.616326299728145e-05,\n",
       "               8.33815643417997e-05,\n",
       "               8.381480126212499e-05,\n",
       "               8.817837957241318e-05,\n",
       "               8.455625796519155e-05,\n",
       "               8.696261266478662e-05,\n",
       "               8.961305868389429e-05,\n",
       "               8.57325047236974e-05,\n",
       "               8.362210604100771e-05,\n",
       "               8.892350729529429e-05,\n",
       "               8.844021293350916e-05,\n",
       "               8.862459583290053e-05,\n",
       "               8.899522253085028e-05,\n",
       "               8.992056784820182e-05,\n",
       "               8.915130025969569e-05,\n",
       "               8.925661120271388e-05,\n",
       "               8.804937829189448e-05,\n",
       "               8.870027412321713e-05,\n",
       "               9.095366683031001e-05,\n",
       "               9.165466389866536e-05,\n",
       "               9.289472580313485e-05,\n",
       "               9.52925465615794e-05,\n",
       "               0.00010282733124660046,\n",
       "               0.00011196968165069219,\n",
       "               0.0001191153698187999,\n",
       "               0.00012134300142199919,\n",
       "               0.00012053436089811714,\n",
       "               0.00012762531319839942,\n",
       "               0.0001259248579265282,\n",
       "               0.0001302077458216833,\n",
       "               0.00013105892259699574,\n",
       "               0.00013410723834397532,\n",
       "               0.00013876626870233406,\n",
       "               0.00014404384456570665,\n",
       "               0.00014480416500771662,\n",
       "               0.0001472706684137815,\n",
       "               0.0001452581528960754,\n",
       "               0.0001443353205326422,\n",
       "               0.00014390817413346057,\n",
       "               0.00014502682958959478,\n",
       "               0.00014262283077907367,\n",
       "               0.0001414876946954318,\n",
       "               0.00013962905829049602,\n",
       "               0.00013851266644080633,\n",
       "               0.00013938581278184973,\n",
       "               0.0001385702005323355,\n",
       "               0.00013838900160632086,\n",
       "               0.00013823319820230331,\n",
       "               0.00013797027325572947],\n",
       "              'train-Logloss-mean': [0.35176721840712855,\n",
       "               0.23018842093806988,\n",
       "               0.1838362830940481,\n",
       "               0.1637560661806244,\n",
       "               0.15521621418876286,\n",
       "               0.15093659878292745,\n",
       "               0.1488188462995693,\n",
       "               0.1476920105594722,\n",
       "               0.14671708789146032,\n",
       "               0.14605019227412705,\n",
       "               0.14556231998984911,\n",
       "               0.1452189806667257,\n",
       "               0.14483764258838752,\n",
       "               0.14458950136337978,\n",
       "               0.1443172819396642,\n",
       "               0.14410078484716118,\n",
       "               0.14390084996005137,\n",
       "               0.14370604154415495,\n",
       "               0.14353251937575573,\n",
       "               0.14340012470374497,\n",
       "               0.14322025600933078,\n",
       "               0.14310482505853225,\n",
       "               0.14297094016168024,\n",
       "               0.14287329100760862,\n",
       "               0.14275130106845982,\n",
       "               0.14262723412591133,\n",
       "               0.1425432461616484,\n",
       "               0.14248055831343437,\n",
       "               0.1423648128953731,\n",
       "               0.1422773597415666,\n",
       "               0.14216179724717595,\n",
       "               0.14209819480826724,\n",
       "               0.1420227139142685,\n",
       "               0.14193682403856936,\n",
       "               0.14186934286402508,\n",
       "               0.1418026651434272,\n",
       "               0.14174519012985098,\n",
       "               0.14167588021524805,\n",
       "               0.14163163263175252,\n",
       "               0.1415760647990761,\n",
       "               0.14152758080569702,\n",
       "               0.14148548714575404,\n",
       "               0.14139658523078957,\n",
       "               0.14134049592515532,\n",
       "               0.1412918696079497,\n",
       "               0.1412541544053633,\n",
       "               0.1412048862507497,\n",
       "               0.1411464863801517,\n",
       "               0.1411136668722227,\n",
       "               0.14109406663470267,\n",
       "               0.14104309364786013,\n",
       "               0.1409899621362947,\n",
       "               0.14094532052216216,\n",
       "               0.14089379790577983,\n",
       "               0.14084314509024884,\n",
       "               0.14081988885124908,\n",
       "               0.14076198231224032,\n",
       "               0.14071287725196743,\n",
       "               0.14067329311589327,\n",
       "               0.1406303483791496,\n",
       "               0.14056224316823054,\n",
       "               0.14051547619762605,\n",
       "               0.1404605995826689,\n",
       "               0.14042185860137638,\n",
       "               0.14038286198638272,\n",
       "               0.14034686979815425,\n",
       "               0.14031191238563992,\n",
       "               0.14027631371489327,\n",
       "               0.14025295150255293,\n",
       "               0.14021773805975285,\n",
       "               0.14018570402470198,\n",
       "               0.140149593603721,\n",
       "               0.14009985433640054,\n",
       "               0.14006119981030782,\n",
       "               0.14003182976801853,\n",
       "               0.140000544512758,\n",
       "               0.139964987252702,\n",
       "               0.13992762671854178,\n",
       "               0.13989929046056523,\n",
       "               0.13986530571484754,\n",
       "               0.13984549887885372,\n",
       "               0.13980110006972193,\n",
       "               0.13976558407486048,\n",
       "               0.13971955001567735,\n",
       "               0.139691095773846,\n",
       "               0.13965934458884802,\n",
       "               0.13962680430399296,\n",
       "               0.13959602711289984,\n",
       "               0.13956875061736038,\n",
       "               0.13955078360140794,\n",
       "               0.1395212324693135,\n",
       "               0.1394914874166521,\n",
       "               0.1394671521363924,\n",
       "               0.13944711552922892,\n",
       "               0.13942401330689383,\n",
       "               0.13938733043924798,\n",
       "               0.13936795548748163,\n",
       "               0.13932981536095262,\n",
       "               0.13930586524010427,\n",
       "               0.13928601266442842,\n",
       "               0.1392610478298087,\n",
       "               0.13923770436125366,\n",
       "               0.13921257494781888,\n",
       "               0.13919281800977,\n",
       "               0.13916664335970874,\n",
       "               0.13914009683618428,\n",
       "               0.13911115037410074,\n",
       "               0.13908892379008056,\n",
       "               0.1390572420808804,\n",
       "               0.13902560863172353,\n",
       "               0.13901676431024837,\n",
       "               0.13899053672650707,\n",
       "               0.13896310717817936,\n",
       "               0.13893024044937322,\n",
       "               0.1388802017174836,\n",
       "               0.1388501502047339,\n",
       "               0.13881533933858872,\n",
       "               0.13878572017969776,\n",
       "               0.13877051537350063,\n",
       "               0.13874405253161692,\n",
       "               0.13873303992089822,\n",
       "               0.13871209658056027,\n",
       "               0.13868508877185384,\n",
       "               0.13866600846156332,\n",
       "               0.1386463450327977,\n",
       "               0.13862644474617528,\n",
       "               0.13860084722938792,\n",
       "               0.138577370653178,\n",
       "               0.1385567784350801,\n",
       "               0.1385466444498232,\n",
       "               0.13852269145485283,\n",
       "               0.1385029150149172,\n",
       "               0.13848510504995346,\n",
       "               0.13846949528800026,\n",
       "               0.1384584285571925,\n",
       "               0.13844391243212137,\n",
       "               0.13842507419017588,\n",
       "               0.13840730331161372,\n",
       "               0.13838153326715827,\n",
       "               0.1383463701971577,\n",
       "               0.1383259651487725,\n",
       "               0.13830732996701875,\n",
       "               0.13828988481385085,\n",
       "               0.13827020096039208,\n",
       "               0.1382551670503943,\n",
       "               0.1382335031033536,\n",
       "               0.13821220569502998,\n",
       "               0.1381878223719961,\n",
       "               0.13817164396648865,\n",
       "               0.1381550512735098,\n",
       "               0.13813050478242536,\n",
       "               0.13810830184237036,\n",
       "               0.1380880724420219,\n",
       "               0.1380751897656275,\n",
       "               0.13805856440065714,\n",
       "               0.1380311861421408,\n",
       "               0.13800259229750325,\n",
       "               0.13798666800783746,\n",
       "               0.13796589764485132,\n",
       "               0.13794644316558324,\n",
       "               0.13792268394644283,\n",
       "               0.13790889415935423,\n",
       "               0.1378919428130947,\n",
       "               0.1378868830103335,\n",
       "               0.13787497480088065,\n",
       "               0.13786192435724057,\n",
       "               0.13784064026862777,\n",
       "               0.1378169141196284,\n",
       "               0.13779313403588592,\n",
       "               0.13776636564750858,\n",
       "               0.13774503746505687,\n",
       "               0.1377277387857395,\n",
       "               0.13770511218496997,\n",
       "               0.13768097015880446,\n",
       "               0.13765684393063704,\n",
       "               0.1376279556886996,\n",
       "               0.13760999495604478,\n",
       "               0.13759731793082552,\n",
       "               0.1375712740771714,\n",
       "               0.13755815944580205,\n",
       "               0.1375484091235318,\n",
       "               0.1375426207381006,\n",
       "               0.13753019131851948,\n",
       "               0.13750704657677051,\n",
       "               0.13749691127694194,\n",
       "               0.13749264866577232,\n",
       "               0.1374833582061873,\n",
       "               0.13747181164986075,\n",
       "               0.13746590979443882,\n",
       "               0.13745502502730517,\n",
       "               0.1374432703113095,\n",
       "               0.13742796269885318,\n",
       "               0.13741876916890677,\n",
       "               0.1374118154860651,\n",
       "               0.137396334060938,\n",
       "               0.13738451172119714,\n",
       "               0.1373745758659598,\n",
       "               0.13736175204492085,\n",
       "               0.13735508492162815,\n",
       "               0.13734589424327126],\n",
       "              'train-Logloss-std': [0.0005285859497856227,\n",
       "               0.0001724951039144888,\n",
       "               0.00025956986131828104,\n",
       "               0.00022496022328593243,\n",
       "               0.00017580056977787765,\n",
       "               0.000404254940537632,\n",
       "               0.00010263781081431873,\n",
       "               0.0002396043237037392,\n",
       "               0.00018537336205053955,\n",
       "               3.078236545027917e-05,\n",
       "               6.054536488274788e-05,\n",
       "               0.00018359831556467136,\n",
       "               0.00012219361960540188,\n",
       "               0.00017086153499480818,\n",
       "               0.00020493159309817086,\n",
       "               0.00021026966137538161,\n",
       "               0.00018015468961708593,\n",
       "               0.00012592144578431837,\n",
       "               0.00010886510033230686,\n",
       "               0.00013486777613450692,\n",
       "               0.0001517120456242865,\n",
       "               0.00017043539010006407,\n",
       "               0.00014277172835272417,\n",
       "               0.00010586724348178574,\n",
       "               0.00012855019253002226,\n",
       "               0.00013250272593265047,\n",
       "               0.00014365708490960213,\n",
       "               0.00014764553679245032,\n",
       "               0.00015706758987282682,\n",
       "               0.00016343054351362194,\n",
       "               0.0001250883894274553,\n",
       "               0.00012314908357763914,\n",
       "               0.00011373022629257952,\n",
       "               8.720021132652935e-05,\n",
       "               9.512227837272311e-05,\n",
       "               0.00010298573060979864,\n",
       "               0.00010210478354556449,\n",
       "               0.00011165049279775778,\n",
       "               9.919094220391805e-05,\n",
       "               0.00011476459250603898,\n",
       "               0.00010429643747693835,\n",
       "               0.00010747334385549215,\n",
       "               0.00010872582070796802,\n",
       "               0.00014759805544408986,\n",
       "               0.00013835707583457728,\n",
       "               0.00014369213401962274,\n",
       "               0.0001531440216473359,\n",
       "               0.00013794673515662837,\n",
       "               0.00015988839949567516,\n",
       "               0.00017167276068744556,\n",
       "               0.00016570646891429455,\n",
       "               0.0001687709561524094,\n",
       "               0.0001703388236619904,\n",
       "               0.00014801970688812496,\n",
       "               0.00014255119325574781,\n",
       "               0.00013368997163518083,\n",
       "               0.00014699790508239093,\n",
       "               0.0001583273981988634,\n",
       "               0.00014421152377796647,\n",
       "               0.0001414881304748178,\n",
       "               0.0001475405174773958,\n",
       "               0.00014011290550204307,\n",
       "               0.00016302141963962888,\n",
       "               0.00016506028837294404,\n",
       "               0.00015960354969256945,\n",
       "               0.00015837030470253264,\n",
       "               0.00016129532735844248,\n",
       "               0.00017576909584325756,\n",
       "               0.00015749762776877315,\n",
       "               0.00017057821534079948,\n",
       "               0.00017862188807734952,\n",
       "               0.00018632728693156094,\n",
       "               0.00018295208480445294,\n",
       "               0.00017851389874293152,\n",
       "               0.00018423079033022812,\n",
       "               0.00017470720181221545,\n",
       "               0.0001901626697785432,\n",
       "               0.00017640376277095471,\n",
       "               0.00017515635672423017,\n",
       "               0.0001675944834156824,\n",
       "               0.00018357587697156706,\n",
       "               0.00017683939379386005,\n",
       "               0.0001836948618113667,\n",
       "               0.00019138473447048303,\n",
       "               0.00017907168233227555,\n",
       "               0.00019789970337782947,\n",
       "               0.00018435570859666516,\n",
       "               0.00018607527514636314,\n",
       "               0.00018633498226311394,\n",
       "               0.0001758855477547297,\n",
       "               0.0001794105903620079,\n",
       "               0.00019684341721436135,\n",
       "               0.00019852254585758577,\n",
       "               0.0002030731157898976,\n",
       "               0.0001954318704669913,\n",
       "               0.0001984955201909476,\n",
       "               0.00021185374080156226,\n",
       "               0.00020923688937042048,\n",
       "               0.00020728690377439876,\n",
       "               0.0002181194354089805,\n",
       "               0.00023324779019839813,\n",
       "               0.00023041861747999338,\n",
       "               0.00022630355483092837,\n",
       "               0.00021378628281811104,\n",
       "               0.00020316378328769137,\n",
       "               0.0002087339823524504,\n",
       "               0.0002054782555150741,\n",
       "               0.00020174810729570747,\n",
       "               0.00018284297622297448,\n",
       "               0.00018156079277095215,\n",
       "               0.0001852505014592404,\n",
       "               0.00018831531482320075,\n",
       "               0.0001895226257879383,\n",
       "               0.00018574296058357015,\n",
       "               0.00020940967566002462,\n",
       "               0.0002060273670065533,\n",
       "               0.00020890962798296775,\n",
       "               0.00021763963831431806,\n",
       "               0.00021621835775528322,\n",
       "               0.0002152636010268415,\n",
       "               0.00020973500285833568,\n",
       "               0.0002069539105045894,\n",
       "               0.000204181371300185,\n",
       "               0.000212509182965831,\n",
       "               0.0002220682723092787,\n",
       "               0.0002218799302961182,\n",
       "               0.0002344005912472272,\n",
       "               0.00023016101737716052,\n",
       "               0.0002211406087114977,\n",
       "               0.0002254788764410636,\n",
       "               0.0002240109546059361,\n",
       "               0.00023209509987769312,\n",
       "               0.0002344647253573436,\n",
       "               0.0002319471504334184,\n",
       "               0.000238856316368723,\n",
       "               0.0002454106374958728,\n",
       "               0.00024529926536625993,\n",
       "               0.00022927148652566196,\n",
       "               0.00023205889954256708,\n",
       "               0.00019884828728649337,\n",
       "               0.00019092767080716974,\n",
       "               0.0001886237265973517,\n",
       "               0.00019387594709236407,\n",
       "               0.00019519344917595232,\n",
       "               0.00020405856922089215,\n",
       "               0.00019682226767867082,\n",
       "               0.00020467508591132303,\n",
       "               0.00019663791117165762,\n",
       "               0.00019474684810997917,\n",
       "               0.00018947796872728422,\n",
       "               0.00020531358145854428,\n",
       "               0.00019619821151415232,\n",
       "               0.00020301899479277,\n",
       "               0.00021297488576562632,\n",
       "               0.00020380194652188142,\n",
       "               0.0002174354205637758,\n",
       "               0.00020784304602385712,\n",
       "               0.00019932892111356495,\n",
       "               0.00020144450209196764,\n",
       "               0.00020832523764318948,\n",
       "               0.0002176971013620358,\n",
       "               0.00021110341340097887,\n",
       "               0.00021148644755659637,\n",
       "               0.00020928166214866553,\n",
       "               0.0001974080143002658,\n",
       "               0.0002025729334612479,\n",
       "               0.000209322297519319,\n",
       "               0.00021196267951857837,\n",
       "               0.0002159513316942829,\n",
       "               0.00019919760231606662,\n",
       "               0.0001965326132326443,\n",
       "               0.00020401979417468709,\n",
       "               0.0001977303475661469,\n",
       "               0.00019648439620748983,\n",
       "               0.0001786854586575178,\n",
       "               0.00017762549001625177,\n",
       "               0.00017113203497562922,\n",
       "               0.00017897356096562895,\n",
       "               0.00017204671894432164,\n",
       "               0.0001728103842133515,\n",
       "               0.00016827360301073754,\n",
       "               0.0001599470137579489,\n",
       "               0.00014074053987737543,\n",
       "               0.00012075480184389592,\n",
       "               0.0001179179718607274,\n",
       "               0.00011241224121733347,\n",
       "               0.0001098845986741196,\n",
       "               0.0001038341059744456,\n",
       "               9.874429818183713e-05,\n",
       "               9.884504529195414e-05,\n",
       "               9.552246400539639e-05,\n",
       "               9.185434477057187e-05,\n",
       "               0.00010214981000028083,\n",
       "               9.471013252230616e-05,\n",
       "               0.00010220054873369587,\n",
       "               0.00010819572680575705,\n",
       "               0.00011406853932196002,\n",
       "               0.00012411586485373656,\n",
       "               0.00012553951198232088,\n",
       "               0.00013564530033628817]})}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_model = catboost.CatBoostClassifier(n_estimators=200, learning_rate=0.3, eval_metric='AUC',\n",
    "                                             early_stopping_rounds=20, verbose=20, random_seed=42)\n",
    "\n",
    "params = {'depth': np.arange(4, 8, 1), 'learning_rate': np.arange(0.1, 0.5, 0.1), 'min_data_in_leaf': np.arange(3, 5, 1)}\n",
    "\n",
    "boosting_model.grid_search(params, X_train, y_train, cv=3, search_by_train_test_split=True, calc_cv_statistics=True,\n",
    "                           plot=True, refit=True, stratified=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nan_mode': 'Min',\n",
       " 'eval_metric': 'AUC',\n",
       " 'iterations': 200,\n",
       " 'sampling_frequency': 'PerTree',\n",
       " 'leaf_estimation_method': 'Newton',\n",
       " 'od_pval': 0,\n",
       " 'random_score_type': 'NormalWithModelSizeDecrease',\n",
       " 'grow_policy': 'SymmetricTree',\n",
       " 'penalties_coefficient': 1,\n",
       " 'boosting_type': 'Plain',\n",
       " 'model_shrink_mode': 'Constant',\n",
       " 'feature_border_type': 'GreedyLogSum',\n",
       " 'bayesian_matrix_reg': 0.10000000149011612,\n",
       " 'eval_fraction': 0,\n",
       " 'force_unit_auto_pair_weights': False,\n",
       " 'l2_leaf_reg': 3,\n",
       " 'random_strength': 1,\n",
       " 'od_type': 'Iter',\n",
       " 'rsm': 1,\n",
       " 'boost_from_average': False,\n",
       " 'model_size_reg': 0.5,\n",
       " 'pool_metainfo_options': {'tags': {}},\n",
       " 'subsample': 0.800000011920929,\n",
       " 'use_best_model': True,\n",
       " 'od_wait': 20,\n",
       " 'class_names': [0, 1],\n",
       " 'random_seed': 42,\n",
       " 'depth': 7,\n",
       " 'posterior_sampling': False,\n",
       " 'border_count': 254,\n",
       " 'classes_count': 0,\n",
       " 'auto_class_weights': 'None',\n",
       " 'sparse_features_conflict_fraction': 0,\n",
       " 'leaf_estimation_backtracking': 'AnyImprovement',\n",
       " 'best_model_min_trees': 1,\n",
       " 'model_shrink_rate': 0,\n",
       " 'min_data_in_leaf': 3,\n",
       " 'loss_function': 'Logloss',\n",
       " 'learning_rate': 0.30000001192092896,\n",
       " 'score_function': 'Cosine',\n",
       " 'task_type': 'CPU',\n",
       " 'leaf_estimation_iterations': 10,\n",
       " 'bootstrap_type': 'MVS',\n",
       " 'max_leaves': 128}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = boosting_model.get_all_params()\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1303f6d9f554ed293b05496ed6463aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5276175\tbest: 0.5276175 (0)\ttotal: 305ms\tremaining: 1m\n",
      "20:\ttest: 0.7306887\tbest: 0.7306887 (20)\ttotal: 8.18s\tremaining: 1m 9s\n",
      "40:\ttest: 0.7397627\tbest: 0.7397627 (40)\ttotal: 17s\tremaining: 1m 6s\n",
      "60:\ttest: 0.7437593\tbest: 0.7437593 (60)\ttotal: 25.5s\tremaining: 58.1s\n",
      "80:\ttest: 0.7457337\tbest: 0.7457337 (80)\ttotal: 33.7s\tremaining: 49.5s\n",
      "100:\ttest: 0.7474146\tbest: 0.7474146 (100)\ttotal: 41.7s\tremaining: 40.8s\n",
      "120:\ttest: 0.7485615\tbest: 0.7485615 (120)\ttotal: 49.8s\tremaining: 32.5s\n",
      "140:\ttest: 0.7490383\tbest: 0.7490383 (140)\ttotal: 58.6s\tremaining: 24.5s\n",
      "160:\ttest: 0.7495891\tbest: 0.7495891 (160)\ttotal: 1m 7s\tremaining: 16.3s\n",
      "180:\ttest: 0.7499892\tbest: 0.7499892 (180)\ttotal: 1m 16s\tremaining: 8.08s\n",
      "199:\ttest: 0.7503665\tbest: 0.7503665 (199)\ttotal: 1m 26s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.7503665149\n",
      "bestIteration = 199\n",
      "\n",
      "train:  0.7635428735697274\n",
      "test:  0.7503665148902103\n"
     ]
    }
   ],
   "source": [
    "boosting_model.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=20, plot=True)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, boosting_model.predict_proba(X_train)[:, 1]))\n",
    "print('test: ', roc_auc_score(y_test, boosting_model.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Id</th>\n",
       "      <th>Importances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pre_util_3</td>\n",
       "      <td>8.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is_zero_loans530</td>\n",
       "      <td>7.547831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pre_loans_credit_limit_2</td>\n",
       "      <td>6.612090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pre_util_6</td>\n",
       "      <td>5.342999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enc_paym_22_3</td>\n",
       "      <td>4.650208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>pre_loans3060_6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>pre_loans3060_9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>pre_loans6090_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>pre_over2limit_18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>enc_loans_account_cur_3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Feature Id  Importances\n",
       "0                  pre_util_3     8.154100\n",
       "1            is_zero_loans530     7.547831\n",
       "2    pre_loans_credit_limit_2     6.612090\n",
       "3                  pre_util_6     5.342999\n",
       "4               enc_paym_22_3     4.650208\n",
       "..                        ...          ...\n",
       "190           pre_loans3060_6     0.000000\n",
       "191           pre_loans3060_9     0.000000\n",
       "192           pre_loans6090_3     0.000000\n",
       "193         pre_over2limit_18     0.000000\n",
       "194   enc_loans_account_cur_3     0.000000\n",
       "\n",
       "[195 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boosting_model.get_feature_importance(prettified=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2001418, 175)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing unnecessary features\n",
    "df_imp = pd.DataFrame(boosting_model.get_feature_importance(prettified=True))\n",
    "new_cols = df_imp.loc[df_imp['Importances']>0]['Feature Id'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[new_cols], df['target'], stratify=df['target'], train_size=0.7)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5719056103704d2cb394d90dbf72c277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.5947286\tbest: 0.5947286 (0)\ttotal: 345ms\tremaining: 2m 52s\n",
      "50:\ttest: 0.7426050\tbest: 0.7426050 (50)\ttotal: 21s\tremaining: 3m 4s\n",
      "100:\ttest: 0.7477287\tbest: 0.7477287 (100)\ttotal: 40.5s\tremaining: 2m 39s\n",
      "150:\ttest: 0.7498418\tbest: 0.7498418 (150)\ttotal: 1m\tremaining: 2m 18s\n",
      "200:\ttest: 0.7507420\tbest: 0.7507819 (198)\ttotal: 1m 19s\tremaining: 1m 58s\n",
      "250:\ttest: 0.7513661\tbest: 0.7513809 (249)\ttotal: 1m 38s\tremaining: 1m 37s\n",
      "300:\ttest: 0.7518082\tbest: 0.7518316 (299)\ttotal: 1m 57s\tremaining: 1m 17s\n",
      "350:\ttest: 0.7521967\tbest: 0.7522053 (349)\ttotal: 2m 17s\tremaining: 58.4s\n",
      "Stopped by overfitting detector  (20 iterations wait)\n",
      "\n",
      "bestTest = 0.7522874505\n",
      "bestIteration = 357\n",
      "\n",
      "Shrink model to first 358 iterations.\n",
      "train:  0.7709935366016526\n",
      "test:  0.7522874505188494\n"
     ]
    }
   ],
   "source": [
    "boosting_model_2 = catboost.CatBoostClassifier(n_estimators=500, learning_rate=0.3, eval_metric='AUC',\n",
    "                                             early_stopping_rounds=20, verbose=20, random_seed=42,\n",
    "                                            depth=7, min_data_in_leaf=3)\n",
    "\n",
    "boosting_model_2.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=50, plot=True)\n",
    "\n",
    "print('train: ', roc_auc_score(y_train, boosting_model_2.predict_proba(X_train)[:, 1]))\n",
    "print('test: ', roc_auc_score(y_test, boosting_model_2.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64f12946c2db40e2ab072cf1fe690674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_loss 0.2359727960617388\n",
      "train:  0.650847376082941\n",
      "curr_loss 0.15126705807239854\n",
      "train:  0.7086950887309611\n",
      "curr_loss 0.14611232458655513\n",
      "train:  0.7185658752234406\n",
      "curr_loss 0.14462867305053406\n",
      "train:  0.7272222836191474\n",
      "curr_loss 0.14372804968511288\n",
      "train:  0.7334430346906636\n",
      "curr_loss 0.14292318903984716\n",
      "train:  0.7384723728444073\n",
      "curr_loss 0.14242374682011297\n",
      "train:  0.7416209782891743\n",
      "curr_loss 0.14190471105611147\n",
      "train:  0.7445734660276564\n",
      "curr_loss 0.14161245480402193\n",
      "train:  0.7448674024342689\n",
      "curr_loss 0.14104232544181358\n",
      "train:  0.7480668140633159\n",
      "curr_loss 0.14079206677811656\n",
      "train:  0.7517071718017255\n",
      "curr_loss 0.14047291645066656\n",
      "train:  0.753422058242482\n",
      "curr_loss 0.1404433159982387\n",
      "train:  0.7544725121163185\n",
      "curr_loss 0.14023735140686605\n",
      "train:  0.7549749985795071\n",
      "curr_loss 0.13991685567507103\n",
      "train:  0.7564385906765015\n",
      "curr_loss 0.13992527623971304\n",
      "train:  0.756995827084343\n",
      "curr_loss 0.13977326456438843\n",
      "train:  0.7584081423670274\n",
      "curr_loss 0.13955010603464657\n",
      "train:  0.7583172199828991\n",
      "curr_loss 0.13958323061169675\n",
      "train:  0.7581179470362166\n",
      "curr_loss 0.13954505680212334\n",
      "train:  0.7598883826016462\n",
      "curr_loss 0.1391196316154442\n",
      "train:  0.7591973926684533\n",
      "curr_loss 0.13944764950530447\n",
      "train:  0.7602507088775607\n",
      "curr_loss 0.13922330848316647\n",
      "train:  0.7609142693004579\n",
      "curr_loss 0.13915357883296794\n",
      "train:  0.7612420935519438\n",
      "curr_loss 0.13896863974297224\n",
      "train:  0.7618834843111338\n",
      "curr_loss 0.13902950946667894\n",
      "train:  0.7630771789887341\n",
      "curr_loss 0.13891405749380292\n",
      "train:  0.7624646911721998\n",
      "curr_loss 0.13870700207812275\n",
      "train:  0.7634678747703149\n",
      "curr_loss 0.1386084223342179\n",
      "train:  0.7631671315315206\n",
      "curr_loss 0.1385279467227447\n",
      "train:  0.763780192662308\n",
      "train:  0.7638222391207407\n",
      "test:  0.7528056057124267\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "X_train_t =  torch.FloatTensor(X_train.values)\n",
    "y_train_t =  torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_t =  torch.FloatTensor(X_test.values)\n",
    "y_test_t =  torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(X_train_t, y_train_t)), batch_size=10000, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(X_test_t, y_test_t)), batch_size=10000, shuffle=False)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.fc1 = nn.Linear(194, 100, bias=True)\n",
    "        self.fc2 = nn.Linear(100, 50, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(50, 20, bias=True)\n",
    "        self.fc4 = nn.Linear(20, 5, bias=True)\n",
    "        self.fc5 = nn.Linear(5, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "nn_model = Net()\n",
    "\n",
    "def train_stochastic(model, loader, criterion, optimazer, num_epoch, X_train_t, y_train_t):\n",
    "    \n",
    "    losses = []\n",
    "    roc_auc_metrics = []\n",
    "    for t in tqdm(range(num_epoch)):\n",
    "        epoch_loss = []\n",
    "        metrics = []\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                     \n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        print(\"curr_loss\", np.mean(epoch_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn_prediction_train = model(X_train_t).tolist()\n",
    "            roc_auc_metrics.append(roc_auc_score(y_train_t, nn_prediction_train))\n",
    "            print('train: ', roc_auc_score(y_train_t, nn_prediction_train))\n",
    "\n",
    "    return model, losses, roc_auc_metrics\n",
    "    \n",
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "\n",
    "model, losses, metrics = train_stochastic(nn_model, train_loader, loss, optimizer, 30, X_train_t, y_train_t)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_prediction_train = model(X_train_t)\n",
    "    nn_prediction_train = nn_prediction_train.tolist() \n",
    "    \n",
    "    nn_prediction_test = model(X_test_t)\n",
    "    nn_prediction_test = nn_prediction_test.tolist()\n",
    "\n",
    "    print('train: ', roc_auc_score(y_train, nn_prediction_train))\n",
    "    print('test: ', roc_auc_score(y_test, nn_prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_trainig(losses, metrics):\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.plot(losses, label=\"train_loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.plot(metrics, label=\"train_roc-auc\")\n",
    "    plt.legend()\n",
    "\n",
    "plot_trainig(losses, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with Elu|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b646472c6a7f433e997bbd77f346ed74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_loss 0.20725272425371616\n",
      "train:  0.5853241595168402\n",
      "curr_loss 0.14964283980540374\n",
      "train:  0.7053703824175186\n",
      "curr_loss 0.14452455315127302\n",
      "train:  0.7267819797173181\n",
      "curr_loss 0.14299860559589234\n",
      "train:  0.7355550818979815\n",
      "curr_loss 0.1422671944792591\n",
      "train:  0.7401445370869421\n",
      "curr_loss 0.1417988324491539\n",
      "train:  0.7425750372918815\n",
      "curr_loss 0.14176862445933308\n",
      "train:  0.744297770226402\n",
      "curr_loss 0.14149810496106077\n",
      "train:  0.7466402951993637\n",
      "curr_loss 0.14127213470823136\n",
      "train:  0.7484027328135977\n",
      "curr_loss 0.14114504904296268\n",
      "train:  0.748767237877688\n",
      "curr_loss 0.1407912692323846\n",
      "train:  0.7505554796956522\n",
      "curr_loss 0.14061302339556206\n",
      "train:  0.7509905697288497\n",
      "curr_loss 0.14046922600387934\n",
      "train:  0.7520235828743655\n",
      "curr_loss 0.14036011614313174\n",
      "train:  0.7529818906469514\n",
      "curr_loss 0.14020013390340616\n",
      "train:  0.7543501213043626\n",
      "curr_loss 0.14006019461510785\n",
      "train:  0.7549502494902925\n",
      "curr_loss 0.13983309569198693\n",
      "train:  0.7551907485964986\n",
      "curr_loss 0.14004789982269059\n",
      "train:  0.756256787247807\n",
      "curr_loss 0.139924663253388\n",
      "train:  0.7569221071314386\n",
      "curr_loss 0.13989602870757306\n",
      "train:  0.7577083009029756\n",
      "curr_loss 0.1397184829287861\n",
      "train:  0.7580877760943167\n",
      "curr_loss 0.1396340766356359\n",
      "train:  0.7582319010243237\n",
      "curr_loss 0.13949959840050977\n",
      "train:  0.7583883890233117\n",
      "curr_loss 0.13943851453747916\n",
      "train:  0.7578598279102953\n",
      "curr_loss 0.13940845622648648\n",
      "train:  0.7598181829247229\n",
      "curr_loss 0.1393945441762013\n",
      "train:  0.7600269875972114\n",
      "curr_loss 0.13923171503626885\n",
      "train:  0.7583672904875418\n",
      "curr_loss 0.139236953943523\n",
      "train:  0.7605697525250327\n",
      "curr_loss 0.1392058562209357\n",
      "train:  0.7610846584868172\n",
      "curr_loss 0.13913167553458047\n",
      "train:  0.761400122935241\n",
      "train:  0.7614684820675409\n",
      "test:  0.756042398792014\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "X_train_t =  torch.FloatTensor(X_train.values)\n",
    "y_train_t =  torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_t =  torch.FloatTensor(X_test.values)\n",
    "y_test_t =  torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(X_train_t, y_train_t)), batch_size=10000, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(X_test_t, y_test_t)), batch_size=10000, shuffle=False)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.fc1 = nn.Linear(194, 100, bias=True)\n",
    "        self.fc2 = nn.Linear(100, 50, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(50, 20, bias=True)\n",
    "        self.fc4 = nn.Linear(20, 5, bias=True)\n",
    "        self.fc5 = nn.Linear(5, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "nn_model = Net()\n",
    "\n",
    "def train_stochastic(model, loader, criterion, optimazer, num_epoch, X_train_t, y_train_t):\n",
    "    \n",
    "    losses = []\n",
    "    roc_auc_metrics = []\n",
    "    for t in tqdm(range(num_epoch)):\n",
    "        epoch_loss = []\n",
    "        metrics = []\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                     \n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        print(\"curr_loss\", np.mean(epoch_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn_prediction_train = model(X_train_t).tolist()\n",
    "            roc_auc_metrics.append(roc_auc_score(y_train_t, nn_prediction_train))\n",
    "            print('train: ', roc_auc_score(y_train_t, nn_prediction_train))\n",
    "\n",
    "    return model, losses, roc_auc_metrics\n",
    "    \n",
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "#optimizer = torch.optim.SGD(nn_model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "model, losses, metrics = train_stochastic(nn_model, train_loader, loss, optimizer, 30, X_train_t, y_train_t)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_prediction_train = model(X_train_t)\n",
    "    nn_prediction_train = nn_prediction_train.tolist() \n",
    "    \n",
    "    nn_prediction_test = model(X_test_t)\n",
    "    nn_prediction_test = nn_prediction_test.tolist()\n",
    "\n",
    "    print('train: ', roc_auc_score(y_train, nn_prediction_train))\n",
    "    print('test: ', roc_auc_score(y_test, nn_prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_trainig(losses, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814f48058a7b486ca5188606275c7627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_loss 0.6261395147783839\n",
      "train:  0.7461130751142657\n",
      "curr_loss 0.5893325449815437\n",
      "train:  0.7556742115033024\n",
      "curr_loss 0.5846785407754319\n",
      "train:  0.759589443777575\n",
      "curr_loss 0.5820759955923356\n",
      "train:  0.7631409501231562\n",
      "curr_loss 0.578950027624766\n",
      "train:  0.7651774079498652\n",
      "curr_loss 0.5760291064556559\n",
      "train:  0.768361973969754\n",
      "curr_loss 0.5728049942510045\n",
      "train:  0.7703245436397607\n",
      "curr_loss 0.5712140531682256\n",
      "train:  0.7716228418794708\n",
      "curr_loss 0.5699308903063115\n",
      "train:  0.7748508450319875\n",
      "curr_loss 0.567281778772079\n",
      "train:  0.7767721408399372\n",
      "curr_loss 0.5656014986299164\n",
      "train:  0.7770813838460681\n",
      "curr_loss 0.5626751116259181\n",
      "train:  0.7794165005943171\n",
      "curr_loss 0.5603623301235597\n",
      "train:  0.7798651930605667\n",
      "curr_loss 0.5590115032385831\n",
      "train:  0.7821612745849612\n",
      "curr_loss 0.5576398660294453\n",
      "train:  0.7840197429473298\n",
      "train:  0.784048614441595\n",
      "test:  0.7434524943750535\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "X_train_t =  torch.FloatTensor(X_train.values)\n",
    "y_train_t =  torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_t =  torch.FloatTensor(X_test.values)\n",
    "y_test_t =  torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "counter = Counter(i.item() for i, in y_train_t)\n",
    "weights = [1/counter.get(i.item()) for i, in y_train_t]\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(weights))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(X_train_t, y_train_t)), batch_size=10000, sampler=sampler)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(X_test_t, y_test_t)), batch_size=10000, shuffle=False)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.fc1 = nn.Linear(194, 100, bias=True)\n",
    "        self.fc2 = nn.Linear(100, 50, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(50, 20, bias=True)\n",
    "        self.fc4 = nn.Linear(20, 5, bias=True)\n",
    "        self.fc5 = nn.Linear(5, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "nn_model = Net()\n",
    "\n",
    "def train_stochastic(model, loader, criterion, optimazer, num_epoch, X_train_t, y_train_t):\n",
    "    \n",
    "    losses = []\n",
    "    roc_auc_metrics = []\n",
    "    for t in tqdm(range(num_epoch)):\n",
    "        epoch_loss = []\n",
    "        metrics = []\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                     \n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        print(\"curr_loss\", np.mean(epoch_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn_prediction_train = model(X_train_t).tolist()\n",
    "            roc_auc_metrics.append(roc_auc_score(y_train_t, nn_prediction_train))\n",
    "            print('train: ', roc_auc_score(y_train_t, nn_prediction_train))\n",
    "\n",
    "    return model, losses, roc_auc_metrics\n",
    "    \n",
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "\n",
    "model, losses, metrics = train_stochastic(nn_model, train_loader, loss, optimizer, 15, X_train_t, y_train_t)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_prediction_train = model(X_train_t)\n",
    "    nn_prediction_train = nn_prediction_train.tolist() \n",
    "    \n",
    "    nn_prediction_test = model(X_test_t)\n",
    "    nn_prediction_test = nn_prediction_test.tolist()\n",
    "\n",
    "    print('train: ', roc_auc_score(y_train, nn_prediction_train))\n",
    "    print('test: ', roc_auc_score(y_test, nn_prediction_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7)\n",
    "\n",
    "os = SMOTE(k_neighbors=4)\n",
    "X_train_2, y_train_2 = os.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "X_train_t =  torch.FloatTensor(X_train_2.values)\n",
    "y_train_t =  torch.FloatTensor(y_train_2.values).view(-1, 1)\n",
    "X_test_t =  torch.FloatTensor(X_test.values)\n",
    "y_test_t =  torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(list(zip(X_train_t, y_train_t)), batch_size=10000, shuffle=True, drop_last=False)\n",
    "test_loader = torch.utils.data.DataLoader(list(zip(X_test_t, y_test_t)), batch_size=10000, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8598f2b1c34dc8b90806199586bce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_loss 0.18588580737382637\n",
      "train:  0.9859615008083138\n",
      "curr_loss 0.10496606660398794\n",
      "train:  0.9873833468907136\n",
      "curr_loss 0.10141070694735013\n",
      "train:  0.9879147533370098\n",
      "curr_loss 0.09942607012205791\n",
      "train:  0.9880549809861758\n",
      "curr_loss 0.09811511888636826\n",
      "train:  0.9884078885462964\n",
      "curr_loss 0.09705086202973529\n",
      "train:  0.9884366564742613\n",
      "curr_loss 0.09638362457020296\n",
      "train:  0.9886910615271381\n",
      "curr_loss 0.0954420953178344\n",
      "train:  0.988699184605543\n",
      "curr_loss 0.09439403619713733\n",
      "train:  0.9887138009240545\n",
      "curr_loss 0.09404045393584305\n",
      "train:  0.9889618874597673\n",
      "curr_loss 0.09332302158669487\n",
      "train:  0.989074011020953\n",
      "curr_loss 0.09308623514799257\n",
      "train:  0.989155689801351\n",
      "curr_loss 0.09225401382681002\n",
      "train:  0.9892605796047501\n",
      "curr_loss 0.09227653563871903\n",
      "train:  0.9891110219675068\n",
      "curr_loss 0.0917129758841942\n",
      "train:  0.9892841857956931\n",
      "train:  0.9892759631494056\n",
      "test:  0.7370575638135158\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()        \n",
    "        self.fc1 = nn.Linear(194, 100, bias=True)\n",
    "        self.fc2 = nn.Linear(100, 50, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(50, 20, bias=True)\n",
    "        self.fc4 = nn.Linear(20, 5, bias=True)\n",
    "        self.fc5 = nn.Linear(5, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = F.leaky_relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "nn_model = Net()\n",
    "\n",
    "def train_stochastic(model, loader, criterion, optimazer, num_epoch, X_train_t, y_train_t):\n",
    "    \n",
    "    losses = []\n",
    "    roc_auc_metrics = []\n",
    "    for t in tqdm(range(num_epoch)):\n",
    "        epoch_loss = []\n",
    "        metrics = []\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "                     \n",
    "        losses.append(np.mean(epoch_loss))\n",
    "        print(\"curr_loss\", np.mean(epoch_loss))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            nn_prediction_train = model(X_train_t).tolist()\n",
    "            roc_auc_metrics.append(roc_auc_score(y_train_t, nn_prediction_train))\n",
    "            print('train: ', roc_auc_score(y_train_t, nn_prediction_train))\n",
    "\n",
    "    return model, losses, roc_auc_metrics\n",
    "    \n",
    "loss = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=1e-3)\n",
    "\n",
    "model, losses, metrics = train_stochastic(nn_model, train_loader, loss, optimizer, 15, X_train_t, y_train_t)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn_prediction_train = model(X_train_t)\n",
    "    nn_prediction_train = nn_prediction_train.tolist() \n",
    "    \n",
    "    nn_prediction_test = model(X_test_t)\n",
    "    nn_prediction_test = nn_prediction_test.tolist()\n",
    "\n",
    "    print('train: ', roc_auc_score(y_train_t, nn_prediction_train))\n",
    "    print('test: ', roc_auc_score(y_test, nn_prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x900 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_trainig(losses, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble with best RELU model (+dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "X_train_t =  torch.FloatTensor(X_train.values)\n",
    "y_train_t =  torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_t =  torch.FloatTensor(X_test.values)\n",
    "y_test_t =  torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "X_train_nump = X_train.to_numpy()\n",
    "y_train_nump =  y_train.to_numpy()\n",
    "X_test_nump =  X_test.to_numpy()\n",
    "y_test_nump =  y_test.to_numpy()\n",
    "\n",
    "class Net_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_model, self).__init__()  \n",
    "        self.fc1 = nn.Linear(194, 100, bias=True)\n",
    "        self.fc2 = nn.Linear(100, 50, bias=True)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc3 = nn.Linear(50, 20, bias=True)\n",
    "        self.fc4 = nn.Linear(20, 5, bias=True)\n",
    "        self.fc5 = nn.Linear(5, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "        \n",
    "net = Net_model()\n",
    "\n",
    "class PytorchModel(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, net_type, net_params, optimizer_type, optimizer_params, loss_fn,\n",
    "               batch_size=10000, auc_tol=0.04, tol_epochs=10):\n",
    "        self.net_type = net_type\n",
    "        self.net_params = net_params\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.auc_tol = auc_tol \n",
    "        self.tol_epochs = tol_epochs\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        super().__init__()\n",
    "        self.net = self.net_type(**self.net_params)\n",
    "        self.optimizer = self.optimizer_type(self.net.parameters(), **self.optimizer_params)\n",
    "        \n",
    "        uniq_classes = np.sort(np.unique(y))\n",
    "        self.classes_ = uniq_classes\n",
    "    \n",
    "        X_t = torch.FloatTensor(X)\n",
    "        y_t = torch.FloatTensor(y).view(-1, 1)\n",
    "        train_dataset = TensorDataset(X_t, y_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        metrics = []\n",
    "        epoch = 0\n",
    "        keep_training = True   \n",
    "        while keep_training:\n",
    "            self.net.train()\n",
    "            epoch_loss = []\n",
    "\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                y_pred = self.net(X_batch)\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                epoch_loss.append(loss.item())\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()               \n",
    "            print(\"curr_loss\", np.mean(epoch_loss))\n",
    "            with torch.no_grad():\n",
    "                nn_prediction_train = self.net(X_t).tolist()\n",
    "                roc_auc_sc = roc_auc_score(y_t, nn_prediction_train)\n",
    "                print('train: ', roc_auc_score(y_t, nn_prediction_train))\n",
    "                metrics.append(roc_auc_sc)\n",
    "                \n",
    "            if len(metrics) > self.tol_epochs:\n",
    "                metrics.pop(0)\n",
    "            if len(metrics) == self.tol_epochs:\n",
    "                roc_auc_diff = max(metrics) - min(metrics)\n",
    "                if roc_auc_diff <= self.auc_tol:\n",
    "                    print('-----------------------------------------------------------------------------------------')\n",
    "                    keep_training = False\n",
    "    \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.tensor(X.astype(np.float32))\n",
    "        self.net.eval()\n",
    "        preds_proba = self.net(X_tensor).detach().numpy()\n",
    "        return preds_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds_proba = self.predict_proba(X)\n",
    "        predictions = np.amax(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(predictions):\n",
    "    predictions = np.amax(predictions, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_loss 0.20490523312815387\n",
      "train:  0.6500348038281465\n",
      "curr_loss 0.15001269574485607\n",
      "train:  0.7113328796444929\n",
      "curr_loss 0.14520823162289995\n",
      "train:  0.724146813570458\n",
      "curr_loss 0.14348693340275417\n",
      "train:  0.7325087481859762\n",
      "curr_loss 0.1426303187264732\n",
      "train:  0.7381164592895236\n",
      "curr_loss 0.14194670389985564\n",
      "train:  0.7429805433149134\n",
      "curr_loss 0.1413740165049757\n",
      "train:  0.7447568774055168\n",
      "curr_loss 0.14082292133747643\n",
      "train:  0.751394389321732\n",
      "curr_loss 0.14031384581357093\n",
      "train:  0.7538891590298382\n",
      "curr_loss 0.13969195207849663\n",
      "train:  0.7578323201673329\n",
      "curr_loss 0.13900843314567016\n",
      "train:  0.7602754773522206\n",
      "curr_loss 0.13881815006187306\n",
      "train:  0.7629383350848101\n",
      "curr_loss 0.13838796480674648\n",
      "train:  0.7663456646568453\n",
      "curr_loss 0.1380709820422367\n",
      "train:  0.7665710123429721\n",
      "curr_loss 0.13769617105894422\n",
      "train:  0.7701457622902853\n",
      "curr_loss 0.13728609466137578\n",
      "train:  0.7683728126928824\n",
      "curr_loss 0.1372589359977352\n",
      "train:  0.7736943185266442\n",
      "curr_loss 0.1370307042156879\n",
      "train:  0.7724921845808903\n",
      "curr_loss 0.13659237325191498\n",
      "train:  0.7740847640899382\n",
      "curr_loss 0.1365342793328252\n",
      "train:  0.7759456337334001\n",
      "curr_loss 0.13617723887387792\n",
      "train:  0.7779782635062051\n",
      "curr_loss 0.13602610021384795\n",
      "train:  0.7774305052648744\n",
      "curr_loss 0.13574934335638636\n",
      "train:  0.779294247630709\n",
      "curr_loss 0.13538188172217033\n",
      "train:  0.7814748573010141\n",
      "curr_loss 0.13548952229877018\n",
      "train:  0.7808536490642518\n",
      "curr_loss 0.1350378394126892\n",
      "train:  0.782199877982773\n",
      "curr_loss 0.13483021693739725\n",
      "train:  0.7817075715439561\n",
      "curr_loss 0.1347585002803684\n",
      "train:  0.7828066057160019\n",
      "curr_loss 0.13465338689622594\n",
      "train:  0.784488749584234\n",
      "curr_loss 0.13430307720282778\n",
      "train:  0.7840274407540619\n",
      "curr_loss 0.1342829267925291\n",
      "train:  0.7870530155165455\n",
      "curr_loss 0.13417014112668252\n",
      "train:  0.785387070341544\n",
      "curr_loss 0.13378264788371413\n",
      "train:  0.7868291159236505\n",
      "curr_loss 0.13373750225821537\n",
      "train:  0.7879761925100615\n",
      "curr_loss 0.13374598042585364\n",
      "train:  0.7886289698530702\n",
      "curr_loss 0.13356542913474848\n",
      "train:  0.7877039858579882\n",
      "curr_loss 0.1332749671010829\n",
      "train:  0.7878515676910929\n",
      "curr_loss 0.1333776611964501\n",
      "train:  0.7899767632628516\n",
      "curr_loss 0.13295190570069781\n",
      "train:  0.7895278635312495\n",
      "curr_loss 0.13278185754124797\n",
      "train:  0.79003534057079\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.23412493597808762\n",
      "train:  0.6612656720668116\n",
      "curr_loss 0.15051360554363005\n",
      "train:  0.7052729495359642\n",
      "curr_loss 0.14677073091120268\n",
      "train:  0.7156986780375808\n",
      "curr_loss 0.14490661151077025\n",
      "train:  0.7266787321862207\n",
      "curr_loss 0.14379993081092834\n",
      "train:  0.7270608757069426\n",
      "curr_loss 0.14299447441575538\n",
      "train:  0.7400157771257065\n",
      "curr_loss 0.142160928738651\n",
      "train:  0.7462249171033937\n",
      "curr_loss 0.14109894203309395\n",
      "train:  0.7491918012276105\n",
      "curr_loss 0.14058301764637676\n",
      "train:  0.7526304342038095\n",
      "curr_loss 0.14045620490959035\n",
      "train:  0.7551186862605764\n",
      "curr_loss 0.1400258125950448\n",
      "train:  0.7582624231468015\n",
      "curr_loss 0.1396337969608568\n",
      "train:  0.7591955052568764\n",
      "curr_loss 0.1394131803393957\n",
      "train:  0.761313050651802\n",
      "curr_loss 0.13908906432971432\n",
      "train:  0.7613136547635396\n",
      "curr_loss 0.1389895802113547\n",
      "train:  0.7648184558698121\n",
      "curr_loss 0.13870602534778081\n",
      "train:  0.7649869560904179\n",
      "curr_loss 0.1385399800776249\n",
      "train:  0.7668472667731329\n",
      "curr_loss 0.13831846441943846\n",
      "train:  0.7679052372195045\n",
      "curr_loss 0.13800023777864465\n",
      "train:  0.7694848105488618\n",
      "curr_loss 0.13792523452595098\n",
      "train:  0.7690028532139797\n",
      "curr_loss 0.1376143117523312\n",
      "train:  0.7704465642343065\n",
      "curr_loss 0.13751200975766822\n",
      "train:  0.7716496640309343\n",
      "curr_loss 0.13730493491858392\n",
      "train:  0.7729232282848166\n",
      "curr_loss 0.13701833258220805\n",
      "train:  0.7738223384701342\n",
      "curr_loss 0.13678678362375468\n",
      "train:  0.7735553640776006\n",
      "curr_loss 0.13682175505517133\n",
      "train:  0.7747496275757252\n",
      "curr_loss 0.13660841160894033\n",
      "train:  0.7736481685345471\n",
      "curr_loss 0.13649916311549903\n",
      "train:  0.7763217952691617\n",
      "curr_loss 0.13625504395261925\n",
      "train:  0.7765151405679154\n",
      "curr_loss 0.13620222823240272\n",
      "train:  0.7786105596802813\n",
      "curr_loss 0.13587910889541333\n",
      "train:  0.7783610651607136\n",
      "curr_loss 0.1358660592220316\n",
      "train:  0.7800471594493138\n",
      "curr_loss 0.1356115127677348\n",
      "train:  0.7800941751167937\n",
      "curr_loss 0.13564038584333155\n",
      "train:  0.7814333925342085\n",
      "curr_loss 0.13526814872056098\n",
      "train:  0.7804787080671787\n",
      "curr_loss 0.135364753689932\n",
      "train:  0.782086151832615\n",
      "curr_loss 0.13504398851400584\n",
      "train:  0.7830713069871682\n",
      "curr_loss 0.13501796018869722\n",
      "train:  0.782607000386878\n",
      "curr_loss 0.13481357923491083\n",
      "train:  0.7827314574811299\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.2577125217339293\n",
      "train:  0.6342602782272696\n",
      "curr_loss 0.15550401703042177\n",
      "train:  0.6929512387664917\n",
      "curr_loss 0.14883031351352805\n",
      "train:  0.7019282212971277\n",
      "curr_loss 0.14709896775917033\n",
      "train:  0.7155293191669801\n",
      "curr_loss 0.14523246068859574\n",
      "train:  0.7310176380884711\n",
      "curr_loss 0.14367100837367092\n",
      "train:  0.7381667928079393\n",
      "curr_loss 0.14262476711723934\n",
      "train:  0.7453710091968897\n",
      "curr_loss 0.14179098569042053\n",
      "train:  0.7481136876056671\n",
      "curr_loss 0.14125494740495634\n",
      "train:  0.752365719835373\n",
      "curr_loss 0.14078053098116347\n",
      "train:  0.7556706199110901\n",
      "curr_loss 0.14045422231380025\n",
      "train:  0.7573509087112833\n",
      "curr_loss 0.1401992737134891\n",
      "train:  0.7590653760165996\n",
      "curr_loss 0.14006777074354798\n",
      "train:  0.7610008018488641\n",
      "curr_loss 0.13981957348128457\n",
      "train:  0.7626685135394926\n",
      "curr_loss 0.1397257086204652\n",
      "train:  0.7636159107756936\n",
      "curr_loss 0.13927505009654742\n",
      "train:  0.7645280054956587\n",
      "curr_loss 0.13904834542404954\n",
      "train:  0.7660852982816229\n",
      "curr_loss 0.13887515552897953\n",
      "train:  0.7659112870922961\n",
      "curr_loss 0.1389263658900166\n",
      "train:  0.766388013424487\n",
      "curr_loss 0.1385741071840424\n",
      "train:  0.7685245353565011\n",
      "curr_loss 0.13867582425252714\n",
      "train:  0.7693329490995683\n",
      "curr_loss 0.13811025238452265\n",
      "train:  0.769525362198892\n",
      "curr_loss 0.13806398186962404\n",
      "train:  0.77172574057122\n",
      "curr_loss 0.13794484919873043\n",
      "train:  0.7724691353122123\n",
      "curr_loss 0.13768525254815372\n",
      "train:  0.7728552654214194\n",
      "curr_loss 0.13756178931069019\n",
      "train:  0.7748248207619913\n",
      "curr_loss 0.13741787578632583\n",
      "train:  0.7747687060115842\n",
      "curr_loss 0.13724917866549088\n",
      "train:  0.775571636050181\n",
      "curr_loss 0.13709863671912484\n",
      "train:  0.7746238069580013\n",
      "curr_loss 0.13702622056007385\n",
      "train:  0.7759762354936671\n",
      "curr_loss 0.13691875049427374\n",
      "train:  0.7761369118542892\n",
      "curr_loss 0.13692509623902355\n",
      "train:  0.776662049243124\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.24040638407071432\n",
      "train:  0.6675935795418094\n",
      "curr_loss 0.1504861785478853\n",
      "train:  0.7024084474676484\n",
      "curr_loss 0.14726379943724296\n",
      "train:  0.7152187285404906\n",
      "curr_loss 0.14557401443002235\n",
      "train:  0.721046083283629\n",
      "curr_loss 0.14504083635202095\n",
      "train:  0.7250246316686417\n",
      "curr_loss 0.14424263815678173\n",
      "train:  0.7304839417733298\n",
      "curr_loss 0.1438670969424556\n",
      "train:  0.7348831842531817\n",
      "curr_loss 0.14301700778861545\n",
      "train:  0.740449095458123\n",
      "curr_loss 0.14196970628861763\n",
      "train:  0.7411427163419276\n",
      "curr_loss 0.14152947446303582\n",
      "train:  0.7482283929598782\n",
      "curr_loss 0.14099539742244416\n",
      "train:  0.7519101432341057\n",
      "curr_loss 0.14045872743153454\n",
      "train:  0.755196304243902\n",
      "curr_loss 0.13991847878961422\n",
      "train:  0.7582569364282701\n",
      "curr_loss 0.13967345262048256\n",
      "train:  0.7593348779706033\n",
      "curr_loss 0.1392936525282575\n",
      "train:  0.7626600607664051\n",
      "curr_loss 0.13901310002625877\n",
      "train:  0.7623990739837503\n",
      "curr_loss 0.13876498053174707\n",
      "train:  0.7663297234774227\n",
      "curr_loss 0.13856129348278046\n",
      "train:  0.7670860144027265\n",
      "curr_loss 0.13818340763375533\n",
      "train:  0.7687504312677884\n",
      "curr_loss 0.13791095042851434\n",
      "train:  0.7698186018629279\n",
      "curr_loss 0.1376358017102996\n",
      "train:  0.7708935333170712\n",
      "curr_loss 0.1374342615776394\n",
      "train:  0.7724793237589369\n",
      "curr_loss 0.13732032307344882\n",
      "train:  0.7733952770628205\n",
      "curr_loss 0.13691146037916638\n",
      "train:  0.7712605211216117\n",
      "curr_loss 0.13697693904685737\n",
      "train:  0.7749517863773507\n",
      "curr_loss 0.13674493159969053\n",
      "train:  0.7757860715130611\n",
      "curr_loss 0.1364795278702209\n",
      "train:  0.7750273645949285\n",
      "curr_loss 0.13636592527230582\n",
      "train:  0.7775914241960604\n",
      "curr_loss 0.1359653780190506\n",
      "train:  0.7781152843442664\n",
      "curr_loss 0.13580245053886775\n",
      "train:  0.7807711609885742\n",
      "curr_loss 0.13577433212183007\n",
      "train:  0.7785173764478295\n",
      "curr_loss 0.13562504127992325\n",
      "train:  0.7810867571759702\n",
      "curr_loss 0.13535667199697068\n",
      "train:  0.7823179961434388\n",
      "curr_loss 0.1350121778338703\n",
      "train:  0.7810153850195226\n",
      "curr_loss 0.13499323660461462\n",
      "train:  0.7822695762169385\n",
      "curr_loss 0.13475787572896303\n",
      "train:  0.7853833580277465\n",
      "curr_loss 0.13450060117600568\n",
      "train:  0.7812122231858235\n",
      "curr_loss 0.1347258043526417\n",
      "train:  0.7864873003575874\n",
      "curr_loss 0.1343406724618442\n",
      "train:  0.7847158998281722\n",
      "curr_loss 0.1342599903469655\n",
      "train:  0.78512083131748\n",
      "curr_loss 0.1341165435032465\n",
      "train:  0.7866337869085088\n",
      "curr_loss 0.13382283942912943\n",
      "train:  0.7849515419286339\n",
      "curr_loss 0.13375940981937284\n",
      "train:  0.7882112357241692\n",
      "curr_loss 0.1335553587298488\n",
      "train:  0.7873724649061433\n",
      "curr_loss 0.13381456828384258\n",
      "train:  0.7887638073535161\n",
      "curr_loss 0.13360361873510465\n",
      "train:  0.7894495697586856\n",
      "curr_loss 0.13348472874556014\n",
      "train:  0.7884025981928697\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.2598132094488808\n",
      "train:  0.6480779307050423\n",
      "curr_loss 0.15281293960056494\n",
      "train:  0.699120871537804\n",
      "curr_loss 0.14805062826889664\n",
      "train:  0.709751452372676\n",
      "curr_loss 0.14534283618428814\n",
      "train:  0.7254023758069604\n",
      "curr_loss 0.14344244609721266\n",
      "train:  0.7355534339374725\n",
      "curr_loss 0.1424981497265213\n",
      "train:  0.7435565549879706\n",
      "curr_loss 0.141404670788281\n",
      "train:  0.7483421073167388\n",
      "curr_loss 0.14076611911182974\n",
      "train:  0.7521202833790013\n",
      "curr_loss 0.14032512969935118\n",
      "train:  0.7551536270192645\n",
      "curr_loss 0.13998453393207855\n",
      "train:  0.7578626613331084\n",
      "curr_loss 0.13974105348041402\n",
      "train:  0.759401080246811\n",
      "curr_loss 0.13935772614989114\n",
      "train:  0.7600739122621891\n",
      "curr_loss 0.13922246067381616\n",
      "train:  0.7630360072641085\n",
      "curr_loss 0.1389485097198344\n",
      "train:  0.764300006323942\n",
      "curr_loss 0.13855915757554088\n",
      "train:  0.7650317694329373\n",
      "curr_loss 0.1383495118784074\n",
      "train:  0.7669626079250551\n",
      "curr_loss 0.13809503474045748\n",
      "train:  0.7659093571133687\n",
      "curr_loss 0.13809482849652494\n",
      "train:  0.7685696373323392\n",
      "curr_loss 0.13789165668671405\n",
      "train:  0.7695577530321491\n",
      "curr_loss 0.1376235954722955\n",
      "train:  0.7688866126598979\n",
      "curr_loss 0.13757805415053867\n",
      "train:  0.7718445818912621\n",
      "curr_loss 0.1374323606046278\n",
      "train:  0.7731313275965628\n",
      "curr_loss 0.13714470061941528\n",
      "train:  0.7740866707235099\n",
      "curr_loss 0.13698898455989894\n",
      "train:  0.7751020110622886\n",
      "curr_loss 0.13670544181741887\n",
      "train:  0.7742585037043918\n",
      "curr_loss 0.1365494999422956\n",
      "train:  0.7752516505601047\n",
      "curr_loss 0.13629477915923988\n",
      "train:  0.7770892524727036\n",
      "curr_loss 0.13635404645210475\n",
      "train:  0.7783767496981824\n",
      "curr_loss 0.13601344705221072\n",
      "train:  0.7796885645241716\n",
      "curr_loss 0.13590020590012347\n",
      "train:  0.7802767289397428\n",
      "curr_loss 0.1356496079940701\n",
      "train:  0.7805775655936946\n",
      "curr_loss 0.13571355654973888\n",
      "train:  0.7803077884753465\n",
      "curr_loss 0.135426054805962\n",
      "train:  0.7820484090655914\n",
      "curr_loss 0.13517392452676497\n",
      "train:  0.7813161987365055\n",
      "curr_loss 0.1352125433695257\n",
      "train:  0.7835746495899227\n",
      "curr_loss 0.13490843016709855\n",
      "train:  0.7827747155343139\n",
      "curr_loss 0.1347837660887941\n",
      "train:  0.782536399406192\n",
      "curr_loss 0.13471119861994216\n",
      "train:  0.7834167810813566\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.2131352006499447\n",
      "train:  0.687312239546896\n",
      "curr_loss 0.14702705216051928\n",
      "train:  0.7164286845520303\n",
      "curr_loss 0.1439275138858539\n",
      "train:  0.7332592561702403\n",
      "curr_loss 0.1424325521460813\n",
      "train:  0.7406618811945059\n",
      "curr_loss 0.14166467373643943\n",
      "train:  0.7450638101443442\n",
      "curr_loss 0.14084755051047054\n",
      "train:  0.7498561120165487\n",
      "curr_loss 0.14044403321855697\n",
      "train:  0.7525918935388194\n",
      "curr_loss 0.14042621598907964\n",
      "train:  0.7551175698689409\n",
      "curr_loss 0.13991145600578678\n",
      "train:  0.7572940834793782\n",
      "curr_loss 0.13966995625946652\n",
      "train:  0.7575338275784702\n",
      "curr_loss 0.13941247079206343\n",
      "train:  0.760735475929132\n",
      "curr_loss 0.13903862486282983\n",
      "train:  0.7614563776392306\n",
      "curr_loss 0.1388627625816497\n",
      "train:  0.7605992619107915\n",
      "curr_loss 0.13873952768038755\n",
      "train:  0.7639018107423233\n",
      "curr_loss 0.1386406419437323\n",
      "train:  0.7666836149912146\n",
      "curr_loss 0.13805835901652996\n",
      "train:  0.7678506271984709\n",
      "curr_loss 0.13798826254570662\n",
      "train:  0.7686769820742957\n",
      "curr_loss 0.13770750176105925\n",
      "train:  0.7709646697707631\n",
      "curr_loss 0.13736191799688102\n",
      "train:  0.7716512055913859\n",
      "curr_loss 0.13725720542432063\n",
      "train:  0.7733047688418434\n",
      "curr_loss 0.13694400351438948\n",
      "train:  0.7734431537095859\n",
      "curr_loss 0.13688823599275665\n",
      "train:  0.773481448797563\n",
      "curr_loss 0.13676936968938627\n",
      "train:  0.7754801853867338\n",
      "curr_loss 0.13628221269863755\n",
      "train:  0.7775794528441767\n",
      "curr_loss 0.13610159483418535\n",
      "train:  0.7782755463912551\n",
      "curr_loss 0.1359082315098587\n",
      "train:  0.7799671094154589\n",
      "curr_loss 0.13581363841965424\n",
      "train:  0.7803757701368744\n",
      "curr_loss 0.135734928299242\n",
      "train:  0.78017270400044\n",
      "curr_loss 0.13534619992793495\n",
      "train:  0.781133916281583\n",
      "curr_loss 0.1351919940380908\n",
      "train:  0.7825476129331638\n",
      "curr_loss 0.134987145810578\n",
      "train:  0.7826627517216439\n",
      "curr_loss 0.1350110945342785\n",
      "train:  0.7836634035945109\n",
      "curr_loss 0.13473085866342135\n",
      "train:  0.7846935411838891\n",
      "curr_loss 0.13465675426211524\n",
      "train:  0.7842605780237234\n",
      "curr_loss 0.13467319958393847\n",
      "train:  0.7857298555927903\n",
      "curr_loss 0.13438361994366146\n",
      "train:  0.7871254934065525\n",
      "curr_loss 0.1340181245584393\n",
      "train:  0.786371238249139\n",
      "curr_loss 0.13389876503404693\n",
      "train:  0.7874910207035086\n",
      "curr_loss 0.13387604017014526\n",
      "train:  0.7884527915796367\n",
      "curr_loss 0.1338152607430273\n",
      "train:  0.7898425479889091\n",
      "curr_loss 0.1335760834279345\n",
      "train:  0.7885362967418883\n",
      "curr_loss 0.13365551102813797\n",
      "train:  0.7894304094177154\n",
      "curr_loss 0.13343112107681399\n",
      "train:  0.7908137397598062\n",
      "curr_loss 0.13329255784773708\n",
      "train:  0.7910673850049946\n",
      "curr_loss 0.1330487510532289\n",
      "train:  0.7919813462027999\n",
      "curr_loss 0.13310220600360662\n",
      "train:  0.7914542876233421\n",
      "curr_loss 0.13283682633098678\n",
      "train:  0.7908572726594943\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.19584080569483153\n",
      "train:  0.6898605661036907\n",
      "curr_loss 0.1468705497126081\n",
      "train:  0.7167457526662608\n",
      "curr_loss 0.14447702119006448\n",
      "train:  0.7246355065843489\n",
      "curr_loss 0.14292654521133177\n",
      "train:  0.7449103344403837\n",
      "curr_loss 0.1416107159645403\n",
      "train:  0.7506848364559838\n",
      "curr_loss 0.14110074492532815\n",
      "train:  0.7536504763421229\n",
      "curr_loss 0.14077220130619125\n",
      "train:  0.7568223796658972\n",
      "curr_loss 0.14022400093019305\n",
      "train:  0.7586220993595435\n",
      "curr_loss 0.14016021928977018\n",
      "train:  0.7605727395336138\n",
      "curr_loss 0.13964072366555533\n",
      "train:  0.7619119436121394\n",
      "curr_loss 0.13924089096375367\n",
      "train:  0.7642060915992515\n",
      "curr_loss 0.13919941039376\n",
      "train:  0.7661678462406724\n",
      "curr_loss 0.13888735683699746\n",
      "train:  0.7666371611205614\n",
      "curr_loss 0.1385472014323989\n",
      "train:  0.769122947371878\n",
      "curr_loss 0.138164963777089\n",
      "train:  0.7694634502355823\n",
      "curr_loss 0.13805207721333004\n",
      "train:  0.772414488325348\n",
      "curr_loss 0.13780313711706085\n",
      "train:  0.7719933621778785\n",
      "curr_loss 0.13762004654947205\n",
      "train:  0.7751083773843485\n",
      "curr_loss 0.13722295395622205\n",
      "train:  0.7757463469019603\n",
      "curr_loss 0.13694502079664772\n",
      "train:  0.7764021408652493\n",
      "curr_loss 0.1369527721805359\n",
      "train:  0.7751345582845459\n",
      "curr_loss 0.1366862294018565\n",
      "train:  0.7791484427945794\n",
      "curr_loss 0.1365221314762362\n",
      "train:  0.7771096682097945\n",
      "curr_loss 0.1362966934024398\n",
      "train:  0.7810394231604131\n",
      "curr_loss 0.13596797634416552\n",
      "train:  0.7810407120575295\n",
      "curr_loss 0.13581845446012505\n",
      "train:  0.7806521393380579\n",
      "curr_loss 0.13560891080999848\n",
      "train:  0.7828222733398631\n",
      "curr_loss 0.13553753361773135\n",
      "train:  0.7810155682039941\n",
      "curr_loss 0.13528165132252137\n",
      "train:  0.7829995036112716\n",
      "curr_loss 0.13526098700749933\n",
      "train:  0.7843203380289681\n",
      "curr_loss 0.1350228994788222\n",
      "train:  0.784512993214864\n",
      "curr_loss 0.13468882778835534\n",
      "train:  0.786250656663581\n",
      "curr_loss 0.13472014005800978\n",
      "train:  0.7864988707194391\n",
      "curr_loss 0.1344584023848695\n",
      "train:  0.783607575684876\n",
      "curr_loss 0.13416034339079216\n",
      "train:  0.7871014974250523\n",
      "curr_loss 0.13424618616329498\n",
      "train:  0.7867162314327785\n",
      "curr_loss 0.1339038381751497\n",
      "train:  0.7882773602521449\n",
      "curr_loss 0.13409923398820914\n",
      "train:  0.7887619802887978\n",
      "curr_loss 0.13374447811450532\n",
      "train:  0.788561113068953\n",
      "curr_loss 0.13369022828726032\n",
      "train:  0.7903091295634266\n",
      "curr_loss 0.13364448958071903\n",
      "train:  0.7902421710417744\n",
      "curr_loss 0.1335316531323082\n",
      "train:  0.7876479760850793\n",
      "curr_loss 0.1335440557542725\n",
      "train:  0.7919359015035403\n",
      "curr_loss 0.13329244353136613\n",
      "train:  0.7923130070368043\n",
      "curr_loss 0.13301599926468152\n",
      "train:  0.7915698949758893\n",
      "curr_loss 0.13305482820640155\n",
      "train:  0.7897892732671653\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.21777430176734924\n",
      "train:  0.6483719699581716\n",
      "curr_loss 0.15043675855024538\n",
      "train:  0.7042202334896176\n",
      "curr_loss 0.14659031877173714\n",
      "train:  0.714750669645883\n",
      "curr_loss 0.1451405134070572\n",
      "train:  0.7262208677781681\n",
      "curr_loss 0.14384725623166383\n",
      "train:  0.7318223087023308\n",
      "curr_loss 0.14322229219016744\n",
      "train:  0.7377810890389126\n",
      "curr_loss 0.14244592052638827\n",
      "train:  0.741165622740219\n",
      "curr_loss 0.1419520561226565\n",
      "train:  0.7449007368217964\n",
      "curr_loss 0.14139206800146484\n",
      "train:  0.743499998576896\n",
      "curr_loss 0.14115520296108663\n",
      "train:  0.7494536990762073\n",
      "curr_loss 0.14051539035727137\n",
      "train:  0.7523611038262821\n",
      "curr_loss 0.1401888306312893\n",
      "train:  0.7563012818742776\n",
      "curr_loss 0.13994101831569009\n",
      "train:  0.7585930993711361\n",
      "curr_loss 0.13947952876043557\n",
      "train:  0.760957179605732\n",
      "curr_loss 0.1391975772543926\n",
      "train:  0.7624069929262579\n",
      "curr_loss 0.13893452001300025\n",
      "train:  0.7629447084570705\n",
      "curr_loss 0.13859970789791934\n",
      "train:  0.765472776881256\n",
      "curr_loss 0.13841329577995176\n",
      "train:  0.7668473707993669\n",
      "curr_loss 0.1381399496574307\n",
      "train:  0.7678178260179065\n",
      "curr_loss 0.1377472956456355\n",
      "train:  0.7699178931371712\n",
      "curr_loss 0.1375465373049921\n",
      "train:  0.7708296883240173\n",
      "curr_loss 0.13749483446428432\n",
      "train:  0.7709958213458387\n",
      "curr_loss 0.13730435217939205\n",
      "train:  0.7733252316482369\n",
      "curr_loss 0.13702005507489343\n",
      "train:  0.7740798520547926\n",
      "curr_loss 0.13680042671178705\n",
      "train:  0.7751474507810816\n",
      "curr_loss 0.13664120283737705\n",
      "train:  0.7755148770419483\n",
      "curr_loss 0.13633496907368228\n",
      "train:  0.7756628446259399\n",
      "curr_loss 0.13636797473798343\n",
      "train:  0.7773758390385879\n",
      "curr_loss 0.13612572337264445\n",
      "train:  0.7779175770456384\n",
      "curr_loss 0.13580526478254973\n",
      "train:  0.7787680586525486\n",
      "curr_loss 0.13572451020058116\n",
      "train:  0.7789800535850523\n",
      "curr_loss 0.1355079171150478\n",
      "train:  0.7773506353528377\n",
      "curr_loss 0.13567649017074215\n",
      "train:  0.7795834919497737\n",
      "curr_loss 0.1353750808618555\n",
      "train:  0.7815223540910338\n",
      "curr_loss 0.1351682579710116\n",
      "train:  0.782838917098215\n",
      "curr_loss 0.13517837504397578\n",
      "train:  0.7807928702400709\n",
      "curr_loss 0.13500867194649\n",
      "train:  0.7844172396234191\n",
      "curr_loss 0.1345448148858488\n",
      "train:  0.7827063482213314\n",
      "curr_loss 0.13446326865189112\n",
      "train:  0.7834020607080673\n",
      "curr_loss 0.13432961828376525\n",
      "train:  0.7847778041134036\n",
      "curr_loss 0.13437507531387888\n",
      "train:  0.7840717667484441\n",
      "curr_loss 0.13415585150617865\n",
      "train:  0.7858103998173653\n",
      "curr_loss 0.13411144207959153\n",
      "train:  0.7855444981689504\n",
      "curr_loss 0.13389534414259355\n",
      "train:  0.786302737814694\n",
      "curr_loss 0.1337628098788546\n",
      "train:  0.7862050671707234\n",
      "curr_loss 0.133735213150729\n",
      "train:  0.7866441729967352\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.21282892284998253\n",
      "train:  0.6891293532776633\n",
      "curr_loss 0.14849453419446945\n",
      "train:  0.7075893648777073\n",
      "curr_loss 0.14582631399678947\n",
      "train:  0.7209253717283747\n",
      "curr_loss 0.14419082437285144\n",
      "train:  0.7274161522844239\n",
      "curr_loss 0.1434651284075495\n",
      "train:  0.7361747672449859\n",
      "curr_loss 0.1423533275649322\n",
      "train:  0.738531271319883\n",
      "curr_loss 0.14171847953132136\n",
      "train:  0.7420263265063087\n",
      "curr_loss 0.1411807237572931\n",
      "train:  0.7465976579496412\n",
      "curr_loss 0.14090244531334928\n",
      "train:  0.7522304051002429\n",
      "curr_loss 0.14022468801458082\n",
      "train:  0.7535953031574504\n",
      "curr_loss 0.1399771132650067\n",
      "train:  0.7568777062388864\n",
      "curr_loss 0.13960211887140178\n",
      "train:  0.759001100454554\n",
      "curr_loss 0.13908687041173526\n",
      "train:  0.7615191252911672\n",
      "curr_loss 0.1389768091748603\n",
      "train:  0.7630768983386415\n",
      "curr_loss 0.1388159146652886\n",
      "train:  0.7643157374360369\n",
      "curr_loss 0.13828091345616242\n",
      "train:  0.7633183370984601\n",
      "curr_loss 0.13802903505107064\n",
      "train:  0.7671098936467757\n",
      "curr_loss 0.137848393611647\n",
      "train:  0.7674942031902384\n",
      "curr_loss 0.1376051496510482\n",
      "train:  0.7695013499362989\n",
      "curr_loss 0.13752456472732535\n",
      "train:  0.7712229749364693\n",
      "curr_loss 0.1373220348165403\n",
      "train:  0.7707905383091261\n",
      "curr_loss 0.1371176248388504\n",
      "train:  0.7727269072185741\n",
      "curr_loss 0.13670183455024787\n",
      "train:  0.7713993774411565\n",
      "curr_loss 0.13632580319150764\n",
      "train:  0.7743805664643868\n",
      "curr_loss 0.1362644776479522\n",
      "train:  0.7759343362519192\n",
      "curr_loss 0.1362434971836669\n",
      "train:  0.777450116910988\n",
      "curr_loss 0.136068284696904\n",
      "train:  0.7775654673162304\n",
      "curr_loss 0.13565577611104765\n",
      "train:  0.7756316029948116\n",
      "curr_loss 0.1356646628373891\n",
      "train:  0.7782751854161952\n",
      "curr_loss 0.13530488282590364\n",
      "train:  0.7800905943109993\n",
      "curr_loss 0.1352498095797662\n",
      "train:  0.7797963707749591\n",
      "curr_loss 0.1349173905096244\n",
      "train:  0.7801702545961561\n",
      "curr_loss 0.13497041136174653\n",
      "train:  0.7809699834895433\n",
      "curr_loss 0.1347587814378501\n",
      "train:  0.7830186065029606\n",
      "curr_loss 0.13455732856223832\n",
      "train:  0.7841581544070992\n",
      "curr_loss 0.13435085184538542\n",
      "train:  0.7854736575732084\n",
      "curr_loss 0.13413356507743768\n",
      "train:  0.7839066489438207\n",
      "curr_loss 0.1340316849487338\n",
      "train:  0.7859436692053837\n",
      "curr_loss 0.13381147236373295\n",
      "train:  0.7864782090722199\n",
      "curr_loss 0.1334262945462222\n",
      "train:  0.7863505460249155\n",
      "curr_loss 0.13359054986072416\n",
      "train:  0.787619905290199\n",
      "curr_loss 0.13317587167321154\n",
      "train:  0.7875913128403532\n",
      "curr_loss 0.1331334809091554\n",
      "train:  0.7866850344550067\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.27693875096923676\n",
      "train:  0.5808496978826516\n",
      "curr_loss 0.16452487146676475\n",
      "train:  0.6945025364917979\n",
      "curr_loss 0.14809672942209007\n",
      "train:  0.705239792928501\n",
      "curr_loss 0.1462095155199962\n",
      "train:  0.7158438885194816\n",
      "curr_loss 0.14524152022392595\n",
      "train:  0.722697410770218\n",
      "curr_loss 0.14447275063588252\n",
      "train:  0.7294146982535187\n",
      "curr_loss 0.1436055972801512\n",
      "train:  0.7353312098554805\n",
      "curr_loss 0.1428414043502428\n",
      "train:  0.7419617600911502\n",
      "curr_loss 0.14196170740459688\n",
      "train:  0.7455588649334322\n",
      "curr_loss 0.14173399564342118\n",
      "train:  0.7493751392933881\n",
      "curr_loss 0.1412317631256521\n",
      "train:  0.7518231839225151\n",
      "curr_loss 0.14073234718682162\n",
      "train:  0.7522295566623232\n",
      "curr_loss 0.14073822165454797\n",
      "train:  0.7547950337591958\n",
      "curr_loss 0.14011115640105298\n",
      "train:  0.7580515091272539\n",
      "curr_loss 0.140012009968212\n",
      "train:  0.7582860629752682\n",
      "curr_loss 0.13977587363909727\n",
      "train:  0.7607744600440753\n",
      "curr_loss 0.13945683140066725\n",
      "train:  0.7628322120045913\n",
      "curr_loss 0.1391423558121297\n",
      "train:  0.7648784152534679\n",
      "curr_loss 0.13890648496091662\n",
      "train:  0.7658261680675841\n",
      "curr_loss 0.13877818210801082\n",
      "train:  0.7675043408941822\n",
      "curr_loss 0.13861786759463116\n",
      "train:  0.7674799678904934\n",
      "curr_loss 0.13834166886349816\n",
      "train:  0.7679130302061329\n",
      "curr_loss 0.13811166258297156\n",
      "train:  0.7705729319780518\n",
      "curr_loss 0.1378359405407265\n",
      "train:  0.7719102646346421\n",
      "curr_loss 0.1376787376641041\n",
      "train:  0.7719811196078491\n",
      "curr_loss 0.1375621426016537\n",
      "train:  0.7737636752462992\n",
      "curr_loss 0.1374244999826251\n",
      "train:  0.7742520217027253\n",
      "curr_loss 0.1372569408731081\n",
      "train:  0.7754629344966631\n",
      "curr_loss 0.137138691374022\n",
      "train:  0.7763503654973932\n",
      "curr_loss 0.1370468187094921\n",
      "train:  0.7766512064572197\n",
      "curr_loss 0.1369208181230583\n",
      "train:  0.7684443866208128\n",
      "curr_loss 0.1367328194762344\n",
      "train:  0.7775659889192104\n",
      "curr_loss 0.13651473449533852\n",
      "train:  0.7777400864811114\n",
      "curr_loss 0.13624111866921335\n",
      "train:  0.7791586399273691\n",
      "curr_loss 0.1362773547718181\n",
      "train:  0.7799838510943519\n",
      "curr_loss 0.136114281801442\n",
      "train:  0.7799015465084317\n",
      "curr_loss 0.13605360390238502\n",
      "train:  0.7808736762904797\n",
      "curr_loss 0.1359309683465839\n",
      "train:  0.7817409251160521\n",
      "curr_loss 0.135781289481405\n",
      "train:  0.7829154483064921\n",
      "curr_loss 0.13557984142457669\n",
      "train:  0.7829911330891404\n",
      "curr_loss 0.13537050757686891\n",
      "train:  0.7828784864159154\n",
      "curr_loss 0.1354075193182746\n",
      "train:  0.7830617788455451\n",
      "curr_loss 0.1350521151699237\n",
      "train:  0.7830621739413227\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=PytorchModel(auc_tol=0.005, loss_fn=BCELoss(),\n",
       "                                         net_params={},\n",
       "                                         net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "                                         optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "                                         optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;BaggingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.BaggingClassifier.html\">?<span>Documentation for BaggingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>BaggingClassifier(estimator=PytorchModel(auc_tol=0.005, loss_fn=BCELoss(),\n",
       "                                         net_params={},\n",
       "                                         net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "                                         optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "                                         optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;))</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: PytorchModel</label><div class=\"sk-toggleable__content fitted\"><pre>PytorchModel(auc_tol=0.005, loss_fn=BCELoss(), net_params={},\n",
       "             net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "             optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "             optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">PytorchModel</label><div class=\"sk-toggleable__content fitted\"><pre>PytorchModel(auc_tol=0.005, loss_fn=BCELoss(), net_params={},\n",
       "             net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "             optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "             optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(estimator=PytorchModel(auc_tol=0.005, loss_fn=BCELoss(),\n",
       "                                         net_params={},\n",
       "                                         net_type=<class '__main__.Net_model'>,\n",
       "                                         optimizer_params={'lr': 0.001},\n",
       "                                         optimizer_type=<class 'torch.optim.adam.Adam'>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = PytorchModel(net_type=Net_model, net_params=dict(), optimizer_type=Adam,\n",
    "                          optimizer_params={\"lr\": 1e-3}, loss_fn=torch.nn.BCELoss(),\n",
    "                          batch_size=10000, auc_tol=0.003, tol_epochs=10)\n",
    "\n",
    "meta_classifier = BaggingClassifier(estimator=base_model, n_estimators=10)\n",
    "meta_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = meta_classifier.predict_proba(X_test)\n",
    "pred_2 = get_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7574447479170181"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, pred_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensamble with best RELU model (without dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[df.columns[:-1]], df['target'], stratify=df['target'], train_size=0.7)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "X_train_t =  torch.FloatTensor(X_train.values)\n",
    "y_train_t =  torch.FloatTensor(y_train.values).view(-1, 1)\n",
    "X_test_t =  torch.FloatTensor(X_test.values)\n",
    "y_test_t =  torch.FloatTensor(y_test.values).view(-1, 1)\n",
    "\n",
    "X_train_nump = X_train.to_numpy()\n",
    "y_train_nump =  y_train.to_numpy()\n",
    "X_test_nump =  X_test.to_numpy()\n",
    "y_test_nump =  y_test.to_numpy()\n",
    "\n",
    "class Net_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_model, self).__init__()  \n",
    "        self.fc1 = nn.Linear(194, 100, bias=True)\n",
    "        self.fc2 = nn.Linear(100, 50, bias=True)\n",
    "        self.fc3 = nn.Linear(50, 20, bias=True)\n",
    "        self.fc4 = nn.Linear(20, 5, bias=True)\n",
    "        self.fc5 = nn.Linear(5, 1, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.sigmoid(self.fc5(x))\n",
    "        return x\n",
    "        \n",
    "net = Net_model()\n",
    "\n",
    "class PytorchModel(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, net_type, net_params, optimizer_type, optimizer_params, loss_fn,\n",
    "               batch_size=10000, auc_tol=0.04, tol_epochs=10):\n",
    "        self.net_type = net_type\n",
    "        self.net_params = net_params\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.auc_tol = auc_tol \n",
    "        self.tol_epochs = tol_epochs\n",
    "\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        super().__init__()\n",
    "        self.net = self.net_type(**self.net_params)\n",
    "        self.optimizer = self.optimizer_type(self.net.parameters(), **self.optimizer_params)\n",
    "        \n",
    "        uniq_classes = np.sort(np.unique(y))\n",
    "        self.classes_ = uniq_classes\n",
    "    \n",
    "        X_t = torch.FloatTensor(X)\n",
    "        y_t = torch.FloatTensor(y).view(-1, 1)\n",
    "        train_dataset = TensorDataset(X_t, y_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        metrics = []\n",
    "        epoch = 0\n",
    "        keep_training = True   \n",
    "        while keep_training:\n",
    "            self.net.train()\n",
    "            epoch_loss = []\n",
    "\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                y_pred = self.net(X_batch)\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                epoch_loss.append(loss.item())\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()               \n",
    "            print(\"curr_loss\", np.mean(epoch_loss))\n",
    "            with torch.no_grad():\n",
    "                nn_prediction_train = self.net(X_t).tolist()\n",
    "                roc_auc_sc = roc_auc_score(y_t, nn_prediction_train)\n",
    "                print('train: ', roc_auc_score(y_t, nn_prediction_train))\n",
    "                metrics.append(roc_auc_sc)\n",
    "                \n",
    "            if len(metrics) > self.tol_epochs:\n",
    "                metrics.pop(0)\n",
    "            if len(metrics) == self.tol_epochs:\n",
    "                roc_auc_diff = max(metrics) - min(metrics)\n",
    "                if roc_auc_diff <= self.auc_tol:\n",
    "                    print('-----------------------------------------------------------------------------------------')\n",
    "                    keep_training = False\n",
    "    \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.tensor(X.astype(np.float32))\n",
    "        self.net.eval()\n",
    "        preds_proba = self.net(X_tensor).detach().numpy()\n",
    "        return preds_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds_proba = self.predict_proba(X)\n",
    "        predictions = np.amax(predictions, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(predictions):\n",
    "    predictions = np.amax(predictions, axis=1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_loss 0.23042401701063658\n",
      "train:  0.702924246773754\n",
      "curr_loss 0.1448930241723559\n",
      "train:  0.7364370538922558\n",
      "curr_loss 0.14236768835516117\n",
      "train:  0.7464012499631537\n",
      "curr_loss 0.14132727052441876\n",
      "train:  0.752195106375956\n",
      "curr_loss 0.14093855653532703\n",
      "train:  0.754829566371274\n",
      "curr_loss 0.1404608132604936\n",
      "train:  0.7589541116803166\n",
      "curr_loss 0.1398800436537064\n",
      "train:  0.7603154277967308\n",
      "curr_loss 0.1398379325644294\n",
      "train:  0.758734163409503\n",
      "curr_loss 0.1393679123909319\n",
      "train:  0.7630489409548029\n",
      "curr_loss 0.13896753805786816\n",
      "train:  0.7642064627761472\n",
      "curr_loss 0.13880301467073497\n",
      "train:  0.7648764546180308\n",
      "curr_loss 0.13854043259846038\n",
      "train:  0.7682798487715515\n",
      "curr_loss 0.13843151167109238\n",
      "train:  0.7648497109911425\n",
      "curr_loss 0.13828840421799996\n",
      "train:  0.7698752161956878\n",
      "curr_loss 0.13797582912059567\n",
      "train:  0.7708837272929068\n",
      "curr_loss 0.13782793772754384\n",
      "train:  0.771477978980317\n",
      "curr_loss 0.1376117870063331\n",
      "train:  0.7711683430482029\n",
      "curr_loss 0.13762096853102024\n",
      "train:  0.773874899649634\n",
      "curr_loss 0.13734105974435806\n",
      "train:  0.7738660892914607\n",
      "curr_loss 0.1371476773673029\n",
      "train:  0.7728813300865378\n",
      "curr_loss 0.13674114955894984\n",
      "train:  0.7659967227109306\n",
      "curr_loss 0.13680588772789162\n",
      "train:  0.7755163353464528\n",
      "curr_loss 0.1366514013477819\n",
      "train:  0.7720781111245509\n",
      "curr_loss 0.13644026872233964\n",
      "train:  0.7786252630578093\n",
      "curr_loss 0.13629259936400315\n",
      "train:  0.7788508200929511\n",
      "curr_loss 0.13600432305638468\n",
      "train:  0.7796819478750731\n",
      "curr_loss 0.13598258593189183\n",
      "train:  0.7775909226643415\n",
      "curr_loss 0.13585291398846688\n",
      "train:  0.7794792810807274\n",
      "curr_loss 0.1355264811076928\n",
      "train:  0.7799760936510048\n",
      "curr_loss 0.13534306612477373\n",
      "train:  0.781770591691728\n",
      "curr_loss 0.13526543800658847\n",
      "train:  0.7838142230029759\n",
      "curr_loss 0.13515519046813101\n",
      "train:  0.7838856367997845\n",
      "curr_loss 0.13492082159465818\n",
      "train:  0.7842333951065542\n",
      "curr_loss 0.13471425661993264\n",
      "train:  0.7843474540650942\n",
      "curr_loss 0.13466542747928134\n",
      "train:  0.7861860704899173\n",
      "curr_loss 0.13426300686835058\n",
      "train:  0.7860352472461526\n",
      "curr_loss 0.13414708966046424\n",
      "train:  0.788238807580046\n",
      "curr_loss 0.1338475660156848\n",
      "train:  0.7882745596947709\n",
      "curr_loss 0.13364931659318915\n",
      "train:  0.7877181794046249\n",
      "curr_loss 0.13371173058872793\n",
      "train:  0.7901505613916423\n",
      "curr_loss 0.133277662246678\n",
      "train:  0.7882050087508227\n",
      "curr_loss 0.13326897309045888\n",
      "train:  0.790016673748459\n",
      "curr_loss 0.1330943163577004\n",
      "train:  0.7881280833049674\n",
      "curr_loss 0.13322220101433607\n",
      "train:  0.7911598341805676\n",
      "curr_loss 0.13300869914133157\n",
      "train:  0.7918167812816768\n",
      "curr_loss 0.1325880961112715\n",
      "train:  0.7920578935704402\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.2279359100766443\n",
      "train:  0.6222057976047003\n",
      "curr_loss 0.1497236947663388\n",
      "train:  0.7269806849850158\n",
      "curr_loss 0.14284834577076472\n",
      "train:  0.7412441944675071\n",
      "curr_loss 0.14107508620071174\n",
      "train:  0.7523731534122702\n",
      "curr_loss 0.14013704368427618\n",
      "train:  0.7563924545661683\n",
      "curr_loss 0.1397040545273183\n",
      "train:  0.7544186331249161\n",
      "curr_loss 0.13925139253857124\n",
      "train:  0.7584144297875534\n",
      "curr_loss 0.13900328160666708\n",
      "train:  0.7616420887403519\n",
      "curr_loss 0.1387267602765145\n",
      "train:  0.7647169542711783\n",
      "curr_loss 0.138163625193176\n",
      "train:  0.7670351069606907\n",
      "curr_loss 0.13809596917670758\n",
      "train:  0.7682081511076717\n",
      "curr_loss 0.13782194190060915\n",
      "train:  0.7695636224211275\n",
      "curr_loss 0.13790023193430545\n",
      "train:  0.7714780857948583\n",
      "curr_loss 0.13754800499523456\n",
      "train:  0.7667313035177633\n",
      "curr_loss 0.1372713999294523\n",
      "train:  0.7695219569731027\n",
      "curr_loss 0.1368403465519497\n",
      "train:  0.7718083270110525\n",
      "curr_loss 0.13695441649772636\n",
      "train:  0.7755632594245658\n",
      "curr_loss 0.13660802069439817\n",
      "train:  0.7759093594289597\n",
      "curr_loss 0.13659817462240287\n",
      "train:  0.7771489444709162\n",
      "curr_loss 0.13628606246181982\n",
      "train:  0.7773295666671435\n",
      "curr_loss 0.13615903113760167\n",
      "train:  0.7787325773312895\n",
      "curr_loss 0.13598411092859003\n",
      "train:  0.7789658963524245\n",
      "curr_loss 0.13569823011236998\n",
      "train:  0.7803425897348526\n",
      "curr_loss 0.13569407239185638\n",
      "train:  0.7821023934059972\n",
      "curr_loss 0.13551399100627473\n",
      "train:  0.7814646417718644\n",
      "curr_loss 0.13523160184348992\n",
      "train:  0.7825990225279394\n",
      "curr_loss 0.13531225474912728\n",
      "train:  0.7826447855244493\n",
      "curr_loss 0.13485758069587583\n",
      "train:  0.7843365974519518\n",
      "curr_loss 0.13474808062486981\n",
      "train:  0.78541816810413\n",
      "curr_loss 0.13447539274817083\n",
      "train:  0.7852928203873163\n",
      "curr_loss 0.13443286368502907\n",
      "train:  0.7861526717176182\n",
      "curr_loss 0.13430250050564904\n",
      "train:  0.7862337353406381\n",
      "curr_loss 0.13422342178536883\n",
      "train:  0.7853408556933986\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.6930818800902485\n",
      "train:  0.5\n",
      "curr_loss 0.6085564989355666\n",
      "train:  0.5\n",
      "curr_loss 0.5370606418272749\n",
      "train:  0.5\n",
      "curr_loss 0.4767441052702529\n",
      "train:  0.5\n",
      "curr_loss 0.42589896442878306\n",
      "train:  0.5\n",
      "curr_loss 0.38316376307117406\n",
      "train:  0.5\n",
      "curr_loss 0.34723404494684135\n",
      "train:  0.5\n",
      "curr_loss 0.3169774702235834\n",
      "train:  0.5\n",
      "curr_loss 0.29142821487502674\n",
      "train:  0.5\n",
      "curr_loss 0.2698999913919031\n",
      "train:  0.5\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.22935504025191217\n",
      "train:  0.6924662699413945\n",
      "curr_loss 0.14559618150120351\n",
      "train:  0.7282081224451428\n",
      "curr_loss 0.14298024121208572\n",
      "train:  0.7441648830311663\n",
      "curr_loss 0.14152011082540103\n",
      "train:  0.7484324529986286\n",
      "curr_loss 0.14085737582462937\n",
      "train:  0.7538378754062607\n",
      "curr_loss 0.14038399102824245\n",
      "train:  0.7574662118546248\n",
      "curr_loss 0.14012503794473202\n",
      "train:  0.7582368384933273\n",
      "curr_loss 0.13984372900493108\n",
      "train:  0.759627877751422\n",
      "curr_loss 0.13956533450244077\n",
      "train:  0.7605869960497832\n",
      "curr_loss 0.13940394528914446\n",
      "train:  0.7628745150679485\n",
      "curr_loss 0.13928402682293706\n",
      "train:  0.7640973963430736\n",
      "curr_loss 0.13887111353340434\n",
      "train:  0.7662083743567775\n",
      "curr_loss 0.13844906497950576\n",
      "train:  0.7664597431575796\n",
      "curr_loss 0.13852020223342365\n",
      "train:  0.7670546289815079\n",
      "curr_loss 0.13814833841810178\n",
      "train:  0.768545252063971\n",
      "curr_loss 0.1380573977433627\n",
      "train:  0.7712948302804077\n",
      "curr_loss 0.1376298931033457\n",
      "train:  0.7689252289293844\n",
      "curr_loss 0.13759354687300487\n",
      "train:  0.771667217217914\n",
      "curr_loss 0.1372814172906662\n",
      "train:  0.7747485971551439\n",
      "curr_loss 0.13726371118974923\n",
      "train:  0.7756212813819919\n",
      "curr_loss 0.13703236324870172\n",
      "train:  0.7754518061706528\n",
      "curr_loss 0.13696087692952275\n",
      "train:  0.7777774396511326\n",
      "curr_loss 0.13647652743615915\n",
      "train:  0.7733930172355851\n",
      "curr_loss 0.13658611680292965\n",
      "train:  0.7781760443527085\n",
      "curr_loss 0.13618319841166635\n",
      "train:  0.7785430033428816\n",
      "curr_loss 0.13606978411698223\n",
      "train:  0.7810118499846319\n",
      "curr_loss 0.1358679269825048\n",
      "train:  0.7815130851409754\n",
      "curr_loss 0.13580778834238574\n",
      "train:  0.7824762272956864\n",
      "curr_loss 0.13546485064634636\n",
      "train:  0.774782612551761\n",
      "curr_loss 0.13580273051018737\n",
      "train:  0.7820067639430257\n",
      "curr_loss 0.13535856926322576\n",
      "train:  0.7834310771519997\n",
      "curr_loss 0.1353861953117954\n",
      "train:  0.7847290539289556\n",
      "curr_loss 0.1352955658563334\n",
      "train:  0.7861609282576724\n",
      "curr_loss 0.1351884115320533\n",
      "train:  0.7861399696104386\n",
      "curr_loss 0.13455788594721563\n",
      "train:  0.7856038072945088\n",
      "curr_loss 0.13451441681355386\n",
      "train:  0.7872744697563182\n",
      "curr_loss 0.13464709163749988\n",
      "train:  0.787148314910544\n",
      "curr_loss 0.13449911795445343\n",
      "train:  0.7851703771740743\n",
      "curr_loss 0.1342609357552149\n",
      "train:  0.7889330439323001\n",
      "curr_loss 0.13415268604731678\n",
      "train:  0.7907240873611507\n",
      "curr_loss 0.13393951570661508\n",
      "train:  0.7883748910105469\n",
      "curr_loss 0.13398896847198258\n",
      "train:  0.7901082504098467\n",
      "curr_loss 0.13366686497161637\n",
      "train:  0.7910383611524249\n",
      "curr_loss 0.1334773173453796\n",
      "train:  0.7910742657965236\n",
      "curr_loss 0.1333272140936472\n",
      "train:  0.7906534023957906\n",
      "curr_loss 0.13334906053038972\n",
      "train:  0.7944657202484312\n",
      "curr_loss 0.13310832693357372\n",
      "train:  0.7928562776826099\n",
      "curr_loss 0.13288725312076397\n",
      "train:  0.7951771116553469\n",
      "curr_loss 0.13299492524186177\n",
      "train:  0.7928033761172287\n",
      "curr_loss 0.13296028237734267\n",
      "train:  0.794621021627297\n",
      "curr_loss 0.13264910650638798\n",
      "train:  0.7966182372582118\n",
      "curr_loss 0.1323468744087575\n",
      "train:  0.795846030569607\n",
      "curr_loss 0.13236045366644267\n",
      "train:  0.7978708305101548\n",
      "curr_loss 0.13225873756171458\n",
      "train:  0.7989303676961806\n",
      "curr_loss 0.13211429163591185\n",
      "train:  0.7990288877839175\n",
      "curr_loss 0.13181004443423666\n",
      "train:  0.7995058010601088\n",
      "curr_loss 0.1318453639671577\n",
      "train:  0.7996033388655639\n",
      "curr_loss 0.1318136456964621\n",
      "train:  0.796977526306178\n",
      "curr_loss 0.1316479067155971\n",
      "train:  0.8005632050126751\n",
      "curr_loss 0.13157847482914947\n",
      "train:  0.8002962854482455\n",
      "-----------------------------------------------------------------------------------------\n",
      "curr_loss 0.21048926054245204\n",
      "train:  0.6438931557940508\n",
      "curr_loss 0.14907749286338465\n",
      "train:  0.7334100273746967\n",
      "curr_loss 0.14342342406066497\n",
      "train:  0.7377553922859065\n",
      "curr_loss 0.1429763710617426\n",
      "train:  0.7442513181909082\n",
      "curr_loss 0.14212056860994937\n",
      "train:  0.7476580276974022\n",
      "curr_loss 0.1414238723059792\n",
      "train:  0.7516807610174202\n",
      "curr_loss 0.1410914975315777\n",
      "train:  0.7550535497743214\n",
      "curr_loss 0.14082628043729867\n",
      "train:  0.7566216763612923\n",
      "curr_loss 0.1406852860578257\n",
      "train:  0.7587829971513307\n",
      "curr_loss 0.14028048863754936\n",
      "train:  0.760089295629015\n",
      "curr_loss 0.14030858785358827\n",
      "train:  0.7613199341197446\n",
      "curr_loss 0.139907890662032\n",
      "train:  0.7625606517578365\n",
      "curr_loss 0.13955777479493203\n",
      "train:  0.7649890949288916\n",
      "curr_loss 0.13937267401621709\n",
      "train:  0.7655376832086835\n",
      "curr_loss 0.13906153332238175\n",
      "train:  0.765051728444935\n",
      "curr_loss 0.13916763559502748\n",
      "train:  0.7668777077511741\n",
      "curr_loss 0.138843060725957\n",
      "train:  0.7679048026282038\n",
      "curr_loss 0.13864123761950442\n",
      "train:  0.7695926977915748\n",
      "curr_loss 0.13846475215841883\n",
      "train:  0.7699645505888325\n",
      "curr_loss 0.13823200892008358\n",
      "train:  0.7705919056802176\n",
      "curr_loss 0.13807765435223557\n",
      "train:  0.7687942125095502\n",
      "curr_loss 0.13784734044798572\n",
      "train:  0.7724440369716694\n",
      "curr_loss 0.13766602623225443\n",
      "train:  0.7735297946475581\n",
      "curr_loss 0.13752355519218826\n",
      "train:  0.7745492344039754\n",
      "curr_loss 0.137285191398948\n",
      "train:  0.7750020307470241\n",
      "curr_loss 0.13736036490296843\n",
      "train:  0.7752396100427743\n",
      "curr_loss 0.1370949317047845\n",
      "train:  0.776319037437715\n",
      "curr_loss 0.13693241078165633\n",
      "train:  0.776244904814437\n",
      "curr_loss 0.13674062201336248\n",
      "train:  0.7777296383906546\n",
      "curr_loss 0.13655479786111346\n",
      "train:  0.7777438836468026\n",
      "curr_loss 0.13654474762096927\n",
      "train:  0.7794271247881921\n",
      "curr_loss 0.13644064062122088\n",
      "train:  0.7788076884488289\n",
      "curr_loss 0.13619703843967237\n",
      "train:  0.7767195810187523\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BaggingClassifier(estimator=PytorchModel(auc_tol=0.005, loss_fn=BCELoss(),\n",
       "                                         net_params={},\n",
       "                                         net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "                                         optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "                                         optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;),\n",
       "                  n_estimators=5)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;BaggingClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.BaggingClassifier.html\">?<span>Documentation for BaggingClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>BaggingClassifier(estimator=PytorchModel(auc_tol=0.005, loss_fn=BCELoss(),\n",
       "                                         net_params={},\n",
       "                                         net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "                                         optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "                                         optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;),\n",
       "                  n_estimators=5)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">estimator: PytorchModel</label><div class=\"sk-toggleable__content fitted\"><pre>PytorchModel(auc_tol=0.005, loss_fn=BCELoss(), net_params={},\n",
       "             net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "             optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "             optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">PytorchModel</label><div class=\"sk-toggleable__content fitted\"><pre>PytorchModel(auc_tol=0.005, loss_fn=BCELoss(), net_params={},\n",
       "             net_type=&lt;class &#x27;__main__.Net_model&#x27;&gt;,\n",
       "             optimizer_params={&#x27;lr&#x27;: 0.001},\n",
       "             optimizer_type=&lt;class &#x27;torch.optim.adam.Adam&#x27;&gt;)</pre></div> </div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BaggingClassifier(estimator=PytorchModel(auc_tol=0.005, loss_fn=BCELoss(),\n",
       "                                         net_params={},\n",
       "                                         net_type=<class '__main__.Net_model'>,\n",
       "                                         optimizer_params={'lr': 0.001},\n",
       "                                         optimizer_type=<class 'torch.optim.adam.Adam'>),\n",
       "                  n_estimators=5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model_nodrop = PytorchModel(net_type=Net_model, net_params=dict(), optimizer_type=Adam,\n",
    "                          optimizer_params={\"lr\": 1e-3}, loss_fn=torch.nn.BCELoss(),\n",
    "                          batch_size=10000, auc_tol=0.005, tol_epochs=10)\n",
    "\n",
    "meta_classifier_nodrop = BaggingClassifier(estimator=base_model_nodrop, n_estimators=5)\n",
    "meta_classifier_nodrop.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nodrop = meta_classifier_nodrop.predict_proba(X_test)\n",
    "pred_2_nodrop = get_pred(pred_nodrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752063258712846"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test, pred_2_nodrop)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1ad15a495b554f6988a6bf8034b47c87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23e710f9ded3410fb7739bf2f708765b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "241ce64f1fc34f74a8f34aa59ad826f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b69b7d615c44ee184e7411f134801a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fb33ae64d7304648907fca36c15f3b21",
       "IPY_MODEL_4a2acf6316dc48f9bce1d99d87fc9dea",
       "IPY_MODEL_741177c6570644628034c5aa05eabc77"
      ],
      "layout": "IPY_MODEL_1ad15a495b554f6988a6bf8034b47c87"
     }
    },
    "4a2acf6316dc48f9bce1d99d87fc9dea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ba69de817e745e78d82a73d59292a7f",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_23e710f9ded3410fb7739bf2f708765b",
      "value": 2
     }
    },
    "4b611a125375404e860f4d20d62a23ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ba69de817e745e78d82a73d59292a7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "519903cf72e44d38a31dab1b35e63db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "55b69718f08f497e80f3e04e6a48df72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7fa48124bf95428290882f8b2e6613d3",
      "placeholder": "​",
      "style": "IPY_MODEL_241ce64f1fc34f74a8f34aa59ad826f0",
      "value": "Transforming transactions data: 100%"
     }
    },
    "65e640c2bdc142e38063637901fecdd1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7078f2a7e69140baade16d892b921a52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741177c6570644628034c5aa05eabc77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5120879dec04920a9520f48990108f5",
      "placeholder": "​",
      "style": "IPY_MODEL_f1951d6bdf53462cb1237cf4973dd1d0",
      "value": " 2/2 [00:03&lt;00:00,  1.50s/it]"
     }
    },
    "7e321bc6d2ab43e09f084cf5c56871e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7fa48124bf95428290882f8b2e6613d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0ee0775d5224e63bebed82b6da365ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65e640c2bdc142e38063637901fecdd1",
      "placeholder": "​",
      "style": "IPY_MODEL_519903cf72e44d38a31dab1b35e63db2",
      "value": " 1/1 [00:15&lt;00:00, 15.05s/it]"
     }
    },
    "a6e783c7b06c4729ac354630a6d7de37": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55b69718f08f497e80f3e04e6a48df72",
       "IPY_MODEL_fd892497bcc1499995d753e3a5a57bfc",
       "IPY_MODEL_a0ee0775d5224e63bebed82b6da365ef"
      ],
      "layout": "IPY_MODEL_4b611a125375404e860f4d20d62a23ca"
     }
    },
    "c5120879dec04920a9520f48990108f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ceb12588680f454490dd102c0f6763d1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1951d6bdf53462cb1237cf4973dd1d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8f7460fde7f4a37a3aedba33c1d1def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fb33ae64d7304648907fca36c15f3b21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e321bc6d2ab43e09f084cf5c56871e8",
      "placeholder": "​",
      "style": "IPY_MODEL_7078f2a7e69140baade16d892b921a52",
      "value": "Reading dataset with pandas: 100%"
     }
    },
    "fd892497bcc1499995d753e3a5a57bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ceb12588680f454490dd102c0f6763d1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8f7460fde7f4a37a3aedba33c1d1def",
      "value": 1
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
