{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76258575-4e6f-4059-b768-292e84151178",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sklearn\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from torch.optim import Adam\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "\n",
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
    "                                     num_parts_to_read: int = 2, columns=None, verbose=False) -> pd.DataFrame:\n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                              if filename.startswith('train')])\n",
    "    print(dataset_paths)\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "    if verbose:\n",
    "        print('Reading chunks:\\n')\n",
    "        for chunk in chunks:\n",
    "            print(chunk)\n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc=\"Reading dataset with pandas\"):\n",
    "        print('chunk_path', chunk_path)\n",
    "        chunk = pd.read_parquet(chunk_path,columns=columns)\n",
    "        res.append(chunk)\n",
    "\n",
    "    return pd.concat(res).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def create_dataset(path):\n",
    "    for i, elem in enumerate(['df_'+str(a) for a in range(0, 12)]):\n",
    "        elem = read_parquet_dataset_from_local(path, i, 1)\n",
    "        if i==0:\n",
    "            df = elem.copy()\n",
    "        else:\n",
    "            df = pd.concat([df, elem], ignore_index=True, axis=0)\n",
    "        del elem\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_col(df_1, targets=None):\n",
    "    columns_to_drop = ['pre_since_opened', 'pre_since_confirmed', 'pre_fterm', 'pre_till_pclose', \n",
    "                       'pre_till_fclose', 'pre_loans_outstanding', 'pre_loans_total_overdue',\n",
    "                       'pre_loans_max_overdue_sum', 'pre_loans90', 'is_zero_util', 'is_zero_over2limit', \n",
    "                       'is_zero_maxover2limit', 'enc_paym_0', 'enc_paym_1', 'enc_paym_2', 'enc_paym_3', \n",
    "                       'enc_paym_4', 'enc_paym_5', 'enc_paym_6', 'enc_paym_7', 'enc_paym_8', 'enc_paym_9', \n",
    "                       'enc_paym_10', 'enc_paym_11', 'enc_paym_12', 'enc_paym_13', 'enc_paym_14', 'enc_paym_15', \n",
    "                       'enc_paym_16', 'enc_paym_17', 'enc_paym_18', 'enc_paym_19', 'enc_paym_20','enc_paym_21']\n",
    "    return df_1.drop(columns_to_drop, axis=1)\n",
    "\n",
    "\n",
    "def one_hot_enc(df_1, targets=None):\n",
    "    cat_columns = ['pre_pterm', 'pre_loans_credit_limit',  'pre_loans_next_pay_summ', 'pre_loans_credit_cost_rate',\n",
    "              'pre_loans5', 'pre_loans530', 'pre_loans3060', 'pre_loans6090', 'pre_util', 'pre_over2limit', 'is_zero_loans90', \n",
    "               'pre_maxover2limit', 'enc_paym_22', 'enc_paym_23', 'enc_paym_24', 'enc_loans_account_holder_type',\n",
    "               'enc_loans_credit_status', 'enc_loans_credit_type', 'enc_loans_account_cur']\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore', dtype='int8')\n",
    "    df_1[ohe.get_feature_names_out()] = ohe.fit_transform(df_1[cat_columns])\n",
    "    df_1.drop(columns=cat_columns, inplace=True)\n",
    "    return df_1\n",
    "\n",
    "\n",
    "def group_df(df_1, targets=None):\n",
    "    def new_features(df):\n",
    "        df_1 = df.copy()\n",
    "        df_1 = df_1.iloc[:, :2].copy().groupby(['id']).max()\n",
    "        df_1.columns = ['max_rn']\n",
    "        return df_1\n",
    "    \n",
    "    df_max_rn = new_features(df_1)\n",
    "    df_1 = df_1.groupby(['id']).sum()\n",
    "    return df_1.merge(df_max_rn, how='left', on='id').drop(columns='rn')\n",
    "\n",
    "\n",
    "class Net_model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net_model, self).__init__()  \n",
    "            self.fc1 = nn.Linear(195, 100, bias=True)\n",
    "            self.fc2 = nn.Linear(100, 50, bias=True)\n",
    "            self.dropout = nn.Dropout(p=0.4)\n",
    "            self.fc3 = nn.Linear(50, 20, bias=True)\n",
    "            self.fc4 = nn.Linear(20, 5, bias=True)\n",
    "            self.fc5 = nn.Linear(5, 1, bias=True)\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(F.relu(self.fc2(x)))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            x = F.relu(self.fc4(x))\n",
    "            x = F.sigmoid(self.fc5(x))\n",
    "            return x\n",
    "\n",
    "            \n",
    "class PytorchModel(sklearn.base.BaseEstimator):\n",
    "    def __init__(self, net_type, net_params, optimizer_type, optimizer_params, loss_fn,\n",
    "                 batch_size=10000, auc_tol=0.04, tol_epochs=10):\n",
    "        self.net_type = net_type\n",
    "        self.net_params = net_params\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.loss_fn = loss_fn\n",
    "    \n",
    "        self.batch_size = batch_size\n",
    "        self.auc_tol = auc_tol \n",
    "        self.tol_epochs = tol_epochs\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        super().__init__()\n",
    "        self.net = self.net_type(**self.net_params)\n",
    "        self.optimizer = self.optimizer_type(self.net.parameters(), **self.optimizer_params)\n",
    "            \n",
    "        uniq_classes = np.sort(np.unique(y))\n",
    "        self.classes_ = uniq_classes\n",
    "        \n",
    "        X_t = torch.FloatTensor(X)\n",
    "        y_t = torch.FloatTensor(y).view(-1, 1)\n",
    "        train_dataset = TensorDataset(X_t, y_t)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "        metrics = []\n",
    "        epoch = 0\n",
    "        keep_training = True   \n",
    "        while keep_training:\n",
    "            self.net.train()\n",
    "            epoch_loss = []\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                y_pred = self.net(X_batch)\n",
    "                loss = self.loss_fn(y_pred, y_batch)\n",
    "                epoch_loss.append(loss.item())\n",
    "                    \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()               \n",
    "                    \n",
    "            with torch.no_grad():\n",
    "                nn_prediction_train = self.net(X_t).tolist()\n",
    "                roc_auc_sc = roc_auc_score(y_t, nn_prediction_train)\n",
    "                metrics.append(roc_auc_sc)\n",
    "                    \n",
    "            if len(metrics) > self.tol_epochs:\n",
    "                metrics.pop(0)\n",
    "            if len(metrics) == self.tol_epochs:\n",
    "                roc_auc_diff = max(metrics) - min(metrics)\n",
    "                if roc_auc_diff <= self.auc_tol:\n",
    "                    keep_training = False        \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X_tensor = torch.tensor(X.astype(np.float32))\n",
    "        self.net.eval()\n",
    "        preds_proba = self.net(X_tensor).detach().numpy()\n",
    "        return preds_proba\n",
    "        \n",
    "    def predict(self, X):\n",
    "        preds_proba = self.predict_proba(X)\n",
    "        predictions = np.amax(preds_proba, axis=1)\n",
    "        return predictions\n",
    "\n",
    "    \n",
    "def get_pred(predictions):\n",
    "    predictions = np.amax(predictions, axis=1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def main():\n",
    "    path = 'train_data/'\n",
    "    df = create_dataset(path)\n",
    "    targets = pd.read_csv('train_target.csv')\n",
    "\n",
    "\n",
    "    preprocessor = Pipeline(steps=[\n",
    "        ('drop_columns', FunctionTransformer(drop_col)),\n",
    "        ('encoder', FunctionTransformer(one_hot_enc)),\n",
    "        ('group_dataset', FunctionTransformer(group_df))\n",
    "         ])\n",
    "\n",
    "    df =  preprocessor.fit_transform(df, targets)\n",
    "    df.loc[:, 'target'] = targets.loc[:, 'flag']\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    int_64_cols  = df.select_dtypes('int64').columns\n",
    "    df[int_64_cols] = df[int_64_cols].astype('int8')\n",
    "    \n",
    "    X, y =  df[df.columns[:-1]].to_numpy(), df[df.columns[-1]].to_numpy()\n",
    "\n",
    "    net = Net_model()   \n",
    "    estimated_model = PytorchModel(net_type=Net_model, net_params=dict(), optimizer_type=Adam, optimizer_params={\"lr\": 1e-3}, \n",
    "                                   loss_fn=torch.nn.BCELoss(), batch_size=10000, auc_tol=0.003, tol_epochs=10)\n",
    "    meta_classifier = BaggingClassifier(estimator=estimated_model, n_estimators=10)\n",
    "    meta_classifier.fit(X, y)   \n",
    "    \n",
    "    predict_nn = get_pred(meta_classifier.predict_proba(X))\n",
    "    print('roc_auc: ', roc_auc_score(y, predict_nn))\n",
    "\n",
    "    joblib.dump(meta_classifier, 'ML_project.joblib')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
